{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"TeachMe.Codes Python Summer School 2019, Edit in GitHub This online document helps the absolute beginners to persue the future direction in coding and machine learning. The lesson starts with how to write code in Python along with fundamental ideas in data structure, function and class.","title":"Home"},{"location":"#teachmecodes","text":"Python Summer School 2019, Edit in GitHub This online document helps the absolute beginners to persue the future direction in coding and machine learning. The lesson starts with how to write code in Python along with fundamental ideas in data structure, function and class.","title":"TeachMe.Codes"},{"location":"WhyCodes/","text":"Why Codes? Codes are apparently the language for Human-Machine interface. Coding is the most fundamental skill required for growing with modern days technology. Sicen our computers are built up based on logic and algorithm, our coding languages are also higher lavel of logical steps made undersatndable for human. By typing a line of code, we are speaking the laguage of the machine. Fundamentally, each programming language has it's inner working principle with underlying datastructure and functions. Computer while running a piece of code maintains the data in the memory ( temporary at RAM or permanent at Hard Disk). Whil code is live in the machine, it has time and space complexicities with the underlying datastructure and algotihm excuting the task.","title":"Why Codes?"},{"location":"WhyCodes/#why-codes","text":"Codes are apparently the language for Human-Machine interface. Coding is the most fundamental skill required for growing with modern days technology. Sicen our computers are built up based on logic and algorithm, our coding languages are also higher lavel of logical steps made undersatndable for human. By typing a line of code, we are speaking the laguage of the machine. Fundamentally, each programming language has it's inner working principle with underlying datastructure and functions. Computer while running a piece of code maintains the data in the memory ( temporary at RAM or permanent at Hard Disk). Whil code is live in the machine, it has time and space complexicities with the underlying datastructure and algotihm excuting the task.","title":"Why Codes?"},{"location":"Classifiers/Ensamble/ensamble/","text":"Ensamble The goal of ensemble methods is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator. Two families of ensemble methods are usually distinguished: In averaging methods , the driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced. Examples: - Bagging methods, - Forests of randomized trees, ... By contrast, in boosting methods , base estimators are built sequentially and one tries to reduce the bias of the combined estimator. The motivation is to combine several weak models to produce a powerful ensemble. Examples: - AdaBoost, - Gradient Tree Boosting, ... % matplotlib inline 1. Single estimator versus bagging: bias-variance decomposition ( source ) This example illustrates and compares the bias-variance decomposition of the expected mean squared error of a single estimator against a bagging ensemble. In regression, the expected mean squared error of an estimator can be decomposed in terms of bias, variance and noise. On average over datasets of the regression problem, the bias term measures the average amount by which the predictions of the estimator differ from the predictions of the best possible estimator for the problem (i.e., the Bayes model). The variance term measures the variability of the predictions of the estimator when fit over different instances LS of the problem. Finally, the noise measures the irreducible part of the error which is due the variability in the data. The upper left figure illustrates the predictions (in dark red) of a single decision tree trained over a random dataset LS (the blue dots) of a toy 1d regression problem. It also illustrates the predictions (in light red) of other single decision trees trained over other (and different) randomly drawn instances LS of the problem. Intuitively, the variance term here corresponds to the width of the beam of predictions (in light red) of the individual estimators. The larger the variance, the more sensitive are the predictions for x to small changes in the training set. The bias term corresponds to the difference between the average prediction of the estimator (in cyan) and the best possible model (in dark blue). On this problem, we can thus observe that the bias is quite low (both the cyan and the blue curves are close to each other) while the variance is large (the red beam is rather wide). The lower left figure plots the pointwise decomposition of the expected mean squared error of a single decision tree. It confirms that the bias term (in blue) is low while the variance is large (in green). It also illustrates the noise part of the error which, as expected, appears to be constant and around 0.01 . The right figures correspond to the same plots but using instead a bagging ensemble of decision trees. In both figures, we can observe that the bias term is larger than in the previous case. In the upper right figure, the difference between the average prediction (in cyan) and the best possible model is larger (e.g., notice the offset around x=2 ). In the lower right figure, the bias curve is also slightly higher than in the lower left figure. In terms of variance however, the beam of predictions is narrower, which suggests that the variance is lower. Indeed, as the lower right figure confirms, the variance term (in green) is lower than for single decision trees. Overall, the bias- variance decomposition is therefore no longer the same. The tradeoff is better for bagging: averaging several decision trees fit on bootstrap copies of the dataset slightly increases the bias term but allows for a larger reduction of the variance, which results in a lower overall mean squared error (compare the red curves int the lower figures). The script output also confirms this intuition. The total error of the bagging ensemble is lower than the total error of a single decision tree, and this difference indeed mainly stems from a reduced variance. import numpy as np import matplotlib.pyplot as plt from sklearn.ensemble import BaggingRegressor from sklearn.tree import DecisionTreeRegressor Setting Parameters # Settings n_repeat = 50 # Number of iterations for computing expectations n_train = 50 # Size of the training set n_test = 1000 # Size of the test set noise = 0.1 # Standard deviation of the noise np . random . seed( 0 ) Change this for exploring the bias-variance decomposition of other estimators. This should work well for estimators with high variance (e.g., decision trees or KNN), but poorly for estimators with low variance (e.g., linear models). estimators = [( \"Tree\" , DecisionTreeRegressor()), ( \"Bagging(Tree)\" , BaggingRegressor(DecisionTreeRegressor()))] n_estimators = len (estimators) ### Data # Generate data def f (x): x = x . ravel() return np . exp( - x ** 2 ) + 1.5 * np . exp( - (x - 2 ) ** 2 ) def generate (n_samples, noise, n_repeat = 1 ): X = np . random . rand(n_samples) * 10 - 5 X = np . sort(X) if n_repeat == 1 : y = f(X) + np . random . normal( 0.0 , noise, n_samples) else : y = np . zeros((n_samples, n_repeat)) for i in range (n_repeat): y[:, i] = f(X) + np . random . normal( 0.0 , noise, n_samples) X = X . reshape((n_samples, 1 )) return X, y Partition X_train = [] y_train = [] for i in range (n_repeat): X, y = generate(n_samples = n_train, noise = noise) X_train . append(X) y_train . append(y) X_test, y_test = generate(n_samples = n_test, noise = noise, n_repeat = n_repeat) ### Plot # Loop over estimators to compare for n, (name, estimator) in enumerate (estimators): # Compute predictions y_predict = np . zeros((n_test, n_repeat)) for i in range (n_repeat): estimator . fit(X_train[i], y_train[i]) y_predict[:, i] = estimator . predict(X_test) # Bias^2 + Variance + Noise decomposition of the mean squared error y_error = np . zeros(n_test) for i in range (n_repeat): for j in range (n_repeat): y_error += (y_test[:, j] - y_predict[:, i]) ** 2 y_error /= (n_repeat * n_repeat) y_noise = np . var(y_test, axis = 1 ) y_bias = (f(X_test) - np . mean(y_predict, axis = 1 )) ** 2 y_var = np . var(y_predict, axis = 1 ) print ( \"{0}: {1:.4f} (error) = {2:.4f} (bias^2) \" \" + {3:.4f} (var) + {4:.4f} (noise)\" . format(name, np . mean(y_error), np . mean(y_bias), np . mean(y_var), np . mean(y_noise))) # Plot figures plt . subplot( 2 , n_estimators, n + 1 ) plt . plot(X_test, f(X_test), \"b\" , label = \"$f(x)$\" ) plt . plot(X_train[ 0 ], y_train[ 0 ], \".b\" , label = \"LS ~ $y = f(x)+noise$\" ) for i in range (n_repeat): if i == 0 : plt . plot(X_test, y_predict[:, i], \"r\" , label = \"$\\^y(x)$\" ) else : plt . plot(X_test, y_predict[:, i], \"r\" , alpha = 0.05 ) plt . plot(X_test, np . mean(y_predict, axis = 1 ), \"c\" , label = \"$\\mathbb{E}_{LS} \\^y(x)$\" ) plt . xlim([ - 5 , 5 ]) plt . title(name) if n == 0 : plt . legend(loc = \"upper left\" , prop = { \"size\" : 11 }) plt . subplot( 2 , n_estimators, n_estimators + n + 1 ) plt . plot(X_test, y_error, \"r\" , label = \"$error(x)$\" ) plt . plot(X_test, y_bias, \"b\" , label = \"$bias^2(x)$\" ), plt . plot(X_test, y_var, \"g\" , label = \"$variance(x)$\" ), plt . plot(X_test, y_noise, \"c\" , label = \"$noise(x)$\" ) plt . xlim([ - 5 , 5 ]) plt . ylim([ 0 , 0.1 ]) if n == 0 : plt . legend(loc = \"upper left\" , prop = { \"size\" : 11 }) plt . show() Tree: 0.0255 (error) = 0.0003 (bias^2) + 0.0152 (var) + 0.0098 (noise) Bagging(Tree): 0.0196 (error) = 0.0004 (bias^2) + 0.0092 (var) + 0.0098 (noise) 2. OOB Errors for Random Forests ( source ) The RandomForestClassifier is trained using bootstrap aggregation , where each new tree is fit from a bootstrap sample of the training observations $z_i = (x_i, y_i)$. The out-of-bag (OOB) error is the average error for each $z_i$ calculated using predictions from the trees that do not contain $z_i$ in their respective bootstrap sample. This allows the RandomForestClassifier to be fit and validated whilst being trained The example below demonstrates how the OOB error can be measured at the addition of each new tree during training. The resulting plot allows a practitioner to approximate a suitable value of n_estimators at which the error stabilizes. import matplotlib.pyplot as plt from collections import OrderedDict from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier #### Data Set RANDOM_STATE = 123 # Generate a binary classification dataset. X, y = make_classification(n_samples = 500 , n_features = 25 , n_clusters_per_class = 1 , n_informative = 15 , random_state = RANDOM_STATE) warm_start NOTE: Setting the warm_start construction parameter to True disables support for paralellised ensembles but is necessary for tracking the OOB error trajectory during training. ensemble_clfs = [ ( \"RandomForestClassifier, max_features='sqrt'\" , RandomForestClassifier(warm_start = True , oob_score = True , max_features = \"sqrt\" , random_state = RANDOM_STATE)), ( \"RandomForestClassifier, max_features='log2'\" , RandomForestClassifier(warm_start = True , max_features = 'log2' , oob_score = True , random_state = RANDOM_STATE)), ( \"RandomForestClassifier, max_features=None\" , RandomForestClassifier(warm_start = True , max_features = None , oob_score = True , random_state = RANDOM_STATE)) ] Map a classifier name to a list of ( , ) pairs. error_rate = OrderedDict((label, []) for label, _ in ensemble_clfs) # Range of `n_estimators` values to explore. min_estimators = 15 max_estimators = 175 for label, clf in ensemble_clfs: for i in range (min_estimators, max_estimators + 1 ): clf . set_params(n_estimators = i) clf . fit(X, y) # Record the OOB error for each `n_estimators=i` setting. oob_error = 1 - clf . oob_score_ error_rate[label] . append((i, oob_error)) #### Plot # Generate the \"OOB error rate\" vs. \"n_estimators\" plot. for label, clf_err in error_rate . items(): xs, ys = zip ( * clf_err) plt . plot(xs, ys, label = label) plt . xlim(min_estimators, max_estimators) plt . xlabel( \"n_estimators\" ) plt . ylabel( \"OOB error rate\" ) plt . legend(loc = \"upper right\" ) plt . show() 3. Feature transformations with ensembles of trees ( source ) Transform your features into a higher dimensional, sparse space. Then train a linear model on these features. First fit an ensemble of trees (totally random trees, a random forest, or gradient boosted trees) on the training set. Then each leaf of each tree in the ensemble is assigned a fixed arbitrary feature index in a new feature space. These leaf indices are then encoded in a one-hot fashion. Each sample goes through the decisions of each tree of the ensemble and ends up in one leaf per tree. The sample is encoded by setting feature values for these leaves to 1 and the other feature values to 0. The resulting transformer has then learned a supervised, sparse, high-dimensional categorical embedding of the data. import numpy as np np . random . seed( 10 ) import matplotlib.pyplot as plt #======== models ===================================== from sklearn.linear_model import LogisticRegression from sklearn.ensemble import (RandomTreesEmbedding, RandomForestClassifier, GradientBoostingClassifier) from sklearn.preprocessing import OneHotEncoder from sklearn.cross_validation import train_test_split from sklearn.metrics import roc_curve from sklearn.pipeline import make_pipeline #======= data ================================== from sklearn.datasets import make_classification #### Data n_estimator = 10 X, y = make_classification(n_samples = 80000 ) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5 ) It is important to train the ensemble of trees on a different subset of the training data than the linear regression model to avoid overfitting, in particular if the total number of leaves is similar to the number of training samples X_train, X_train_lr, y_train, y_train_lr = train_test_split(X_train, y_train, test_size = 0.5 ) Unsupervised transformation # Unsupervised transformation based on totally random trees rt = RandomTreesEmbedding(max_depth = 3 , n_estimators = n_estimator, random_state = 0 ) rt_lm = LogisticRegression() pipeline = make_pipeline(rt, rt_lm) pipeline . fit(X_train, y_train) y_pred_rt = pipeline . predict_proba(X_test)[:, 1 ] fpr_rt_lm, tpr_rt_lm, _ = roc_curve(y_test, y_pred_rt) #### Model # Supervised transformation based on random forests rf = RandomForestClassifier(max_depth = 3 , n_estimators = n_estimator) rf_enc = OneHotEncoder() rf_lm = LogisticRegression() rf . fit(X_train, y_train) rf_enc . fit(rf . apply(X_train)) rf_lm . fit(rf_enc . transform(rf . apply(X_train_lr)), y_train_lr) y_pred_rf_lm = rf_lm . predict_proba(rf_enc . transform(rf . apply(X_test)))[:, 1 ] fpr_rf_lm, tpr_rf_lm, _ = roc_curve(y_test, y_pred_rf_lm) grd = GradientBoostingClassifier(n_estimators = n_estimator) grd_enc = OneHotEncoder() grd_lm = LogisticRegression() grd . fit(X_train, y_train) grd_enc . fit(grd . apply(X_train)[:, :, 0 ]) grd_lm . fit(grd_enc . transform(grd . apply(X_train_lr)[:, :, 0 ]), y_train_lr) y_pred_grd_lm = grd_lm . predict_proba( grd_enc . transform(grd . apply(X_test)[:, :, 0 ]))[:, 1 ] fpr_grd_lm, tpr_grd_lm, _ = roc_curve(y_test, y_pred_grd_lm) # The gradient boosted model by itself y_pred_grd = grd . predict_proba(X_test)[:, 1 ] fpr_grd, tpr_grd, _ = roc_curve(y_test, y_pred_grd) # The random forest model by itself y_pred_rf = rf . predict_proba(X_test)[:, 1 ] fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_rf) Plot-1 plt . figure( 1 ) plt . plot([ 0 , 1 ], [ 0 , 1 ], 'k--' ) plt . plot(fpr_rt_lm, tpr_rt_lm, label = 'RT + LR' ) plt . plot(fpr_rf, tpr_rf, label = 'RF' ) plt . plot(fpr_rf_lm, tpr_rf_lm, label = 'RF + LR' ) plt . plot(fpr_grd, tpr_grd, label = 'GBT' ) plt . plot(fpr_grd_lm, tpr_grd_lm, label = 'GBT + LR' ) plt . xlabel( 'False positive rate' ) plt . ylabel( 'True positive rate' ) plt . title( 'ROC curve' ) plt . legend(loc = 'best' ) plt . show() Plot-2 plt . figure( 2 ) plt . xlim( 0 , 0.2 ) plt . ylim( 0.8 , 1 ) plt . plot([ 0 , 1 ], [ 0 , 1 ], 'k--' ) plt . plot(fpr_rt_lm, tpr_rt_lm, label = 'RT + LR' ) plt . plot(fpr_rf, tpr_rf, label = 'RF' ) plt . plot(fpr_rf_lm, tpr_rf_lm, label = 'RF + LR' ) plt . plot(fpr_grd, tpr_grd, label = 'GBT' ) plt . plot(fpr_grd_lm, tpr_grd_lm, label = 'GBT + LR' ) plt . xlabel( 'False positive rate' ) plt . ylabel( 'True positive rate' ) plt . title( 'ROC curve (zoomed in at top left)' ) plt . legend(loc = 'best' ) plt . show() 4. Pixel importances with a parallel forest of trees ( source ) This example shows the use of forests of trees to evaluate the importance of the pixels in an image classification task (faces). The hotter the pixel, the more important. The code below also illustrates how the construction and the computation of the predictions can be parallelized within multiple jobs. from time import time import matplotlib.pyplot as plt from sklearn.datasets import fetch_olivetti_faces from sklearn.ensemble import ExtraTreesClassifier #### Data # Number of cores to use to perform parallel fitting of the forest model n_jobs = 1 # Load the faces dataset data = fetch_olivetti_faces() X = data . images . reshape(( len (data . images), - 1 )) y = data . target mask = y < 5 # Limit to 5 classes X = X[mask] y = y[mask] Perform parallel fitting of the forest model # Build a forest and compute the pixel importances print ( \"Fitting ExtraTreesClassifier on faces data with %d cores...\" % n_jobs) t0 = time() forest = ExtraTreesClassifier(n_estimators = 1000 , max_features = 128 , n_jobs = n_jobs, random_state = 0 ) forest . fit(X, y) print ( \"done in %0.3f s\" % (time() - t0)) importances = forest . feature_importances_ importances = importances . reshape(data . images[ 0 ] . shape) Fitting ExtraTreesClassifier on faces data with 1 cores... done in 2.155s ##### Plot # Plot pixel importances plt . matshow(importances, cmap = plt . cm . hot) plt . title( \"Pixel importances with forests of trees\" ) plt . show() 5. Feature importances with forests of trees ( source ) This examples shows the use of forests of trees to evaluate the importance of features on an artificial classification task. The red bars are the feature importances of the forest, along with their inter-trees variability. As expected, the plot suggests that 3 features are informative, while the remaining are not. import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_classification from sklearn.ensemble import ExtraTreesClassifier Build a classification task # Build a classification task using 3 informative features X, y = make_classification(n_samples = 1000 , n_features = 10 , n_informative = 3 , n_redundant = 0 , n_repeated = 0 , n_classes = 2 , random_state = 0 , shuffle = False ) Data # Build a forest and compute the feature importances forest = ExtraTreesClassifier(n_estimators = 250 , random_state = 0 ) forest . fit(X, y) importances = forest . feature_importances_ std = np . std([tree . feature_importances_ for tree in forest . estimators_], axis = 0 ) indices = np . argsort(importances)[:: - 1 ] Feature Ranking # Print the feature ranking print ( \"Feature ranking:\" ) for f in range (X . shape[ 1 ]): print ( \" %d . feature %d ( %f )\" % (f + 1 , indices[f], importances[indices[f]])) Feature ranking: 1. feature 0 (0.250402) 2. feature 1 (0.231094) 3. feature 2 (0.148057) 4. feature 3 (0.055632) 5. feature 5 (0.054583) 6. feature 8 (0.054573) 7. feature 6 (0.052606) 8. feature 7 (0.051109) 9. feature 9 (0.051010) 10. feature 4 (0.050934) Plot the feature importances of the forest # Plot the feature importances of the forest plt . figure() plt . title( \"Feature importances\" ) plt . bar( range (X . shape[ 1 ]), importances[indices], color = \"r\" , yerr = std[indices], align = \"center\" ) plt . xticks( range (X . shape[ 1 ]), indices) plt . xlim([ - 1 , X . shape[ 1 ]]) plt . show() 6. IsolationForest example ( source ) An example using IsolationForest for anomaly detection. The IsolationForest 'isolates' observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature. Since recursive partitioning can be represented by a tree structure, the number of splittings required to isolate a sample is equivalent to the path length from the root node to the terminating node. This path length, averaged over a forest of such random trees, is a measure of abnormality and our decision function. Random partitioning produces noticeable shorter paths for anomalies. Hence, when a forest of random trees collectively produce shorter path lengths for particular samples, they are highly likely to be anomalies. import numpy as np import matplotlib.pyplot as plt from sklearn.ensemble import IsolationForest rng = np . random . RandomState( 42 ) # Generate train data X = 0.3 * rng . randn( 100 , 2 ) X_train = np . r_[X + 2 , X - 2 ] # Generate some regular novel observations X = 0.3 * rng . randn( 20 , 2 ) X_test = np . r_[X + 2 , X - 2 ] # Generate some abnormal novel observations X_outliers = rng . uniform(low =- 4 , high = 4 , size = ( 20 , 2 )) # fit the model clf = IsolationForest(max_samples = 100 , random_state = rng) clf . fit(X_train) y_pred_train = clf . predict(X_train) y_pred_test = clf . predict(X_test) y_pred_outliers = clf . predict(X_outliers) # plot the line, the samples, and the nearest vectors to the plane xx, yy = np . meshgrid(np . linspace( - 5 , 5 , 50 ), np . linspace( - 5 , 5 , 50 )) Z = clf . decision_function(np . c_[xx . ravel(), yy . ravel()]) Z = Z . reshape(xx . shape) plt . title( \"IsolationForest\" ) plt . contourf(xx, yy, Z, cmap = plt . cm . Blues_r) b1 = plt . scatter(X_train[:, 0 ], X_train[:, 1 ], c = 'white' ) b2 = plt . scatter(X_test[:, 0 ], X_test[:, 1 ], c = 'green' ) c = plt . scatter(X_outliers[:, 0 ], X_outliers[:, 1 ], c = 'red' ) plt . axis( 'tight' ) plt . xlim(( - 5 , 5 )) plt . ylim(( - 5 , 5 )) plt . legend([b1, b2, c], [ \"training observations\" , \"new regular observations\" , \"new abnormal observations\" ], loc = \"upper left\" ) plt . show() 7. Hashing feature transformation using Totally Random Trees ( source ) RandomTreesEmbedding provides a way to map data to a very high-dimensional, sparse representation, which might be beneficial for classification. The mapping is completely unsupervised and very efficient. This example visualizes the partitions given by several trees and shows how the transformation can also be used for non-linear dimensionality reduction or non-linear classification. Points that are neighboring often share the same leaf of a tree and therefore share large parts of their hashed representation. This allows to separate two concentric circles simply based on the principal components of the transformed data. In high-dimensional spaces, linear classifiers often achieve excellent accuracy. For sparse binary data, BernoulliNB is particularly well-suited. The bottom row compares the decision boundary obtained by BernoulliNB in the transformed space with an ExtraTreesClassifier forests learned on the original data. import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_circles from sklearn.ensemble import RandomTreesEmbedding, ExtraTreesClassifier from sklearn.decomposition import TruncatedSVD from sklearn.naive_bayes import BernoulliNB Data # make a synthetic dataset X, y = make_circles(factor = 0.5 , random_state = 0 , noise = 0.05 ) # use RandomTreesEmbedding to transform data hasher = RandomTreesEmbedding(n_estimators = 10 , random_state = 0 , max_depth = 3 ) X_transformed = hasher . fit_transform(X) PCA # Visualize result using PCA pca = TruncatedSVD(n_components = 2 ) X_reduced = pca . fit_transform(X_transformed) Naive Bayes classifier # Learn a Naive Bayes classifier on the transformed data nb = BernoulliNB() nb . fit(X_transformed, y) BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True) ExtraTreesClassifier # Learn an ExtraTreesClassifier for comparison trees = ExtraTreesClassifier(max_depth = 3 , n_estimators = 10 , random_state = 0 ) trees . fit(X, y) ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini', max_depth=3, max_features='auto', max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1, oob_score=False, random_state=0, verbose=0, warm_start=False) Plot # scatter plot of original and reduced data fig = plt . figure(figsize = ( 9 , 8 )) ax = plt . subplot( 221 ) ax . scatter(X[:, 0 ], X[:, 1 ], c = y, s = 50 ) ax . set_title( \"Original Data (2d)\" ) ax . set_xticks(()) ax . set_yticks(()) ax = plt . subplot( 222 ) ax . scatter(X_reduced[:, 0 ], X_reduced[:, 1 ], c = y, s = 50 ) ax . set_title( \"PCA reduction (2d) of transformed data ( %d d)\" % X_transformed . shape[ 1 ]) ax . set_xticks(()) ax . set_yticks(()) # Plot the decision in original space. For that, we will assign a color to each # point in the mesh [x_min, m_max] x [y_min, y_max]. h = . 01 x_min, x_max = X[:, 0 ] . min() - . 5 , X[:, 0 ] . max() + . 5 y_min, y_max = X[:, 1 ] . min() - . 5 , X[:, 1 ] . max() + . 5 xx, yy = np . meshgrid(np . arange(x_min, x_max, h), np . arange(y_min, y_max, h)) # transform grid using RandomTreesEmbedding transformed_grid = hasher . transform(np . c_[xx . ravel(), yy . ravel()]) y_grid_pred = nb . predict_proba(transformed_grid)[:, 1 ] ax = plt . subplot( 223 ) ax . set_title( \"Naive Bayes on Transformed data\" ) ax . pcolormesh(xx, yy, y_grid_pred . reshape(xx . shape)) ax . scatter(X[:, 0 ], X[:, 1 ], c = y, s = 50 ) ax . set_ylim( - 1.4 , 1.4 ) ax . set_xlim( - 1.4 , 1.4 ) ax . set_xticks(()) ax . set_yticks(()) # transform grid using ExtraTreesClassifier y_grid_pred = trees . predict_proba(np . c_[xx . ravel(), yy . ravel()])[:, 1 ] ax = plt . subplot( 224 ) ax . set_title( \"ExtraTrees predictions\" ) ax . pcolormesh(xx, yy, y_grid_pred . reshape(xx . shape)) ax . scatter(X[:, 0 ], X[:, 1 ], c = y, s = 50 ) ax . set_ylim( - 1.4 , 1.4 ) ax . set_xlim( - 1.4 , 1.4 ) ax . set_xticks(()) ax . set_yticks(()) plt . tight_layout() plt . show() 8. Two-class AdaBoost ( source ) This example fits an AdaBoosted decision stump on a non-linearly separable classification dataset composed of two \"Gaussian quantiles\" clusters (see :func: sklearn.datasets.make_gaussian_quantiles ) and plots the decision boundary and decision scores. The distributions of decision scores are shown separately for samples of class A and B. The predicted class label for each sample is determined by the sign of the decision score. Samples with decision scores greater than zero are classified as B, and are otherwise classified as A. The magnitude of a decision score determines the degree of likeness with the predicted class label. Additionally, a new dataset could be constructed containing a desired purity of class B, for example, by only selecting samples with a decision score above some value. import numpy as np import matplotlib.pyplot as plt from sklearn.ensemble import AdaBoostClassifier from sklearn.tree import DecisionTreeClassifier from sklearn.datasets import make_gaussian_quantiles Data # Construct dataset X1, y1 = make_gaussian_quantiles(cov = 2. , n_samples = 200 , n_features = 2 , n_classes = 2 , random_state = 1 ) X2, y2 = make_gaussian_quantiles(mean = ( 3 , 3 ), cov = 1.5 , n_samples = 300 , n_features = 2 , n_classes = 2 , random_state = 1 ) X = np . concatenate((X1, X2)) y = np . concatenate((y1, - y2 + 1 )) Model # Create and fit an AdaBoosted decision tree bdt = AdaBoostClassifier(DecisionTreeClassifier(max_depth = 1 ), algorithm = \"SAMME\" , n_estimators = 200 ) bdt . fit(X, y) AdaBoostClassifier(algorithm='SAMME', base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1, max_features=None, max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=None, splitter='best'), learning_rate=1.0, n_estimators=200, random_state=None) Plot plot_colors = \"br\" plot_step = 0.02 class_names = \"AB\" plt . figure(figsize = ( 10 , 5 )) # Plot the decision boundaries plt . subplot( 121 ) x_min, x_max = X[:, 0 ] . min() - 1 , X[:, 0 ] . max() + 1 y_min, y_max = X[:, 1 ] . min() - 1 , X[:, 1 ] . max() + 1 xx, yy = np . meshgrid(np . arange(x_min, x_max, plot_step), np . arange(y_min, y_max, plot_step)) Z = bdt . predict(np . c_[xx . ravel(), yy . ravel()]) Z = Z . reshape(xx . shape) cs = plt . contourf(xx, yy, Z, cmap = plt . cm . Paired) plt . axis( \"tight\" ) # Plot the training points for i, n, c in zip ( range ( 2 ), class_names, plot_colors): idx = np . where(y == i) plt . scatter(X[idx, 0 ], X[idx, 1 ], c = c, cmap = plt . cm . Paired, label = \"Class %s \" % n) plt . xlim(x_min, x_max) plt . ylim(y_min, y_max) plt . legend(loc = 'upper right' ) plt . xlabel( 'x' ) plt . ylabel( 'y' ) plt . title( 'Decision Boundary' ) # Plot the two-class decision scores twoclass_output = bdt . decision_function(X) plot_range = (twoclass_output . min(), twoclass_output . max()) plt . subplot( 122 ) for i, n, c in zip ( range ( 2 ), class_names, plot_colors): plt . hist(twoclass_output[y == i], bins = 10 , range = plot_range, facecolor = c, label = 'Class %s ' % n, alpha =. 5 ) x1, x2, y1, y2 = plt . axis() plt . axis((x1, x2, y1, y2 * 1.2 )) plt . legend(loc = 'upper right' ) plt . ylabel( 'Samples' ) plt . xlabel( 'Score' ) plt . title( 'Decision Scores' ) plt . tight_layout() plt . subplots_adjust(wspace = 0.35 ) plt . show() 9. Multi-class AdaBoosted Decision Trees ( source ) This example reproduces Figure 1 of Zhu et al [1] and shows how boosting can improve prediction accuracy on a multi-class problem. The classification dataset is constructed by taking a ten-dimensional standard normal distribution and defining three classes separated by nested concentric ten-dimensional spheres such that roughly equal numbers of samples are in each class (quantiles of the :math: \\chi^2 distribution). The performance of the SAMME and SAMME.R [1] algorithms are compared. SAMME.R uses the probability estimates to update the additive model, while SAMME uses the classifications only. As the example illustrates, the SAMME.R algorithm typically converges faster than SAMME, achieving a lower test error with fewer boosting iterations. The error of each algorithm on the test set after each boosting iteration is shown on the left, the classification error on the test set of each tree is shown in the middle, and the boost weight of each tree is shown on the right. All trees have a weight of one in the SAMME.R algorithm and therefore are not shown. from sklearn.externals.six.moves import zip import matplotlib.pyplot as plt from sklearn.datasets import make_gaussian_quantiles from sklearn.ensemble import AdaBoostClassifier from sklearn.metrics import accuracy_score from sklearn.tree import DecisionTreeClassifier Data X, y = make_gaussian_quantiles(n_samples = 13000 , n_features = 10 , n_classes = 3 , random_state = 1 ) n_split = 3000 X_train, X_test = X[:n_split], X[n_split:] y_train, y_test = y[:n_split], y[n_split:] Model bdt_real = AdaBoostClassifier( DecisionTreeClassifier(max_depth = 2 ), n_estimators = 600 , learning_rate = 1 ) bdt_discrete = AdaBoostClassifier( DecisionTreeClassifier(max_depth = 2 ), n_estimators = 600 , learning_rate = 1.5 , algorithm = \"SAMME\" ) bdt_real . fit(X_train, y_train) bdt_discrete . fit(X_train, y_train) real_test_errors = [] discrete_test_errors = [] for real_test_predict, discrete_train_predict in zip ( bdt_real . staged_predict(X_test), bdt_discrete . staged_predict(X_test)): real_test_errors . append( 1. - accuracy_score(real_test_predict, y_test)) discrete_test_errors . append( 1. - accuracy_score(discrete_train_predict, y_test)) n_trees_discrete = len (bdt_discrete) n_trees_real = len (bdt_real) # Boosting might terminate early, but the following arrays are always # n_estimators long. We crop them to the actual number of trees here: discrete_estimator_errors = bdt_discrete . estimator_errors_[:n_trees_discrete] real_estimator_errors = bdt_real . estimator_errors_[:n_trees_real] discrete_estimator_weights = bdt_discrete . estimator_weights_[:n_trees_discrete] Plot plt . figure(figsize = ( 15 , 5 )) plt . subplot( 131 ) plt . plot( range ( 1 , n_trees_discrete + 1 ), discrete_test_errors, c = 'black' , label = 'SAMME' ) plt . plot( range ( 1 , n_trees_real + 1 ), real_test_errors, c = 'black' , linestyle = 'dashed' , label = 'SAMME.R' ) plt . legend() plt . ylim( 0.18 , 0.62 ) plt . ylabel( 'Test Error' ) plt . xlabel( 'Number of Trees' ) plt . subplot( 132 ) plt . plot( range ( 1 , n_trees_discrete + 1 ), discrete_estimator_errors, \"b\" , label = 'SAMME' , alpha =. 5 ) plt . plot( range ( 1 , n_trees_real + 1 ), real_estimator_errors, \"r\" , label = 'SAMME.R' , alpha =. 5 ) plt . legend() plt . ylabel( 'Error' ) plt . xlabel( 'Number of Trees' ) plt . ylim(( . 2 , max (real_estimator_errors . max(), discrete_estimator_errors . max()) * 1.2 )) plt . xlim(( - 20 , len (bdt_discrete) + 20 )) plt . subplot( 133 ) plt . plot( range ( 1 , n_trees_discrete + 1 ), discrete_estimator_weights, \"b\" , label = 'SAMME' ) plt . legend() plt . ylabel( 'Weight' ) plt . xlabel( 'Number of Trees' ) plt . ylim(( 0 , discrete_estimator_weights . max() * 1.2 )) plt . xlim(( - 20 , n_trees_discrete + 20 )) # prevent overlapping y-axis labels plt . subplots_adjust(wspace = 0.25 ) plt . show() 10 .Discrete versus Real AdaBoost ( source ) This example is based on Figure 10.2 from Hastie et al 2009 [1] and illustrates the difference in performance between the discrete SAMME [2] boosting algorithm and real SAMME.R boosting algorithm. Both algorithms are evaluated on a binary classification task where the target Y is a non-linear function of 10 input features. Discrete SAMME AdaBoost adapts based on errors in predicted class labels whereas real SAMME.R uses the predicted class probabilities. import numpy as np import matplotlib.pyplot as plt from sklearn import datasets from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import zero_one_loss from sklearn.ensemble import AdaBoostClassifier Data n_estimators = 400 # A learning rate of 1. may not be optimal for both SAMME and SAMME.R learning_rate = 1. X, y = datasets . make_hastie_10_2(n_samples = 12000 , random_state = 1 ) X_test, y_test = X[ 2000 :], y[ 2000 :] X_train, y_train = X[: 2000 ], y[: 2000 ] Model dt_stump = DecisionTreeClassifier(max_depth = 1 , min_samples_leaf = 1 ) dt_stump . fit(X_train, y_train) dt_stump_err = 1.0 - dt_stump . score(X_test, y_test) dt = DecisionTreeClassifier(max_depth = 9 , min_samples_leaf = 1 ) dt . fit(X_train, y_train) dt_err = 1.0 - dt . score(X_test, y_test) ada_discrete = AdaBoostClassifier( base_estimator = dt_stump, learning_rate = learning_rate, n_estimators = n_estimators, algorithm = \"SAMME\" ) ada_discrete . fit(X_train, y_train) ada_real = AdaBoostClassifier( base_estimator = dt_stump, learning_rate = learning_rate, n_estimators = n_estimators, algorithm = \"SAMME.R\" ) ada_real . fit(X_train, y_train) AdaBoostClassifier(algorithm='SAMME.R', base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1, max_features=None, max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=None, splitter='best'), learning_rate=1.0, n_estimators=400, random_state=None) Plot fig = plt . figure() ax = fig . add_subplot( 111 ) ax . plot([ 1 , n_estimators], [dt_stump_err] * 2 , 'k-' , label = 'Decision Stump Error' ) ax . plot([ 1 , n_estimators], [dt_err] * 2 , 'k--' , label = 'Decision Tree Error' ) ada_discrete_err = np . zeros((n_estimators,)) for i, y_pred in enumerate (ada_discrete . staged_predict(X_test)): ada_discrete_err[i] = zero_one_loss(y_pred, y_test) ada_discrete_err_train = np . zeros((n_estimators,)) for i, y_pred in enumerate (ada_discrete . staged_predict(X_train)): ada_discrete_err_train[i] = zero_one_loss(y_pred, y_train) ada_real_err = np . zeros((n_estimators,)) for i, y_pred in enumerate (ada_real . staged_predict(X_test)): ada_real_err[i] = zero_one_loss(y_pred, y_test) ada_real_err_train = np . zeros((n_estimators,)) for i, y_pred in enumerate (ada_real . staged_predict(X_train)): ada_real_err_train[i] = zero_one_loss(y_pred, y_train) ax . plot(np . arange(n_estimators) + 1 , ada_discrete_err, label = 'Discrete AdaBoost Test Error' , color = 'red' ) ax . plot(np . arange(n_estimators) + 1 , ada_discrete_err_train, label = 'Discrete AdaBoost Train Error' , color = 'blue' ) ax . plot(np . arange(n_estimators) + 1 , ada_real_err, label = 'Real AdaBoost Test Error' , color = 'orange' ) ax . plot(np . arange(n_estimators) + 1 , ada_real_err_train, label = 'Real AdaBoost Train Error' , color = 'green' ) ax . set_ylim(( 0.0 , 0.5 )) ax . set_xlabel( 'n_estimators' ) ax . set_ylabel( 'error rate' ) leg = ax . legend(loc = 'upper right' , fancybox = True ) leg . get_frame() . set_alpha( 0.7 ) plt . show() 11. Decision Tree Regression with AdaBoost ( source ) A decision tree is boosted using the AdaBoost.R2 [1] algorithm on a 1D sinusoidal dataset with a small amount of Gaussian noise. 299 boosts (300 decision trees) is compared with a single decision tree regressor. As the number of boosts is increased the regressor can fit more detail. # importing necessary libraries import numpy as np import matplotlib.pyplot as plt from sklearn.tree import DecisionTreeRegressor from sklearn.ensemble import AdaBoostRegressor Data # Create the dataset rng = np . random . RandomState( 1 ) X = np . linspace( 0 , 6 , 100 )[:, np . newaxis] y = np . sin(X) . ravel() + np . sin( 6 * X) . ravel() + rng . normal( 0 , 0.1 , X . shape[ 0 ]) Model # Fit regression model regr_1 = DecisionTreeRegressor(max_depth = 4 ) regr_2 = AdaBoostRegressor(DecisionTreeRegressor(max_depth = 4 ), n_estimators = 300 , random_state = rng) regr_1 . fit(X, y) regr_2 . fit(X, y) AdaBoostRegressor(base_estimator=DecisionTreeRegressor(criterion='mse', max_depth=4, max_features=None, max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=None, splitter='best'), learning_rate=1.0, loss='linear', n_estimators=300, random_state=&lt;mtrand.RandomState object at 0x114302d90&gt;) Predict # Predict y_1 = regr_1 . predict(X) y_2 = regr_2 . predict(X) Plot # Plot the results plt . figure() plt . scatter(X, y, c = \"k\" , label = \"training samples\" ) plt . plot(X, y_1, c = \"g\" , label = \"n_estimators=1\" , linewidth = 2 ) plt . plot(X, y_2, c = \"r\" , label = \"n_estimators=300\" , linewidth = 2 ) plt . xlabel( \"data\" ) plt . ylabel( \"target\" ) plt . title( \"Boosted Decision Tree Regression\" ) plt . legend() plt . show() 12. Gradient Boosting Out-of-Bag estimates ( source ) Out-of-bag (OOB) estimates can be a useful heuristic to estimate the \"optimal\" number of boosting iterations. OOB estimates are almost identical to cross-validation estimates but they can be computed on-the-fly without the need for repeated model fitting. OOB estimates are only available for Stochastic Gradient Boosting (i.e. subsample < 1.0 ), the estimates are derived from the improvement in loss based on the examples not included in the bootstrap sample (the so-called out-of-bag examples). The OOB estimator is a pessimistic estimator of the true test loss, but remains a fairly good approximation for a small number of trees. The figure shows the cumulative sum of the negative OOB improvements as a function of the boosting iteration. As you can see, it tracks the test loss for the first hundred iterations but then diverges in a pessimistic way. The figure also shows the performance of 3-fold cross validation which usually gives a better estimate of the test loss but is computationally more demanding. import numpy as np import matplotlib.pyplot as plt from sklearn import ensemble from sklearn.cross_validation import KFold from sklearn.cross_validation import train_test_split /home/bitnami/anaconda3/envs/caseolap/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20. \"This module will be removed in 0.20.\", DeprecationWarning) Data # Generate data (adapted from G. Ridgeway's gbm example) n_samples = 1000 random_state = np . random . RandomState( 13 ) x1 = random_state . uniform(size = n_samples) x2 = random_state . uniform(size = n_samples) x3 = random_state . randint( 0 , 4 , size = n_samples) p = 1 / ( 1.0 + np . exp( - (np . sin( 3 * x1) - 4 * x2 + x3))) y = random_state . binomial( 1 , p, size = n_samples) X = np . c_[x1, x2, x3] X = X . astype(np . float32) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5 , random_state = 9 ) Model # Fit classifier with out-of-bag estimates params = { 'n_estimators' : 1200 , 'max_depth' : 3 , 'subsample' : 0.5 , 'learning_rate' : 0.01 , 'min_samples_leaf' : 1 , 'random_state' : 3 } clf = ensemble . GradientBoostingClassifier( ** params) clf . fit(X_train, y_train) acc = clf . score(X_test, y_test) print ( \"Accuracy: {:.4f}\" . format(acc)) n_estimators = params[ 'n_estimators' ] x = np . arange(n_estimators) + 1 def heldout_score (clf, X_test, y_test): \"\"\"compute deviance scores on ``X_test`` and ``y_test``. \"\"\" score = np . zeros((n_estimators,), dtype = np . float64) for i, y_pred in enumerate (clf . staged_decision_function(X_test)): score[i] = clf . loss_(y_test, y_pred) return score def cv_estimate (n_folds = 3 ): cv = KFold(n_folds = n_folds) cv_clf = ensemble . GradientBoostingClassifier( ** params) val_scores = np . zeros((n_estimators,), dtype = np . float64) for train, test in cv . split(X_train, y_train): cv_clf . fit(X_train[train], y_train[train]) val_scores += heldout_score(cv_clf, X_train[test], y_train[test]) val_scores /= n_folds return val_scores # Estimate best n_estimator using cross-validation cv_score = cv_estimate( 3 ) # Compute best n_estimator for test data test_score = heldout_score(clf, X_test, y_test) # negative cumulative sum of oob improvements cumsum = - np . cumsum(clf . oob_improvement_) # min loss according to OOB oob_best_iter = x[np . argmin(cumsum)] # min loss according to test (normalize such that first loss is 0) test_score -= test_score[ 0 ] test_best_iter = x[np . argmin(test_score)] # min loss according to cv (normalize such that first loss is 0) cv_score -= cv_score[ 0 ] cv_best_iter = x[np . argmin(cv_score)] Accuracy: 0.6840 --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-6-331f159e761d&gt; in &lt;module&gt;() 32 33 # Estimate best n_estimator using cross-validation ---&gt; 34 cv_score = cv_estimate(3) 35 36 # Compute best n_estimator for test data &lt;ipython-input-6-331f159e761d&gt; in cv_estimate(n_folds) 21 22 def cv_estimate(n_folds=3): ---&gt; 23 cv = KFold(n_folds=n_folds) 24 cv_clf = ensemble.GradientBoostingClassifier(**params) 25 val_scores = np.zeros((n_estimators,), dtype=np.float64) TypeError: __init__() missing 1 required positional argument: 'n' Plot # color brew for the three curves oob_color = list ( map ( lambda x: x / 256.0 , ( 190 , 174 , 212 ))) test_color = list ( map ( lambda x: x / 256.0 , ( 127 , 201 , 127 ))) cv_color = list ( map ( lambda x: x / 256.0 , ( 253 , 192 , 134 ))) # plot curves and vertical lines for best iterations plt . plot(x, cumsum, label = 'OOB loss' , color = oob_color) plt . plot(x, test_score, label = 'Test loss' , color = test_color) plt . plot(x, cv_score, label = 'CV loss' , color = cv_color) plt . axvline(x = oob_best_iter, color = oob_color) plt . axvline(x = test_best_iter, color = test_color) plt . axvline(x = cv_best_iter, color = cv_color) # add three vertical lines to xticks xticks = plt . xticks() xticks_pos = np . array(xticks[ 0 ] . tolist() + [oob_best_iter, cv_best_iter, test_best_iter]) xticks_label = np . array( list ( map ( lambda t: int (t), xticks[ 0 ])) + [ 'OOB' , 'CV' , 'Test' ]) ind = np . argsort(xticks_pos) xticks_pos = xticks_pos[ind] xticks_label = xticks_label[ind] plt . xticks(xticks_pos, xticks_label) plt . legend(loc = 'upper right' ) plt . ylabel( 'normalized loss' ) plt . xlabel( 'number of iterations' ) plt . show() --------------------------------------------------------------------------- NameError Traceback (most recent call last) &lt;ipython-input-7-3dcde55ed4f9&gt; in &lt;module&gt;() 6 7 # plot curves and vertical lines for best iterations ----&gt; 8 plt.plot(x, cumsum, label='OOB loss', color=oob_color) 9 plt.plot(x, test_score, label='Test loss', color=test_color) 10 plt.plot(x, cv_score, label='CV loss', color=cv_color) NameError: name 'cumsum' is not defined 13. Prediction Intervals for Gradient Boosting Regression ( source ) This example shows how quantile regression can be used to create prediction intervals. import numpy as np import matplotlib.pyplot as plt from sklearn.ensemble import GradientBoostingRegressor Data np . random . seed( 1 ) def f (x): \"\"\"The function to predict.\"\"\" return x * np . sin(x) #---------------------------------------------------------------------- # First the noiseless case X = np . atleast_2d(np . random . uniform( 0 , 10.0 , size = 100 )) . T X = X . astype(np . float32) # Observations y = f(X) . ravel() dy = 1.5 + 1.0 * np . random . random(y . shape) noise = np . random . normal( 0 , dy) y += noise y = y . astype(np . float32) # Mesh the input space for evaluations of the real function, the prediction and # its MSE xx = np . atleast_2d(np . linspace( 0 , 10 , 1000 )) . T xx = xx . astype(np . float32) alpha = 0.95 Model clf = GradientBoostingRegressor(loss = 'quantile' , alpha = alpha, n_estimators = 250 , max_depth = 3 , learning_rate =. 1 , min_samples_leaf = 9 , min_samples_split = 9 ) clf . fit(X, y) # Make the prediction on the meshed x-axis y_upper = clf . predict(xx) clf . set_params(alpha = 1.0 - alpha) clf . fit(X, y) # Make the prediction on the meshed x-axis y_lower = clf . predict(xx) clf . set_params(loss = 'ls' ) clf . fit(X, y) # Make the prediction on the meshed x-axis y_pred = clf . predict(xx) Plot # Plot the function, the prediction and the 90% confidence interval based on # the MSE fig = plt . figure() plt . plot(xx, f(xx), 'g:' , label = u'$f(x) = x\\,\\sin(x)$' ) plt . plot(X, y, 'b.' , markersize = 10 , label = u'Observations' ) plt . plot(xx, y_pred, 'r-' , label = u'Prediction' ) plt . plot(xx, y_upper, 'k-' ) plt . plot(xx, y_lower, 'k-' ) plt . fill(np . concatenate([xx, xx[:: - 1 ]]), np . concatenate([y_upper, y_lower[:: - 1 ]]), alpha =. 5 , fc = 'b' , ec = 'None' , label = '90% prediction interval' ) plt . xlabel( '$x$' ) plt . ylabel( '$f(x)$' ) plt . ylim( - 10 , 20 ) plt . legend(loc = 'upper left' ) plt . show() 14. Gradient Boosting regression ( source ) Demonstrate Gradient Boosting on the Boston housing dataset. This example fits a Gradient Boosting model with least squares loss and 500 regression trees of depth 4. import numpy as np import matplotlib.pyplot as plt from sklearn import ensemble from sklearn import datasets from sklearn.utils import shuffle from sklearn.metrics import mean_squared_error Data # Load data boston = datasets . load_boston() X, y = shuffle(boston . data, boston . target, random_state = 13 ) X = X . astype(np . float32) offset = int (X . shape[ 0 ] * 0.9 ) X_train, y_train = X[:offset], y[:offset] X_test, y_test = X[offset:], y[offset:] Model # Fit regression model params = { 'n_estimators' : 500 , 'max_depth' : 4 , 'min_samples_split' : 2 , 'learning_rate' : 0.01 , 'loss' : 'ls' } clf = ensemble . GradientBoostingRegressor( ** params) clf . fit(X_train, y_train) mse = mean_squared_error(y_test, clf . predict(X_test)) print ( \"MSE: %.4f \" % mse) MSE: 6.6213 Plot training deviance # Plot training deviance # compute test set deviance test_score = np . zeros((params[ 'n_estimators' ],), dtype = np . float64) for i, y_pred in enumerate (clf . staged_predict(X_test)): test_score[i] = clf . loss_(y_test, y_pred) plt . figure(figsize = ( 12 , 6 )) plt . subplot( 1 , 2 , 1 ) plt . title( 'Deviance' ) plt . plot(np . arange(params[ 'n_estimators' ]) + 1 , clf . train_score_, 'b-' , label = 'Training Set Deviance' ) plt . plot(np . arange(params[ 'n_estimators' ]) + 1 , test_score, 'r-' , label = 'Test Set Deviance' ) plt . legend(loc = 'upper right' ) plt . xlabel( 'Boosting Iterations' ) plt . ylabel( 'Deviance' ) plt . show() Plot # Plot feature importance feature_importance = clf . feature_importances_ # make importances relative to max importance feature_importance = 100.0 * (feature_importance / feature_importance . max()) sorted_idx = np . argsort(feature_importance) pos = np . arange(sorted_idx . shape[ 0 ]) + . 5 plt . subplot( 1 , 2 , 2 ) plt . barh(pos, feature_importance[sorted_idx], align = 'center' ) plt . yticks(pos, boston . feature_names[sorted_idx]) plt . xlabel( 'Relative Importance' ) plt . title( 'Variable Importance' ) plt . show() 15. Gradient Boosting regularization ( source ) Illustration of the effect of different regularization strategies for Gradient Boosting. The example is taken from Hastie et al 2009. The loss function used is binomial deviance. Regularization via shrinkage ( learning_rate < 1.0 ) improves performance considerably. In combination with shrinkage, stochastic gradient boosting ( subsample < 1.0 ) can produce more accurate models by reducing the variance via bagging. Subsampling without shrinkage usually does poorly. Another strategy to reduce the variance is by subsampling the features analogous to the random splits in Random Forests (via the max_features parameter). import numpy as np import matplotlib.pyplot as plt from sklearn import ensemble from sklearn import datasets Data X, y = datasets . make_hastie_10_2(n_samples = 12000 , random_state = 1 ) X = X . astype(np . float32) # map labels from {-1, 1} to {0, 1} labels, y = np . unique(y, return_inverse = True ) X_train, X_test = X[: 2000 ], X[ 2000 :] y_train, y_test = y[: 2000 ], y[ 2000 :] original_params = { 'n_estimators' : 1000 , 'max_leaf_nodes' : 4 , 'max_depth' : None , 'random_state' : 2 , 'min_samples_split' : 5 } Plot n Model plt . figure() for label, color, setting in [( 'No shrinkage' , 'orange' , { 'learning_rate' : 1.0 , 'subsample' : 1.0 }), ( 'learning_rate=0.1' , 'turquoise' , { 'learning_rate' : 0.1 , 'subsample' : 1.0 }), ( 'subsample=0.5' , 'blue' , { 'learning_rate' : 1.0 , 'subsample' : 0.5 }), ( 'learning_rate=0.1, subsample=0.5' , 'gray' , { 'learning_rate' : 0.1 , 'subsample' : 0.5 }), ( 'learning_rate=0.1, max_features=2' , 'magenta' , { 'learning_rate' : 0.1 , 'max_features' : 2 })]: params = dict (original_params) params . update(setting) clf = ensemble . GradientBoostingClassifier( ** params) clf . fit(X_train, y_train) # compute test set deviance test_deviance = np . zeros((params[ 'n_estimators' ],), dtype = np . float64) for i, y_pred in enumerate (clf . staged_decision_function(X_test)): # clf.loss_ assumes that y_test[i] in {0, 1} test_deviance[i] = clf . loss_(y_test, y_pred) plt . plot((np . arange(test_deviance . shape[ 0 ]) + 1 )[:: 5 ], test_deviance[:: 5 ], '-' , color = color, label = label) plt . legend(loc = 'upper left' ) plt . xlabel( 'Boosting Iterations' ) plt . ylabel( 'Test Set Deviance' ) plt . show() 16. Partial Dependence Plots ( source ) Partial dependence plots show the dependence between the target function [2]_ and a set of 'target' features, marginalizing over the values of all other features (the complement features). Due to the limits of human perception the size of the target feature set must be small (usually, one or two) thus the target features are usually chosen among the most important features (see :attr: ~sklearn.ensemble.GradientBoostingRegressor.feature_importances_ ). This example shows how to obtain partial dependence plots from a :class: ~sklearn.ensemble.GradientBoostingRegressor trained on the California housing dataset. The example is taken from [1]_. The plot shows four one-way and one two-way partial dependence plots. The target variables for the one-way PDP are: median income ( MedInc ), avg. occupants per household ( AvgOccup ), median house age ( HouseAge ), and avg. rooms per household ( AveRooms ). We can clearly see that the median house price shows a linear relationship with the median income (top left) and that the house price drops when the avg. occupants per household increases (top middle). The top right plot shows that the house age in a district does not have a strong influence on the (median) house price; so does the average rooms per household. The tick marks on the x-axis represent the deciles of the feature values in the training data. Partial dependence plots with two target features enable us to visualize interactions among them. The two-way partial dependence plot shows the dependence of median house price on joint values of house age and avg. occupants per household. We can clearly see an interaction between the two features: For an avg. occupancy greater than two, the house price is nearly independent of the house age, whereas for values less than two there is a strong dependence on age. from __future__ import print_function import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D from sklearn.cross_validation import train_test_split from sklearn.ensemble import GradientBoostingRegressor from sklearn.ensemble.partial_dependence import plot_partial_dependence from sklearn.ensemble.partial_dependence import partial_dependence from sklearn.datasets.california_housing import fetch_california_housing Alltogether def main (): cal_housing = fetch_california_housing() # split 80/20 train-test X_train, X_test, y_train, y_test = train_test_split(cal_housing . data, cal_housing . target, test_size = 0.2 , random_state = 1 ) names = cal_housing . feature_names print ( \"Training GBRT...\" , flush = True , end = '' ) clf = GradientBoostingRegressor(n_estimators = 100 , max_depth = 4 , learning_rate = 0.1 , loss = 'huber' , random_state = 1 ) clf . fit(X_train, y_train) print ( \" done.\" ) print ( 'Convenience plot with ``partial_dependence_plots``' ) features = [ 0 , 5 , 1 , 2 , ( 5 , 1 )] fig, axs = plot_partial_dependence(clf, X_train, features, feature_names = names, n_jobs = 3 , grid_resolution = 50 ) fig . suptitle( 'Partial dependence of house value on nonlocation features \\n ' 'for the California housing dataset' ) plt . subplots_adjust(top = 0.9 ) # tight_layout causes overlap with suptitle print ( 'Custom 3d plot via ``partial_dependence``' ) fig = plt . figure() target_feature = ( 1 , 5 ) pdp, axes = partial_dependence(clf, target_feature, X = X_train, grid_resolution = 50 ) XX, YY = np . meshgrid(axes[ 0 ], axes[ 1 ]) Z = pdp[ 0 ] . reshape( list ( map (np . size, axes))) . T ax = Axes3D(fig) surf = ax . plot_surface(XX, YY, Z, rstride = 1 , cstride = 1 , cmap = plt . cm . BuPu) ax . set_xlabel(names[target_feature[ 0 ]]) ax . set_ylabel(names[target_feature[ 1 ]]) ax . set_zlabel( 'Partial dependence' ) # pretty init view ax . view_init(elev = 22 , azim = 122 ) plt . colorbar(surf) plt . suptitle( 'Partial dependence of house value on median age and ' 'average occupancy' ) plt . subplots_adjust(top = 0.9 ) plt . show() # Needed on Windows because plot_partial_dependence uses multiprocessing if __name__ == '__main__' : main() Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /home/bitnami/scikit_learn_data Training GBRT... done. Convenience plot with ``partial_dependence_plots`` Custom 3d plot via ``partial_dependence`` 17. Plot the decision boundaries of a VotingClassifier ( source ) Plot the decision boundaries of a VotingClassifier for two features of the Iris dataset. Plot the class probabilities of the first sample in a toy dataset predicted by three different classifiers and averaged by the VotingClassifier . First, three exemplary classifiers are initialized ( DecisionTreeClassifier , KNeighborsClassifier , and SVC ) and used to initialize a soft-voting VotingClassifier with weights [2, 1, 2] , which means that the predicted probabilities of the DecisionTreeClassifier and SVC count 5 times as much as the weights of the KNeighborsClassifier classifier when the averaged probability is calculated. from itertools import product import numpy as np import matplotlib.pyplot as plt from sklearn import datasets from sklearn.tree import DecisionTreeClassifier from sklearn.neighbors import KNeighborsClassifier from sklearn.svm import SVC from sklearn.ensemble import VotingClassifier Data # Loading some example data iris = datasets . load_iris() X = iris . data[:, [ 0 , 2 ]] y = iris . target Model # Training classifiers clf1 = DecisionTreeClassifier(max_depth = 4 ) clf2 = KNeighborsClassifier(n_neighbors = 7 ) clf3 = SVC(kernel = 'rbf' , probability = True ) eclf = VotingClassifier(estimators = [( 'dt' , clf1), ( 'knn' , clf2), ( 'svc' , clf3)], voting = 'soft' , weights = [ 2 , 1 , 2 ]) clf1 . fit(X, y) clf2 . fit(X, y) clf3 . fit(X, y) eclf . fit(X, y) VotingClassifier(estimators=[('dt', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4, max_features=None, max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=None, splitter='best')), ('knn', ...', max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001, verbose=False))], voting='soft', weights=[2, 1, 2]) Plot # Plotting decision regions x_min, x_max = X[:, 0 ] . min() - 1 , X[:, 0 ] . max() + 1 y_min, y_max = X[:, 1 ] . min() - 1 , X[:, 1 ] . max() + 1 xx, yy = np . meshgrid(np . arange(x_min, x_max, 0.1 ), np . arange(y_min, y_max, 0.1 )) f, axarr = plt . subplots( 2 , 2 , sharex = 'col' , sharey = 'row' , figsize = ( 10 , 8 )) for idx, clf, tt in zip (product([ 0 , 1 ], [ 0 , 1 ]), [clf1, clf2, clf3, eclf], [ 'Decision Tree (depth=4)' , 'KNN (k=7)' , 'Kernel SVM' , 'Soft Voting' ]): Z = clf . predict(np . c_[xx . ravel(), yy . ravel()]) Z = Z . reshape(xx . shape) axarr[idx[ 0 ], idx[ 1 ]] . contourf(xx, yy, Z, alpha = 0.4 ) axarr[idx[ 0 ], idx[ 1 ]] . scatter(X[:, 0 ], X[:, 1 ], c = y, alpha = 0.8 ) axarr[idx[ 0 ], idx[ 1 ]] . set_title(tt) plt . show() 18. Plot class probabilities calculated by the VotingClassifier ( source ) Plot the class probabilities of the first sample in a toy dataset predicted by three different classifiers and averaged by the VotingClassifier . First, three examplary classifiers are initialized ( LogisticRegression , GaussianNB , and RandomForestClassifier ) and used to initialize a soft-voting VotingClassifier with weights [1, 1, 5] , which means that the predicted probabilities of the RandomForestClassifier count 5 times as much as the weights of the other classifiers when the averaged probability is calculated. To visualize the probability weighting, we fit each classifier on the training set and plot the predicted class probabilities for the first sample in this example dataset. import numpy as np import matplotlib.pyplot as plt from sklearn.linear_model import LogisticRegression from sklearn.naive_bayes import GaussianNB from sklearn.ensemble import RandomForestClassifier from sklearn.ensemble import VotingClassifier Model clf1 = LogisticRegression(random_state = 123 ) clf2 = RandomForestClassifier(random_state = 123 ) clf3 = GaussianNB() X = np . array([[ - 1.0 , - 1.0 ], [ - 1.2 , - 1.4 ], [ - 3.4 , - 2.2 ], [ 1.1 , 1.2 ]]) y = np . array([ 1 , 1 , 2 , 2 ]) eclf = VotingClassifier(estimators = [( 'lr' , clf1), ( 'rf' , clf2), ( 'gnb' , clf3)], voting = 'soft' , weights = [ 1 , 1 , 5 ]) Predict # predict class probabilities for all classifiers probas = [c . fit(X, y) . predict_proba(X) for c in (clf1, clf2, clf3, eclf)] # get class probabilities for the first sample in the dataset class1_1 = [pr[ 0 , 0 ] for pr in probas] class2_1 = [pr[ 0 , 1 ] for pr in probas] Plot # plotting N = 4 # number of groups ind = np . arange(N) # group positions width = 0.35 # bar width fig, ax = plt . subplots() # bars for classifier 1-3 p1 = ax . bar(ind, np . hstack(([class1_1[: - 1 ], [ 0 ]])), width, color = 'green' ) p2 = ax . bar(ind + width, np . hstack(([class2_1[: - 1 ], [ 0 ]])), width, color = 'lightgreen' ) # bars for VotingClassifier p3 = ax . bar(ind, [ 0 , 0 , 0 , class1_1[ - 1 ]], width, color = 'blue' ) p4 = ax . bar(ind + width, [ 0 , 0 , 0 , class2_1[ - 1 ]], width, color = 'steelblue' ) # plot annotations plt . axvline( 2.8 , color = 'k' , linestyle = 'dashed' ) ax . set_xticks(ind + width) ax . set_xticklabels([ 'LogisticRegression \\n weight 1' , 'GaussianNB \\n weight 1' , 'RandomForestClassifier \\n weight 5' , 'VotingClassifier \\n (average probabilities)' ], rotation = 40 , ha = 'right' ) plt . ylim([ 0 , 1 ]) plt . title( 'Class probabilities for sample 1 by different classifiers' ) plt . legend([p1[ 0 ], p2[ 0 ]], [ 'class 1' , 'class 2' ], loc = 'upper left' ) plt . show()","title":"Ensamble"},{"location":"Classifiers/Ensamble/ensamble/#ensamble","text":"The goal of ensemble methods is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator. Two families of ensemble methods are usually distinguished: In averaging methods , the driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced. Examples: - Bagging methods, - Forests of randomized trees, ... By contrast, in boosting methods , base estimators are built sequentially and one tries to reduce the bias of the combined estimator. The motivation is to combine several weak models to produce a powerful ensemble. Examples: - AdaBoost, - Gradient Tree Boosting, ... % matplotlib inline","title":"Ensamble"},{"location":"Classifiers/Ensamble/ensamble/#1-single-estimator-versus-bagging-bias-variance-decomposition-source","text":"This example illustrates and compares the bias-variance decomposition of the expected mean squared error of a single estimator against a bagging ensemble. In regression, the expected mean squared error of an estimator can be decomposed in terms of bias, variance and noise. On average over datasets of the regression problem, the bias term measures the average amount by which the predictions of the estimator differ from the predictions of the best possible estimator for the problem (i.e., the Bayes model). The variance term measures the variability of the predictions of the estimator when fit over different instances LS of the problem. Finally, the noise measures the irreducible part of the error which is due the variability in the data. The upper left figure illustrates the predictions (in dark red) of a single decision tree trained over a random dataset LS (the blue dots) of a toy 1d regression problem. It also illustrates the predictions (in light red) of other single decision trees trained over other (and different) randomly drawn instances LS of the problem. Intuitively, the variance term here corresponds to the width of the beam of predictions (in light red) of the individual estimators. The larger the variance, the more sensitive are the predictions for x to small changes in the training set. The bias term corresponds to the difference between the average prediction of the estimator (in cyan) and the best possible model (in dark blue). On this problem, we can thus observe that the bias is quite low (both the cyan and the blue curves are close to each other) while the variance is large (the red beam is rather wide). The lower left figure plots the pointwise decomposition of the expected mean squared error of a single decision tree. It confirms that the bias term (in blue) is low while the variance is large (in green). It also illustrates the noise part of the error which, as expected, appears to be constant and around 0.01 . The right figures correspond to the same plots but using instead a bagging ensemble of decision trees. In both figures, we can observe that the bias term is larger than in the previous case. In the upper right figure, the difference between the average prediction (in cyan) and the best possible model is larger (e.g., notice the offset around x=2 ). In the lower right figure, the bias curve is also slightly higher than in the lower left figure. In terms of variance however, the beam of predictions is narrower, which suggests that the variance is lower. Indeed, as the lower right figure confirms, the variance term (in green) is lower than for single decision trees. Overall, the bias- variance decomposition is therefore no longer the same. The tradeoff is better for bagging: averaging several decision trees fit on bootstrap copies of the dataset slightly increases the bias term but allows for a larger reduction of the variance, which results in a lower overall mean squared error (compare the red curves int the lower figures). The script output also confirms this intuition. The total error of the bagging ensemble is lower than the total error of a single decision tree, and this difference indeed mainly stems from a reduced variance. import numpy as np import matplotlib.pyplot as plt from sklearn.ensemble import BaggingRegressor from sklearn.tree import DecisionTreeRegressor Setting Parameters # Settings n_repeat = 50 # Number of iterations for computing expectations n_train = 50 # Size of the training set n_test = 1000 # Size of the test set noise = 0.1 # Standard deviation of the noise np . random . seed( 0 ) Change this for exploring the bias-variance decomposition of other estimators. This should work well for estimators with high variance (e.g., decision trees or KNN), but poorly for estimators with low variance (e.g., linear models). estimators = [( \"Tree\" , DecisionTreeRegressor()), ( \"Bagging(Tree)\" , BaggingRegressor(DecisionTreeRegressor()))] n_estimators = len (estimators) ### Data # Generate data def f (x): x = x . ravel() return np . exp( - x ** 2 ) + 1.5 * np . exp( - (x - 2 ) ** 2 ) def generate (n_samples, noise, n_repeat = 1 ): X = np . random . rand(n_samples) * 10 - 5 X = np . sort(X) if n_repeat == 1 : y = f(X) + np . random . normal( 0.0 , noise, n_samples) else : y = np . zeros((n_samples, n_repeat)) for i in range (n_repeat): y[:, i] = f(X) + np . random . normal( 0.0 , noise, n_samples) X = X . reshape((n_samples, 1 )) return X, y","title":"1. Single estimator versus bagging: bias-variance decomposition (source)"},{"location":"Classifiers/Ensamble/ensamble/#partition","text":"X_train = [] y_train = [] for i in range (n_repeat): X, y = generate(n_samples = n_train, noise = noise) X_train . append(X) y_train . append(y) X_test, y_test = generate(n_samples = n_test, noise = noise, n_repeat = n_repeat) ### Plot # Loop over estimators to compare for n, (name, estimator) in enumerate (estimators): # Compute predictions y_predict = np . zeros((n_test, n_repeat)) for i in range (n_repeat): estimator . fit(X_train[i], y_train[i]) y_predict[:, i] = estimator . predict(X_test) # Bias^2 + Variance + Noise decomposition of the mean squared error y_error = np . zeros(n_test) for i in range (n_repeat): for j in range (n_repeat): y_error += (y_test[:, j] - y_predict[:, i]) ** 2 y_error /= (n_repeat * n_repeat) y_noise = np . var(y_test, axis = 1 ) y_bias = (f(X_test) - np . mean(y_predict, axis = 1 )) ** 2 y_var = np . var(y_predict, axis = 1 ) print ( \"{0}: {1:.4f} (error) = {2:.4f} (bias^2) \" \" + {3:.4f} (var) + {4:.4f} (noise)\" . format(name, np . mean(y_error), np . mean(y_bias), np . mean(y_var), np . mean(y_noise))) # Plot figures plt . subplot( 2 , n_estimators, n + 1 ) plt . plot(X_test, f(X_test), \"b\" , label = \"$f(x)$\" ) plt . plot(X_train[ 0 ], y_train[ 0 ], \".b\" , label = \"LS ~ $y = f(x)+noise$\" ) for i in range (n_repeat): if i == 0 : plt . plot(X_test, y_predict[:, i], \"r\" , label = \"$\\^y(x)$\" ) else : plt . plot(X_test, y_predict[:, i], \"r\" , alpha = 0.05 ) plt . plot(X_test, np . mean(y_predict, axis = 1 ), \"c\" , label = \"$\\mathbb{E}_{LS} \\^y(x)$\" ) plt . xlim([ - 5 , 5 ]) plt . title(name) if n == 0 : plt . legend(loc = \"upper left\" , prop = { \"size\" : 11 }) plt . subplot( 2 , n_estimators, n_estimators + n + 1 ) plt . plot(X_test, y_error, \"r\" , label = \"$error(x)$\" ) plt . plot(X_test, y_bias, \"b\" , label = \"$bias^2(x)$\" ), plt . plot(X_test, y_var, \"g\" , label = \"$variance(x)$\" ), plt . plot(X_test, y_noise, \"c\" , label = \"$noise(x)$\" ) plt . xlim([ - 5 , 5 ]) plt . ylim([ 0 , 0.1 ]) if n == 0 : plt . legend(loc = \"upper left\" , prop = { \"size\" : 11 }) plt . show() Tree: 0.0255 (error) = 0.0003 (bias^2) + 0.0152 (var) + 0.0098 (noise) Bagging(Tree): 0.0196 (error) = 0.0004 (bias^2) + 0.0092 (var) + 0.0098 (noise)","title":"Partition"},{"location":"Classifiers/Ensamble/ensamble/#2-oob-errors-for-random-forests-source","text":"The RandomForestClassifier is trained using bootstrap aggregation , where each new tree is fit from a bootstrap sample of the training observations $z_i = (x_i, y_i)$. The out-of-bag (OOB) error is the average error for each $z_i$ calculated using predictions from the trees that do not contain $z_i$ in their respective bootstrap sample. This allows the RandomForestClassifier to be fit and validated whilst being trained The example below demonstrates how the OOB error can be measured at the addition of each new tree during training. The resulting plot allows a practitioner to approximate a suitable value of n_estimators at which the error stabilizes. import matplotlib.pyplot as plt from collections import OrderedDict from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier #### Data Set RANDOM_STATE = 123 # Generate a binary classification dataset. X, y = make_classification(n_samples = 500 , n_features = 25 , n_clusters_per_class = 1 , n_informative = 15 , random_state = RANDOM_STATE)","title":"2. OOB Errors for Random Forests (source)"},{"location":"Classifiers/Ensamble/ensamble/#warm_start","text":"NOTE: Setting the warm_start construction parameter to True disables support for paralellised ensembles but is necessary for tracking the OOB error trajectory during training. ensemble_clfs = [ ( \"RandomForestClassifier, max_features='sqrt'\" , RandomForestClassifier(warm_start = True , oob_score = True , max_features = \"sqrt\" , random_state = RANDOM_STATE)), ( \"RandomForestClassifier, max_features='log2'\" , RandomForestClassifier(warm_start = True , max_features = 'log2' , oob_score = True , random_state = RANDOM_STATE)), ( \"RandomForestClassifier, max_features=None\" , RandomForestClassifier(warm_start = True , max_features = None , oob_score = True , random_state = RANDOM_STATE)) ] Map a classifier name to a list of ( , ) pairs. error_rate = OrderedDict((label, []) for label, _ in ensemble_clfs) # Range of `n_estimators` values to explore. min_estimators = 15 max_estimators = 175 for label, clf in ensemble_clfs: for i in range (min_estimators, max_estimators + 1 ): clf . set_params(n_estimators = i) clf . fit(X, y) # Record the OOB error for each `n_estimators=i` setting. oob_error = 1 - clf . oob_score_ error_rate[label] . append((i, oob_error)) #### Plot # Generate the \"OOB error rate\" vs. \"n_estimators\" plot. for label, clf_err in error_rate . items(): xs, ys = zip ( * clf_err) plt . plot(xs, ys, label = label) plt . xlim(min_estimators, max_estimators) plt . xlabel( \"n_estimators\" ) plt . ylabel( \"OOB error rate\" ) plt . legend(loc = \"upper right\" ) plt . show()","title":"warm_start"},{"location":"Classifiers/Ensamble/ensamble/#3-feature-transformations-with-ensembles-of-trees-source","text":"Transform your features into a higher dimensional, sparse space. Then train a linear model on these features. First fit an ensemble of trees (totally random trees, a random forest, or gradient boosted trees) on the training set. Then each leaf of each tree in the ensemble is assigned a fixed arbitrary feature index in a new feature space. These leaf indices are then encoded in a one-hot fashion. Each sample goes through the decisions of each tree of the ensemble and ends up in one leaf per tree. The sample is encoded by setting feature values for these leaves to 1 and the other feature values to 0. The resulting transformer has then learned a supervised, sparse, high-dimensional categorical embedding of the data. import numpy as np np . random . seed( 10 ) import matplotlib.pyplot as plt #======== models ===================================== from sklearn.linear_model import LogisticRegression from sklearn.ensemble import (RandomTreesEmbedding, RandomForestClassifier, GradientBoostingClassifier) from sklearn.preprocessing import OneHotEncoder from sklearn.cross_validation import train_test_split from sklearn.metrics import roc_curve from sklearn.pipeline import make_pipeline #======= data ================================== from sklearn.datasets import make_classification #### Data n_estimator = 10 X, y = make_classification(n_samples = 80000 ) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5 ) It is important to train the ensemble of trees on a different subset of the training data than the linear regression model to avoid overfitting, in particular if the total number of leaves is similar to the number of training samples X_train, X_train_lr, y_train, y_train_lr = train_test_split(X_train, y_train, test_size = 0.5 ) Unsupervised transformation # Unsupervised transformation based on totally random trees rt = RandomTreesEmbedding(max_depth = 3 , n_estimators = n_estimator, random_state = 0 ) rt_lm = LogisticRegression() pipeline = make_pipeline(rt, rt_lm) pipeline . fit(X_train, y_train) y_pred_rt = pipeline . predict_proba(X_test)[:, 1 ] fpr_rt_lm, tpr_rt_lm, _ = roc_curve(y_test, y_pred_rt) #### Model # Supervised transformation based on random forests rf = RandomForestClassifier(max_depth = 3 , n_estimators = n_estimator) rf_enc = OneHotEncoder() rf_lm = LogisticRegression() rf . fit(X_train, y_train) rf_enc . fit(rf . apply(X_train)) rf_lm . fit(rf_enc . transform(rf . apply(X_train_lr)), y_train_lr) y_pred_rf_lm = rf_lm . predict_proba(rf_enc . transform(rf . apply(X_test)))[:, 1 ] fpr_rf_lm, tpr_rf_lm, _ = roc_curve(y_test, y_pred_rf_lm) grd = GradientBoostingClassifier(n_estimators = n_estimator) grd_enc = OneHotEncoder() grd_lm = LogisticRegression() grd . fit(X_train, y_train) grd_enc . fit(grd . apply(X_train)[:, :, 0 ]) grd_lm . fit(grd_enc . transform(grd . apply(X_train_lr)[:, :, 0 ]), y_train_lr) y_pred_grd_lm = grd_lm . predict_proba( grd_enc . transform(grd . apply(X_test)[:, :, 0 ]))[:, 1 ] fpr_grd_lm, tpr_grd_lm, _ = roc_curve(y_test, y_pred_grd_lm) # The gradient boosted model by itself y_pred_grd = grd . predict_proba(X_test)[:, 1 ] fpr_grd, tpr_grd, _ = roc_curve(y_test, y_pred_grd) # The random forest model by itself y_pred_rf = rf . predict_proba(X_test)[:, 1 ] fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_rf) Plot-1 plt . figure( 1 ) plt . plot([ 0 , 1 ], [ 0 , 1 ], 'k--' ) plt . plot(fpr_rt_lm, tpr_rt_lm, label = 'RT + LR' ) plt . plot(fpr_rf, tpr_rf, label = 'RF' ) plt . plot(fpr_rf_lm, tpr_rf_lm, label = 'RF + LR' ) plt . plot(fpr_grd, tpr_grd, label = 'GBT' ) plt . plot(fpr_grd_lm, tpr_grd_lm, label = 'GBT + LR' ) plt . xlabel( 'False positive rate' ) plt . ylabel( 'True positive rate' ) plt . title( 'ROC curve' ) plt . legend(loc = 'best' ) plt . show() Plot-2 plt . figure( 2 ) plt . xlim( 0 , 0.2 ) plt . ylim( 0.8 , 1 ) plt . plot([ 0 , 1 ], [ 0 , 1 ], 'k--' ) plt . plot(fpr_rt_lm, tpr_rt_lm, label = 'RT + LR' ) plt . plot(fpr_rf, tpr_rf, label = 'RF' ) plt . plot(fpr_rf_lm, tpr_rf_lm, label = 'RF + LR' ) plt . plot(fpr_grd, tpr_grd, label = 'GBT' ) plt . plot(fpr_grd_lm, tpr_grd_lm, label = 'GBT + LR' ) plt . xlabel( 'False positive rate' ) plt . ylabel( 'True positive rate' ) plt . title( 'ROC curve (zoomed in at top left)' ) plt . legend(loc = 'best' ) plt . show()","title":"3. Feature transformations with ensembles of trees (source)"},{"location":"Classifiers/Ensamble/ensamble/#4-pixel-importances-with-a-parallel-forest-of-trees-source","text":"This example shows the use of forests of trees to evaluate the importance of the pixels in an image classification task (faces). The hotter the pixel, the more important. The code below also illustrates how the construction and the computation of the predictions can be parallelized within multiple jobs. from time import time import matplotlib.pyplot as plt from sklearn.datasets import fetch_olivetti_faces from sklearn.ensemble import ExtraTreesClassifier #### Data # Number of cores to use to perform parallel fitting of the forest model n_jobs = 1 # Load the faces dataset data = fetch_olivetti_faces() X = data . images . reshape(( len (data . images), - 1 )) y = data . target mask = y < 5 # Limit to 5 classes X = X[mask] y = y[mask] Perform parallel fitting of the forest model # Build a forest and compute the pixel importances print ( \"Fitting ExtraTreesClassifier on faces data with %d cores...\" % n_jobs) t0 = time() forest = ExtraTreesClassifier(n_estimators = 1000 , max_features = 128 , n_jobs = n_jobs, random_state = 0 ) forest . fit(X, y) print ( \"done in %0.3f s\" % (time() - t0)) importances = forest . feature_importances_ importances = importances . reshape(data . images[ 0 ] . shape) Fitting ExtraTreesClassifier on faces data with 1 cores... done in 2.155s ##### Plot # Plot pixel importances plt . matshow(importances, cmap = plt . cm . hot) plt . title( \"Pixel importances with forests of trees\" ) plt . show()","title":"4. Pixel importances with a parallel forest of trees (source)"},{"location":"Classifiers/Ensamble/ensamble/#5-feature-importances-with-forests-of-trees-source","text":"This examples shows the use of forests of trees to evaluate the importance of features on an artificial classification task. The red bars are the feature importances of the forest, along with their inter-trees variability. As expected, the plot suggests that 3 features are informative, while the remaining are not. import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_classification from sklearn.ensemble import ExtraTreesClassifier Build a classification task # Build a classification task using 3 informative features X, y = make_classification(n_samples = 1000 , n_features = 10 , n_informative = 3 , n_redundant = 0 , n_repeated = 0 , n_classes = 2 , random_state = 0 , shuffle = False ) Data # Build a forest and compute the feature importances forest = ExtraTreesClassifier(n_estimators = 250 , random_state = 0 ) forest . fit(X, y) importances = forest . feature_importances_ std = np . std([tree . feature_importances_ for tree in forest . estimators_], axis = 0 ) indices = np . argsort(importances)[:: - 1 ] Feature Ranking # Print the feature ranking print ( \"Feature ranking:\" ) for f in range (X . shape[ 1 ]): print ( \" %d . feature %d ( %f )\" % (f + 1 , indices[f], importances[indices[f]])) Feature ranking: 1. feature 0 (0.250402) 2. feature 1 (0.231094) 3. feature 2 (0.148057) 4. feature 3 (0.055632) 5. feature 5 (0.054583) 6. feature 8 (0.054573) 7. feature 6 (0.052606) 8. feature 7 (0.051109) 9. feature 9 (0.051010) 10. feature 4 (0.050934) Plot the feature importances of the forest # Plot the feature importances of the forest plt . figure() plt . title( \"Feature importances\" ) plt . bar( range (X . shape[ 1 ]), importances[indices], color = \"r\" , yerr = std[indices], align = \"center\" ) plt . xticks( range (X . shape[ 1 ]), indices) plt . xlim([ - 1 , X . shape[ 1 ]]) plt . show()","title":"5. Feature importances with forests of trees (source)"},{"location":"Classifiers/Ensamble/ensamble/#6-isolationforest-example-source","text":"An example using IsolationForest for anomaly detection. The IsolationForest 'isolates' observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature. Since recursive partitioning can be represented by a tree structure, the number of splittings required to isolate a sample is equivalent to the path length from the root node to the terminating node. This path length, averaged over a forest of such random trees, is a measure of abnormality and our decision function. Random partitioning produces noticeable shorter paths for anomalies. Hence, when a forest of random trees collectively produce shorter path lengths for particular samples, they are highly likely to be anomalies. import numpy as np import matplotlib.pyplot as plt from sklearn.ensemble import IsolationForest rng = np . random . RandomState( 42 ) # Generate train data X = 0.3 * rng . randn( 100 , 2 ) X_train = np . r_[X + 2 , X - 2 ] # Generate some regular novel observations X = 0.3 * rng . randn( 20 , 2 ) X_test = np . r_[X + 2 , X - 2 ] # Generate some abnormal novel observations X_outliers = rng . uniform(low =- 4 , high = 4 , size = ( 20 , 2 )) # fit the model clf = IsolationForest(max_samples = 100 , random_state = rng) clf . fit(X_train) y_pred_train = clf . predict(X_train) y_pred_test = clf . predict(X_test) y_pred_outliers = clf . predict(X_outliers) # plot the line, the samples, and the nearest vectors to the plane xx, yy = np . meshgrid(np . linspace( - 5 , 5 , 50 ), np . linspace( - 5 , 5 , 50 )) Z = clf . decision_function(np . c_[xx . ravel(), yy . ravel()]) Z = Z . reshape(xx . shape) plt . title( \"IsolationForest\" ) plt . contourf(xx, yy, Z, cmap = plt . cm . Blues_r) b1 = plt . scatter(X_train[:, 0 ], X_train[:, 1 ], c = 'white' ) b2 = plt . scatter(X_test[:, 0 ], X_test[:, 1 ], c = 'green' ) c = plt . scatter(X_outliers[:, 0 ], X_outliers[:, 1 ], c = 'red' ) plt . axis( 'tight' ) plt . xlim(( - 5 , 5 )) plt . ylim(( - 5 , 5 )) plt . legend([b1, b2, c], [ \"training observations\" , \"new regular observations\" , \"new abnormal observations\" ], loc = \"upper left\" ) plt . show()","title":"6. IsolationForest example (source)"},{"location":"Classifiers/Ensamble/ensamble/#7-hashing-feature-transformation-using-totally-random-trees-source","text":"RandomTreesEmbedding provides a way to map data to a very high-dimensional, sparse representation, which might be beneficial for classification. The mapping is completely unsupervised and very efficient. This example visualizes the partitions given by several trees and shows how the transformation can also be used for non-linear dimensionality reduction or non-linear classification. Points that are neighboring often share the same leaf of a tree and therefore share large parts of their hashed representation. This allows to separate two concentric circles simply based on the principal components of the transformed data. In high-dimensional spaces, linear classifiers often achieve excellent accuracy. For sparse binary data, BernoulliNB is particularly well-suited. The bottom row compares the decision boundary obtained by BernoulliNB in the transformed space with an ExtraTreesClassifier forests learned on the original data. import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_circles from sklearn.ensemble import RandomTreesEmbedding, ExtraTreesClassifier from sklearn.decomposition import TruncatedSVD from sklearn.naive_bayes import BernoulliNB Data # make a synthetic dataset X, y = make_circles(factor = 0.5 , random_state = 0 , noise = 0.05 ) # use RandomTreesEmbedding to transform data hasher = RandomTreesEmbedding(n_estimators = 10 , random_state = 0 , max_depth = 3 ) X_transformed = hasher . fit_transform(X) PCA # Visualize result using PCA pca = TruncatedSVD(n_components = 2 ) X_reduced = pca . fit_transform(X_transformed) Naive Bayes classifier # Learn a Naive Bayes classifier on the transformed data nb = BernoulliNB() nb . fit(X_transformed, y) BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True) ExtraTreesClassifier # Learn an ExtraTreesClassifier for comparison trees = ExtraTreesClassifier(max_depth = 3 , n_estimators = 10 , random_state = 0 ) trees . fit(X, y) ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini', max_depth=3, max_features='auto', max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1, oob_score=False, random_state=0, verbose=0, warm_start=False) Plot # scatter plot of original and reduced data fig = plt . figure(figsize = ( 9 , 8 )) ax = plt . subplot( 221 ) ax . scatter(X[:, 0 ], X[:, 1 ], c = y, s = 50 ) ax . set_title( \"Original Data (2d)\" ) ax . set_xticks(()) ax . set_yticks(()) ax = plt . subplot( 222 ) ax . scatter(X_reduced[:, 0 ], X_reduced[:, 1 ], c = y, s = 50 ) ax . set_title( \"PCA reduction (2d) of transformed data ( %d d)\" % X_transformed . shape[ 1 ]) ax . set_xticks(()) ax . set_yticks(()) # Plot the decision in original space. For that, we will assign a color to each # point in the mesh [x_min, m_max] x [y_min, y_max]. h = . 01 x_min, x_max = X[:, 0 ] . min() - . 5 , X[:, 0 ] . max() + . 5 y_min, y_max = X[:, 1 ] . min() - . 5 , X[:, 1 ] . max() + . 5 xx, yy = np . meshgrid(np . arange(x_min, x_max, h), np . arange(y_min, y_max, h)) # transform grid using RandomTreesEmbedding transformed_grid = hasher . transform(np . c_[xx . ravel(), yy . ravel()]) y_grid_pred = nb . predict_proba(transformed_grid)[:, 1 ] ax = plt . subplot( 223 ) ax . set_title( \"Naive Bayes on Transformed data\" ) ax . pcolormesh(xx, yy, y_grid_pred . reshape(xx . shape)) ax . scatter(X[:, 0 ], X[:, 1 ], c = y, s = 50 ) ax . set_ylim( - 1.4 , 1.4 ) ax . set_xlim( - 1.4 , 1.4 ) ax . set_xticks(()) ax . set_yticks(()) # transform grid using ExtraTreesClassifier y_grid_pred = trees . predict_proba(np . c_[xx . ravel(), yy . ravel()])[:, 1 ] ax = plt . subplot( 224 ) ax . set_title( \"ExtraTrees predictions\" ) ax . pcolormesh(xx, yy, y_grid_pred . reshape(xx . shape)) ax . scatter(X[:, 0 ], X[:, 1 ], c = y, s = 50 ) ax . set_ylim( - 1.4 , 1.4 ) ax . set_xlim( - 1.4 , 1.4 ) ax . set_xticks(()) ax . set_yticks(()) plt . tight_layout() plt . show()","title":"7.  Hashing feature transformation using Totally Random Trees (source)"},{"location":"Classifiers/Ensamble/ensamble/#8-two-class-adaboost-source","text":"This example fits an AdaBoosted decision stump on a non-linearly separable classification dataset composed of two \"Gaussian quantiles\" clusters (see :func: sklearn.datasets.make_gaussian_quantiles ) and plots the decision boundary and decision scores. The distributions of decision scores are shown separately for samples of class A and B. The predicted class label for each sample is determined by the sign of the decision score. Samples with decision scores greater than zero are classified as B, and are otherwise classified as A. The magnitude of a decision score determines the degree of likeness with the predicted class label. Additionally, a new dataset could be constructed containing a desired purity of class B, for example, by only selecting samples with a decision score above some value. import numpy as np import matplotlib.pyplot as plt from sklearn.ensemble import AdaBoostClassifier from sklearn.tree import DecisionTreeClassifier from sklearn.datasets import make_gaussian_quantiles Data # Construct dataset X1, y1 = make_gaussian_quantiles(cov = 2. , n_samples = 200 , n_features = 2 , n_classes = 2 , random_state = 1 ) X2, y2 = make_gaussian_quantiles(mean = ( 3 , 3 ), cov = 1.5 , n_samples = 300 , n_features = 2 , n_classes = 2 , random_state = 1 ) X = np . concatenate((X1, X2)) y = np . concatenate((y1, - y2 + 1 )) Model # Create and fit an AdaBoosted decision tree bdt = AdaBoostClassifier(DecisionTreeClassifier(max_depth = 1 ), algorithm = \"SAMME\" , n_estimators = 200 ) bdt . fit(X, y) AdaBoostClassifier(algorithm='SAMME', base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1, max_features=None, max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=None, splitter='best'), learning_rate=1.0, n_estimators=200, random_state=None) Plot plot_colors = \"br\" plot_step = 0.02 class_names = \"AB\" plt . figure(figsize = ( 10 , 5 )) # Plot the decision boundaries plt . subplot( 121 ) x_min, x_max = X[:, 0 ] . min() - 1 , X[:, 0 ] . max() + 1 y_min, y_max = X[:, 1 ] . min() - 1 , X[:, 1 ] . max() + 1 xx, yy = np . meshgrid(np . arange(x_min, x_max, plot_step), np . arange(y_min, y_max, plot_step)) Z = bdt . predict(np . c_[xx . ravel(), yy . ravel()]) Z = Z . reshape(xx . shape) cs = plt . contourf(xx, yy, Z, cmap = plt . cm . Paired) plt . axis( \"tight\" ) # Plot the training points for i, n, c in zip ( range ( 2 ), class_names, plot_colors): idx = np . where(y == i) plt . scatter(X[idx, 0 ], X[idx, 1 ], c = c, cmap = plt . cm . Paired, label = \"Class %s \" % n) plt . xlim(x_min, x_max) plt . ylim(y_min, y_max) plt . legend(loc = 'upper right' ) plt . xlabel( 'x' ) plt . ylabel( 'y' ) plt . title( 'Decision Boundary' ) # Plot the two-class decision scores twoclass_output = bdt . decision_function(X) plot_range = (twoclass_output . min(), twoclass_output . max()) plt . subplot( 122 ) for i, n, c in zip ( range ( 2 ), class_names, plot_colors): plt . hist(twoclass_output[y == i], bins = 10 , range = plot_range, facecolor = c, label = 'Class %s ' % n, alpha =. 5 ) x1, x2, y1, y2 = plt . axis() plt . axis((x1, x2, y1, y2 * 1.2 )) plt . legend(loc = 'upper right' ) plt . ylabel( 'Samples' ) plt . xlabel( 'Score' ) plt . title( 'Decision Scores' ) plt . tight_layout() plt . subplots_adjust(wspace = 0.35 ) plt . show()","title":"8. Two-class AdaBoost (source)"},{"location":"Classifiers/Ensamble/ensamble/#9-multi-class-adaboosted-decision-trees-source","text":"This example reproduces Figure 1 of Zhu et al [1] and shows how boosting can improve prediction accuracy on a multi-class problem. The classification dataset is constructed by taking a ten-dimensional standard normal distribution and defining three classes separated by nested concentric ten-dimensional spheres such that roughly equal numbers of samples are in each class (quantiles of the :math: \\chi^2 distribution). The performance of the SAMME and SAMME.R [1] algorithms are compared. SAMME.R uses the probability estimates to update the additive model, while SAMME uses the classifications only. As the example illustrates, the SAMME.R algorithm typically converges faster than SAMME, achieving a lower test error with fewer boosting iterations. The error of each algorithm on the test set after each boosting iteration is shown on the left, the classification error on the test set of each tree is shown in the middle, and the boost weight of each tree is shown on the right. All trees have a weight of one in the SAMME.R algorithm and therefore are not shown. from sklearn.externals.six.moves import zip import matplotlib.pyplot as plt from sklearn.datasets import make_gaussian_quantiles from sklearn.ensemble import AdaBoostClassifier from sklearn.metrics import accuracy_score from sklearn.tree import DecisionTreeClassifier Data X, y = make_gaussian_quantiles(n_samples = 13000 , n_features = 10 , n_classes = 3 , random_state = 1 ) n_split = 3000 X_train, X_test = X[:n_split], X[n_split:] y_train, y_test = y[:n_split], y[n_split:] Model bdt_real = AdaBoostClassifier( DecisionTreeClassifier(max_depth = 2 ), n_estimators = 600 , learning_rate = 1 ) bdt_discrete = AdaBoostClassifier( DecisionTreeClassifier(max_depth = 2 ), n_estimators = 600 , learning_rate = 1.5 , algorithm = \"SAMME\" ) bdt_real . fit(X_train, y_train) bdt_discrete . fit(X_train, y_train) real_test_errors = [] discrete_test_errors = [] for real_test_predict, discrete_train_predict in zip ( bdt_real . staged_predict(X_test), bdt_discrete . staged_predict(X_test)): real_test_errors . append( 1. - accuracy_score(real_test_predict, y_test)) discrete_test_errors . append( 1. - accuracy_score(discrete_train_predict, y_test)) n_trees_discrete = len (bdt_discrete) n_trees_real = len (bdt_real) # Boosting might terminate early, but the following arrays are always # n_estimators long. We crop them to the actual number of trees here: discrete_estimator_errors = bdt_discrete . estimator_errors_[:n_trees_discrete] real_estimator_errors = bdt_real . estimator_errors_[:n_trees_real] discrete_estimator_weights = bdt_discrete . estimator_weights_[:n_trees_discrete] Plot plt . figure(figsize = ( 15 , 5 )) plt . subplot( 131 ) plt . plot( range ( 1 , n_trees_discrete + 1 ), discrete_test_errors, c = 'black' , label = 'SAMME' ) plt . plot( range ( 1 , n_trees_real + 1 ), real_test_errors, c = 'black' , linestyle = 'dashed' , label = 'SAMME.R' ) plt . legend() plt . ylim( 0.18 , 0.62 ) plt . ylabel( 'Test Error' ) plt . xlabel( 'Number of Trees' ) plt . subplot( 132 ) plt . plot( range ( 1 , n_trees_discrete + 1 ), discrete_estimator_errors, \"b\" , label = 'SAMME' , alpha =. 5 ) plt . plot( range ( 1 , n_trees_real + 1 ), real_estimator_errors, \"r\" , label = 'SAMME.R' , alpha =. 5 ) plt . legend() plt . ylabel( 'Error' ) plt . xlabel( 'Number of Trees' ) plt . ylim(( . 2 , max (real_estimator_errors . max(), discrete_estimator_errors . max()) * 1.2 )) plt . xlim(( - 20 , len (bdt_discrete) + 20 )) plt . subplot( 133 ) plt . plot( range ( 1 , n_trees_discrete + 1 ), discrete_estimator_weights, \"b\" , label = 'SAMME' ) plt . legend() plt . ylabel( 'Weight' ) plt . xlabel( 'Number of Trees' ) plt . ylim(( 0 , discrete_estimator_weights . max() * 1.2 )) plt . xlim(( - 20 , n_trees_discrete + 20 )) # prevent overlapping y-axis labels plt . subplots_adjust(wspace = 0.25 ) plt . show()","title":"9. Multi-class AdaBoosted Decision Trees (source)"},{"location":"Classifiers/Ensamble/ensamble/#10-discrete-versus-real-adaboost-source","text":"This example is based on Figure 10.2 from Hastie et al 2009 [1] and illustrates the difference in performance between the discrete SAMME [2] boosting algorithm and real SAMME.R boosting algorithm. Both algorithms are evaluated on a binary classification task where the target Y is a non-linear function of 10 input features. Discrete SAMME AdaBoost adapts based on errors in predicted class labels whereas real SAMME.R uses the predicted class probabilities. import numpy as np import matplotlib.pyplot as plt from sklearn import datasets from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import zero_one_loss from sklearn.ensemble import AdaBoostClassifier Data n_estimators = 400 # A learning rate of 1. may not be optimal for both SAMME and SAMME.R learning_rate = 1. X, y = datasets . make_hastie_10_2(n_samples = 12000 , random_state = 1 ) X_test, y_test = X[ 2000 :], y[ 2000 :] X_train, y_train = X[: 2000 ], y[: 2000 ] Model dt_stump = DecisionTreeClassifier(max_depth = 1 , min_samples_leaf = 1 ) dt_stump . fit(X_train, y_train) dt_stump_err = 1.0 - dt_stump . score(X_test, y_test) dt = DecisionTreeClassifier(max_depth = 9 , min_samples_leaf = 1 ) dt . fit(X_train, y_train) dt_err = 1.0 - dt . score(X_test, y_test) ada_discrete = AdaBoostClassifier( base_estimator = dt_stump, learning_rate = learning_rate, n_estimators = n_estimators, algorithm = \"SAMME\" ) ada_discrete . fit(X_train, y_train) ada_real = AdaBoostClassifier( base_estimator = dt_stump, learning_rate = learning_rate, n_estimators = n_estimators, algorithm = \"SAMME.R\" ) ada_real . fit(X_train, y_train) AdaBoostClassifier(algorithm='SAMME.R', base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1, max_features=None, max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=None, splitter='best'), learning_rate=1.0, n_estimators=400, random_state=None) Plot fig = plt . figure() ax = fig . add_subplot( 111 ) ax . plot([ 1 , n_estimators], [dt_stump_err] * 2 , 'k-' , label = 'Decision Stump Error' ) ax . plot([ 1 , n_estimators], [dt_err] * 2 , 'k--' , label = 'Decision Tree Error' ) ada_discrete_err = np . zeros((n_estimators,)) for i, y_pred in enumerate (ada_discrete . staged_predict(X_test)): ada_discrete_err[i] = zero_one_loss(y_pred, y_test) ada_discrete_err_train = np . zeros((n_estimators,)) for i, y_pred in enumerate (ada_discrete . staged_predict(X_train)): ada_discrete_err_train[i] = zero_one_loss(y_pred, y_train) ada_real_err = np . zeros((n_estimators,)) for i, y_pred in enumerate (ada_real . staged_predict(X_test)): ada_real_err[i] = zero_one_loss(y_pred, y_test) ada_real_err_train = np . zeros((n_estimators,)) for i, y_pred in enumerate (ada_real . staged_predict(X_train)): ada_real_err_train[i] = zero_one_loss(y_pred, y_train) ax . plot(np . arange(n_estimators) + 1 , ada_discrete_err, label = 'Discrete AdaBoost Test Error' , color = 'red' ) ax . plot(np . arange(n_estimators) + 1 , ada_discrete_err_train, label = 'Discrete AdaBoost Train Error' , color = 'blue' ) ax . plot(np . arange(n_estimators) + 1 , ada_real_err, label = 'Real AdaBoost Test Error' , color = 'orange' ) ax . plot(np . arange(n_estimators) + 1 , ada_real_err_train, label = 'Real AdaBoost Train Error' , color = 'green' ) ax . set_ylim(( 0.0 , 0.5 )) ax . set_xlabel( 'n_estimators' ) ax . set_ylabel( 'error rate' ) leg = ax . legend(loc = 'upper right' , fancybox = True ) leg . get_frame() . set_alpha( 0.7 ) plt . show()","title":"10 .Discrete versus Real AdaBoost (source)"},{"location":"Classifiers/Ensamble/ensamble/#11-decision-tree-regression-with-adaboost-source","text":"A decision tree is boosted using the AdaBoost.R2 [1] algorithm on a 1D sinusoidal dataset with a small amount of Gaussian noise. 299 boosts (300 decision trees) is compared with a single decision tree regressor. As the number of boosts is increased the regressor can fit more detail. # importing necessary libraries import numpy as np import matplotlib.pyplot as plt from sklearn.tree import DecisionTreeRegressor from sklearn.ensemble import AdaBoostRegressor Data # Create the dataset rng = np . random . RandomState( 1 ) X = np . linspace( 0 , 6 , 100 )[:, np . newaxis] y = np . sin(X) . ravel() + np . sin( 6 * X) . ravel() + rng . normal( 0 , 0.1 , X . shape[ 0 ]) Model # Fit regression model regr_1 = DecisionTreeRegressor(max_depth = 4 ) regr_2 = AdaBoostRegressor(DecisionTreeRegressor(max_depth = 4 ), n_estimators = 300 , random_state = rng) regr_1 . fit(X, y) regr_2 . fit(X, y) AdaBoostRegressor(base_estimator=DecisionTreeRegressor(criterion='mse', max_depth=4, max_features=None, max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=None, splitter='best'), learning_rate=1.0, loss='linear', n_estimators=300, random_state=&lt;mtrand.RandomState object at 0x114302d90&gt;) Predict # Predict y_1 = regr_1 . predict(X) y_2 = regr_2 . predict(X) Plot # Plot the results plt . figure() plt . scatter(X, y, c = \"k\" , label = \"training samples\" ) plt . plot(X, y_1, c = \"g\" , label = \"n_estimators=1\" , linewidth = 2 ) plt . plot(X, y_2, c = \"r\" , label = \"n_estimators=300\" , linewidth = 2 ) plt . xlabel( \"data\" ) plt . ylabel( \"target\" ) plt . title( \"Boosted Decision Tree Regression\" ) plt . legend() plt . show()","title":"11. Decision Tree Regression with AdaBoost (source)"},{"location":"Classifiers/Ensamble/ensamble/#12-gradient-boosting-out-of-bag-estimates-source","text":"Out-of-bag (OOB) estimates can be a useful heuristic to estimate the \"optimal\" number of boosting iterations. OOB estimates are almost identical to cross-validation estimates but they can be computed on-the-fly without the need for repeated model fitting. OOB estimates are only available for Stochastic Gradient Boosting (i.e. subsample < 1.0 ), the estimates are derived from the improvement in loss based on the examples not included in the bootstrap sample (the so-called out-of-bag examples). The OOB estimator is a pessimistic estimator of the true test loss, but remains a fairly good approximation for a small number of trees. The figure shows the cumulative sum of the negative OOB improvements as a function of the boosting iteration. As you can see, it tracks the test loss for the first hundred iterations but then diverges in a pessimistic way. The figure also shows the performance of 3-fold cross validation which usually gives a better estimate of the test loss but is computationally more demanding. import numpy as np import matplotlib.pyplot as plt from sklearn import ensemble from sklearn.cross_validation import KFold from sklearn.cross_validation import train_test_split /home/bitnami/anaconda3/envs/caseolap/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20. \"This module will be removed in 0.20.\", DeprecationWarning) Data # Generate data (adapted from G. Ridgeway's gbm example) n_samples = 1000 random_state = np . random . RandomState( 13 ) x1 = random_state . uniform(size = n_samples) x2 = random_state . uniform(size = n_samples) x3 = random_state . randint( 0 , 4 , size = n_samples) p = 1 / ( 1.0 + np . exp( - (np . sin( 3 * x1) - 4 * x2 + x3))) y = random_state . binomial( 1 , p, size = n_samples) X = np . c_[x1, x2, x3] X = X . astype(np . float32) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5 , random_state = 9 ) Model # Fit classifier with out-of-bag estimates params = { 'n_estimators' : 1200 , 'max_depth' : 3 , 'subsample' : 0.5 , 'learning_rate' : 0.01 , 'min_samples_leaf' : 1 , 'random_state' : 3 } clf = ensemble . GradientBoostingClassifier( ** params) clf . fit(X_train, y_train) acc = clf . score(X_test, y_test) print ( \"Accuracy: {:.4f}\" . format(acc)) n_estimators = params[ 'n_estimators' ] x = np . arange(n_estimators) + 1 def heldout_score (clf, X_test, y_test): \"\"\"compute deviance scores on ``X_test`` and ``y_test``. \"\"\" score = np . zeros((n_estimators,), dtype = np . float64) for i, y_pred in enumerate (clf . staged_decision_function(X_test)): score[i] = clf . loss_(y_test, y_pred) return score def cv_estimate (n_folds = 3 ): cv = KFold(n_folds = n_folds) cv_clf = ensemble . GradientBoostingClassifier( ** params) val_scores = np . zeros((n_estimators,), dtype = np . float64) for train, test in cv . split(X_train, y_train): cv_clf . fit(X_train[train], y_train[train]) val_scores += heldout_score(cv_clf, X_train[test], y_train[test]) val_scores /= n_folds return val_scores # Estimate best n_estimator using cross-validation cv_score = cv_estimate( 3 ) # Compute best n_estimator for test data test_score = heldout_score(clf, X_test, y_test) # negative cumulative sum of oob improvements cumsum = - np . cumsum(clf . oob_improvement_) # min loss according to OOB oob_best_iter = x[np . argmin(cumsum)] # min loss according to test (normalize such that first loss is 0) test_score -= test_score[ 0 ] test_best_iter = x[np . argmin(test_score)] # min loss according to cv (normalize such that first loss is 0) cv_score -= cv_score[ 0 ] cv_best_iter = x[np . argmin(cv_score)] Accuracy: 0.6840 --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-6-331f159e761d&gt; in &lt;module&gt;() 32 33 # Estimate best n_estimator using cross-validation ---&gt; 34 cv_score = cv_estimate(3) 35 36 # Compute best n_estimator for test data &lt;ipython-input-6-331f159e761d&gt; in cv_estimate(n_folds) 21 22 def cv_estimate(n_folds=3): ---&gt; 23 cv = KFold(n_folds=n_folds) 24 cv_clf = ensemble.GradientBoostingClassifier(**params) 25 val_scores = np.zeros((n_estimators,), dtype=np.float64) TypeError: __init__() missing 1 required positional argument: 'n' Plot # color brew for the three curves oob_color = list ( map ( lambda x: x / 256.0 , ( 190 , 174 , 212 ))) test_color = list ( map ( lambda x: x / 256.0 , ( 127 , 201 , 127 ))) cv_color = list ( map ( lambda x: x / 256.0 , ( 253 , 192 , 134 ))) # plot curves and vertical lines for best iterations plt . plot(x, cumsum, label = 'OOB loss' , color = oob_color) plt . plot(x, test_score, label = 'Test loss' , color = test_color) plt . plot(x, cv_score, label = 'CV loss' , color = cv_color) plt . axvline(x = oob_best_iter, color = oob_color) plt . axvline(x = test_best_iter, color = test_color) plt . axvline(x = cv_best_iter, color = cv_color) # add three vertical lines to xticks xticks = plt . xticks() xticks_pos = np . array(xticks[ 0 ] . tolist() + [oob_best_iter, cv_best_iter, test_best_iter]) xticks_label = np . array( list ( map ( lambda t: int (t), xticks[ 0 ])) + [ 'OOB' , 'CV' , 'Test' ]) ind = np . argsort(xticks_pos) xticks_pos = xticks_pos[ind] xticks_label = xticks_label[ind] plt . xticks(xticks_pos, xticks_label) plt . legend(loc = 'upper right' ) plt . ylabel( 'normalized loss' ) plt . xlabel( 'number of iterations' ) plt . show() --------------------------------------------------------------------------- NameError Traceback (most recent call last) &lt;ipython-input-7-3dcde55ed4f9&gt; in &lt;module&gt;() 6 7 # plot curves and vertical lines for best iterations ----&gt; 8 plt.plot(x, cumsum, label='OOB loss', color=oob_color) 9 plt.plot(x, test_score, label='Test loss', color=test_color) 10 plt.plot(x, cv_score, label='CV loss', color=cv_color) NameError: name 'cumsum' is not defined","title":"12. Gradient Boosting Out-of-Bag estimates (source)"},{"location":"Classifiers/Ensamble/ensamble/#13-prediction-intervals-for-gradient-boosting-regression-source","text":"This example shows how quantile regression can be used to create prediction intervals. import numpy as np import matplotlib.pyplot as plt from sklearn.ensemble import GradientBoostingRegressor Data np . random . seed( 1 ) def f (x): \"\"\"The function to predict.\"\"\" return x * np . sin(x) #---------------------------------------------------------------------- # First the noiseless case X = np . atleast_2d(np . random . uniform( 0 , 10.0 , size = 100 )) . T X = X . astype(np . float32) # Observations y = f(X) . ravel() dy = 1.5 + 1.0 * np . random . random(y . shape) noise = np . random . normal( 0 , dy) y += noise y = y . astype(np . float32) # Mesh the input space for evaluations of the real function, the prediction and # its MSE xx = np . atleast_2d(np . linspace( 0 , 10 , 1000 )) . T xx = xx . astype(np . float32) alpha = 0.95 Model clf = GradientBoostingRegressor(loss = 'quantile' , alpha = alpha, n_estimators = 250 , max_depth = 3 , learning_rate =. 1 , min_samples_leaf = 9 , min_samples_split = 9 ) clf . fit(X, y) # Make the prediction on the meshed x-axis y_upper = clf . predict(xx) clf . set_params(alpha = 1.0 - alpha) clf . fit(X, y) # Make the prediction on the meshed x-axis y_lower = clf . predict(xx) clf . set_params(loss = 'ls' ) clf . fit(X, y) # Make the prediction on the meshed x-axis y_pred = clf . predict(xx) Plot # Plot the function, the prediction and the 90% confidence interval based on # the MSE fig = plt . figure() plt . plot(xx, f(xx), 'g:' , label = u'$f(x) = x\\,\\sin(x)$' ) plt . plot(X, y, 'b.' , markersize = 10 , label = u'Observations' ) plt . plot(xx, y_pred, 'r-' , label = u'Prediction' ) plt . plot(xx, y_upper, 'k-' ) plt . plot(xx, y_lower, 'k-' ) plt . fill(np . concatenate([xx, xx[:: - 1 ]]), np . concatenate([y_upper, y_lower[:: - 1 ]]), alpha =. 5 , fc = 'b' , ec = 'None' , label = '90% prediction interval' ) plt . xlabel( '$x$' ) plt . ylabel( '$f(x)$' ) plt . ylim( - 10 , 20 ) plt . legend(loc = 'upper left' ) plt . show()","title":"13. Prediction Intervals for Gradient Boosting Regression (source)"},{"location":"Classifiers/Ensamble/ensamble/#14-gradient-boosting-regression-source","text":"Demonstrate Gradient Boosting on the Boston housing dataset. This example fits a Gradient Boosting model with least squares loss and 500 regression trees of depth 4. import numpy as np import matplotlib.pyplot as plt from sklearn import ensemble from sklearn import datasets from sklearn.utils import shuffle from sklearn.metrics import mean_squared_error Data # Load data boston = datasets . load_boston() X, y = shuffle(boston . data, boston . target, random_state = 13 ) X = X . astype(np . float32) offset = int (X . shape[ 0 ] * 0.9 ) X_train, y_train = X[:offset], y[:offset] X_test, y_test = X[offset:], y[offset:] Model # Fit regression model params = { 'n_estimators' : 500 , 'max_depth' : 4 , 'min_samples_split' : 2 , 'learning_rate' : 0.01 , 'loss' : 'ls' } clf = ensemble . GradientBoostingRegressor( ** params) clf . fit(X_train, y_train) mse = mean_squared_error(y_test, clf . predict(X_test)) print ( \"MSE: %.4f \" % mse) MSE: 6.6213 Plot training deviance # Plot training deviance # compute test set deviance test_score = np . zeros((params[ 'n_estimators' ],), dtype = np . float64) for i, y_pred in enumerate (clf . staged_predict(X_test)): test_score[i] = clf . loss_(y_test, y_pred) plt . figure(figsize = ( 12 , 6 )) plt . subplot( 1 , 2 , 1 ) plt . title( 'Deviance' ) plt . plot(np . arange(params[ 'n_estimators' ]) + 1 , clf . train_score_, 'b-' , label = 'Training Set Deviance' ) plt . plot(np . arange(params[ 'n_estimators' ]) + 1 , test_score, 'r-' , label = 'Test Set Deviance' ) plt . legend(loc = 'upper right' ) plt . xlabel( 'Boosting Iterations' ) plt . ylabel( 'Deviance' ) plt . show() Plot # Plot feature importance feature_importance = clf . feature_importances_ # make importances relative to max importance feature_importance = 100.0 * (feature_importance / feature_importance . max()) sorted_idx = np . argsort(feature_importance) pos = np . arange(sorted_idx . shape[ 0 ]) + . 5 plt . subplot( 1 , 2 , 2 ) plt . barh(pos, feature_importance[sorted_idx], align = 'center' ) plt . yticks(pos, boston . feature_names[sorted_idx]) plt . xlabel( 'Relative Importance' ) plt . title( 'Variable Importance' ) plt . show()","title":"14. Gradient Boosting regression (source)"},{"location":"Classifiers/Ensamble/ensamble/#15-gradient-boosting-regularization-source","text":"Illustration of the effect of different regularization strategies for Gradient Boosting. The example is taken from Hastie et al 2009. The loss function used is binomial deviance. Regularization via shrinkage ( learning_rate < 1.0 ) improves performance considerably. In combination with shrinkage, stochastic gradient boosting ( subsample < 1.0 ) can produce more accurate models by reducing the variance via bagging. Subsampling without shrinkage usually does poorly. Another strategy to reduce the variance is by subsampling the features analogous to the random splits in Random Forests (via the max_features parameter). import numpy as np import matplotlib.pyplot as plt from sklearn import ensemble from sklearn import datasets Data X, y = datasets . make_hastie_10_2(n_samples = 12000 , random_state = 1 ) X = X . astype(np . float32) # map labels from {-1, 1} to {0, 1} labels, y = np . unique(y, return_inverse = True ) X_train, X_test = X[: 2000 ], X[ 2000 :] y_train, y_test = y[: 2000 ], y[ 2000 :] original_params = { 'n_estimators' : 1000 , 'max_leaf_nodes' : 4 , 'max_depth' : None , 'random_state' : 2 , 'min_samples_split' : 5 } Plot n Model plt . figure() for label, color, setting in [( 'No shrinkage' , 'orange' , { 'learning_rate' : 1.0 , 'subsample' : 1.0 }), ( 'learning_rate=0.1' , 'turquoise' , { 'learning_rate' : 0.1 , 'subsample' : 1.0 }), ( 'subsample=0.5' , 'blue' , { 'learning_rate' : 1.0 , 'subsample' : 0.5 }), ( 'learning_rate=0.1, subsample=0.5' , 'gray' , { 'learning_rate' : 0.1 , 'subsample' : 0.5 }), ( 'learning_rate=0.1, max_features=2' , 'magenta' , { 'learning_rate' : 0.1 , 'max_features' : 2 })]: params = dict (original_params) params . update(setting) clf = ensemble . GradientBoostingClassifier( ** params) clf . fit(X_train, y_train) # compute test set deviance test_deviance = np . zeros((params[ 'n_estimators' ],), dtype = np . float64) for i, y_pred in enumerate (clf . staged_decision_function(X_test)): # clf.loss_ assumes that y_test[i] in {0, 1} test_deviance[i] = clf . loss_(y_test, y_pred) plt . plot((np . arange(test_deviance . shape[ 0 ]) + 1 )[:: 5 ], test_deviance[:: 5 ], '-' , color = color, label = label) plt . legend(loc = 'upper left' ) plt . xlabel( 'Boosting Iterations' ) plt . ylabel( 'Test Set Deviance' ) plt . show()","title":"15. Gradient Boosting regularization (source)"},{"location":"Classifiers/Ensamble/ensamble/#16-partial-dependence-plots-source","text":"Partial dependence plots show the dependence between the target function [2]_ and a set of 'target' features, marginalizing over the values of all other features (the complement features). Due to the limits of human perception the size of the target feature set must be small (usually, one or two) thus the target features are usually chosen among the most important features (see :attr: ~sklearn.ensemble.GradientBoostingRegressor.feature_importances_ ). This example shows how to obtain partial dependence plots from a :class: ~sklearn.ensemble.GradientBoostingRegressor trained on the California housing dataset. The example is taken from [1]_. The plot shows four one-way and one two-way partial dependence plots. The target variables for the one-way PDP are: median income ( MedInc ), avg. occupants per household ( AvgOccup ), median house age ( HouseAge ), and avg. rooms per household ( AveRooms ). We can clearly see that the median house price shows a linear relationship with the median income (top left) and that the house price drops when the avg. occupants per household increases (top middle). The top right plot shows that the house age in a district does not have a strong influence on the (median) house price; so does the average rooms per household. The tick marks on the x-axis represent the deciles of the feature values in the training data. Partial dependence plots with two target features enable us to visualize interactions among them. The two-way partial dependence plot shows the dependence of median house price on joint values of house age and avg. occupants per household. We can clearly see an interaction between the two features: For an avg. occupancy greater than two, the house price is nearly independent of the house age, whereas for values less than two there is a strong dependence on age. from __future__ import print_function import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D from sklearn.cross_validation import train_test_split from sklearn.ensemble import GradientBoostingRegressor from sklearn.ensemble.partial_dependence import plot_partial_dependence from sklearn.ensemble.partial_dependence import partial_dependence from sklearn.datasets.california_housing import fetch_california_housing Alltogether def main (): cal_housing = fetch_california_housing() # split 80/20 train-test X_train, X_test, y_train, y_test = train_test_split(cal_housing . data, cal_housing . target, test_size = 0.2 , random_state = 1 ) names = cal_housing . feature_names print ( \"Training GBRT...\" , flush = True , end = '' ) clf = GradientBoostingRegressor(n_estimators = 100 , max_depth = 4 , learning_rate = 0.1 , loss = 'huber' , random_state = 1 ) clf . fit(X_train, y_train) print ( \" done.\" ) print ( 'Convenience plot with ``partial_dependence_plots``' ) features = [ 0 , 5 , 1 , 2 , ( 5 , 1 )] fig, axs = plot_partial_dependence(clf, X_train, features, feature_names = names, n_jobs = 3 , grid_resolution = 50 ) fig . suptitle( 'Partial dependence of house value on nonlocation features \\n ' 'for the California housing dataset' ) plt . subplots_adjust(top = 0.9 ) # tight_layout causes overlap with suptitle print ( 'Custom 3d plot via ``partial_dependence``' ) fig = plt . figure() target_feature = ( 1 , 5 ) pdp, axes = partial_dependence(clf, target_feature, X = X_train, grid_resolution = 50 ) XX, YY = np . meshgrid(axes[ 0 ], axes[ 1 ]) Z = pdp[ 0 ] . reshape( list ( map (np . size, axes))) . T ax = Axes3D(fig) surf = ax . plot_surface(XX, YY, Z, rstride = 1 , cstride = 1 , cmap = plt . cm . BuPu) ax . set_xlabel(names[target_feature[ 0 ]]) ax . set_ylabel(names[target_feature[ 1 ]]) ax . set_zlabel( 'Partial dependence' ) # pretty init view ax . view_init(elev = 22 , azim = 122 ) plt . colorbar(surf) plt . suptitle( 'Partial dependence of house value on median age and ' 'average occupancy' ) plt . subplots_adjust(top = 0.9 ) plt . show() # Needed on Windows because plot_partial_dependence uses multiprocessing if __name__ == '__main__' : main() Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /home/bitnami/scikit_learn_data Training GBRT... done. Convenience plot with ``partial_dependence_plots`` Custom 3d plot via ``partial_dependence``","title":"16. Partial Dependence Plots (source)"},{"location":"Classifiers/Ensamble/ensamble/#17-plot-the-decision-boundaries-of-a-votingclassifier-source","text":"Plot the decision boundaries of a VotingClassifier for two features of the Iris dataset. Plot the class probabilities of the first sample in a toy dataset predicted by three different classifiers and averaged by the VotingClassifier . First, three exemplary classifiers are initialized ( DecisionTreeClassifier , KNeighborsClassifier , and SVC ) and used to initialize a soft-voting VotingClassifier with weights [2, 1, 2] , which means that the predicted probabilities of the DecisionTreeClassifier and SVC count 5 times as much as the weights of the KNeighborsClassifier classifier when the averaged probability is calculated. from itertools import product import numpy as np import matplotlib.pyplot as plt from sklearn import datasets from sklearn.tree import DecisionTreeClassifier from sklearn.neighbors import KNeighborsClassifier from sklearn.svm import SVC from sklearn.ensemble import VotingClassifier Data # Loading some example data iris = datasets . load_iris() X = iris . data[:, [ 0 , 2 ]] y = iris . target Model # Training classifiers clf1 = DecisionTreeClassifier(max_depth = 4 ) clf2 = KNeighborsClassifier(n_neighbors = 7 ) clf3 = SVC(kernel = 'rbf' , probability = True ) eclf = VotingClassifier(estimators = [( 'dt' , clf1), ( 'knn' , clf2), ( 'svc' , clf3)], voting = 'soft' , weights = [ 2 , 1 , 2 ]) clf1 . fit(X, y) clf2 . fit(X, y) clf3 . fit(X, y) eclf . fit(X, y) VotingClassifier(estimators=[('dt', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4, max_features=None, max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=None, splitter='best')), ('knn', ...', max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001, verbose=False))], voting='soft', weights=[2, 1, 2]) Plot # Plotting decision regions x_min, x_max = X[:, 0 ] . min() - 1 , X[:, 0 ] . max() + 1 y_min, y_max = X[:, 1 ] . min() - 1 , X[:, 1 ] . max() + 1 xx, yy = np . meshgrid(np . arange(x_min, x_max, 0.1 ), np . arange(y_min, y_max, 0.1 )) f, axarr = plt . subplots( 2 , 2 , sharex = 'col' , sharey = 'row' , figsize = ( 10 , 8 )) for idx, clf, tt in zip (product([ 0 , 1 ], [ 0 , 1 ]), [clf1, clf2, clf3, eclf], [ 'Decision Tree (depth=4)' , 'KNN (k=7)' , 'Kernel SVM' , 'Soft Voting' ]): Z = clf . predict(np . c_[xx . ravel(), yy . ravel()]) Z = Z . reshape(xx . shape) axarr[idx[ 0 ], idx[ 1 ]] . contourf(xx, yy, Z, alpha = 0.4 ) axarr[idx[ 0 ], idx[ 1 ]] . scatter(X[:, 0 ], X[:, 1 ], c = y, alpha = 0.8 ) axarr[idx[ 0 ], idx[ 1 ]] . set_title(tt) plt . show()","title":"17. Plot the decision boundaries of a VotingClassifier (source)"},{"location":"Classifiers/Ensamble/ensamble/#18-plot-class-probabilities-calculated-by-the-votingclassifier-source","text":"Plot the class probabilities of the first sample in a toy dataset predicted by three different classifiers and averaged by the VotingClassifier . First, three examplary classifiers are initialized ( LogisticRegression , GaussianNB , and RandomForestClassifier ) and used to initialize a soft-voting VotingClassifier with weights [1, 1, 5] , which means that the predicted probabilities of the RandomForestClassifier count 5 times as much as the weights of the other classifiers when the averaged probability is calculated. To visualize the probability weighting, we fit each classifier on the training set and plot the predicted class probabilities for the first sample in this example dataset. import numpy as np import matplotlib.pyplot as plt from sklearn.linear_model import LogisticRegression from sklearn.naive_bayes import GaussianNB from sklearn.ensemble import RandomForestClassifier from sklearn.ensemble import VotingClassifier Model clf1 = LogisticRegression(random_state = 123 ) clf2 = RandomForestClassifier(random_state = 123 ) clf3 = GaussianNB() X = np . array([[ - 1.0 , - 1.0 ], [ - 1.2 , - 1.4 ], [ - 3.4 , - 2.2 ], [ 1.1 , 1.2 ]]) y = np . array([ 1 , 1 , 2 , 2 ]) eclf = VotingClassifier(estimators = [( 'lr' , clf1), ( 'rf' , clf2), ( 'gnb' , clf3)], voting = 'soft' , weights = [ 1 , 1 , 5 ]) Predict # predict class probabilities for all classifiers probas = [c . fit(X, y) . predict_proba(X) for c in (clf1, clf2, clf3, eclf)] # get class probabilities for the first sample in the dataset class1_1 = [pr[ 0 , 0 ] for pr in probas] class2_1 = [pr[ 0 , 1 ] for pr in probas] Plot # plotting N = 4 # number of groups ind = np . arange(N) # group positions width = 0.35 # bar width fig, ax = plt . subplots() # bars for classifier 1-3 p1 = ax . bar(ind, np . hstack(([class1_1[: - 1 ], [ 0 ]])), width, color = 'green' ) p2 = ax . bar(ind + width, np . hstack(([class2_1[: - 1 ], [ 0 ]])), width, color = 'lightgreen' ) # bars for VotingClassifier p3 = ax . bar(ind, [ 0 , 0 , 0 , class1_1[ - 1 ]], width, color = 'blue' ) p4 = ax . bar(ind + width, [ 0 , 0 , 0 , class2_1[ - 1 ]], width, color = 'steelblue' ) # plot annotations plt . axvline( 2.8 , color = 'k' , linestyle = 'dashed' ) ax . set_xticks(ind + width) ax . set_xticklabels([ 'LogisticRegression \\n weight 1' , 'GaussianNB \\n weight 1' , 'RandomForestClassifier \\n weight 5' , 'VotingClassifier \\n (average probabilities)' ], rotation = 40 , ha = 'right' ) plt . ylim([ 0 , 1 ]) plt . title( 'Class probabilities for sample 1 by different classifiers' ) plt . legend([p1[ 0 ], p2[ 0 ]], [ 'class 1' , 'class 2' ], loc = 'upper left' ) plt . show()","title":"18. Plot class probabilities calculated by the VotingClassifier (source)"},{"location":"Classifiers/Logistic/Logistic/","text":"Classification Probability plot 1. Plot classification probability ( source ) Plot the classification probability for different classifiers. We use a 3 class dataset, and we classify it with a Support Vector classifier ( sklearn.svm.SVC ), L1 and L2 penalized logistic regression with either a One-Vs-Rest or multinomial setting ( sklearn.linear_model.LogisticRegression ), and Gaussian process classification ( sklearn.gaussian_process.kernels.RBF ) The logistic regression is not a multiclass classifier out of the box. As a result it can identify only the first class. import matplotlib.pyplot as plt import numpy as np #========Models=================== from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC from sklearn.gaussian_process import GaussianProcessClassifier from sklearn.gaussian_process.kernels import RBF #======= data set =============================== from sklearn import datasets Data iris = datasets . load_iris() X = iris . data[:, 0 : 2 ] # we only take the first two features for visualization y = iris . target n_features = X . shape[ 1 ] C = 1.0 kernel = 1.0 * RBF([ 1.0 , 1.0 ]) # for GPC # Create different classifiers. The logistic regression cannot do # multiclass out of the box. classifiers = { 'L1 logistic' : LogisticRegression(C = C, penalty = 'l1' ), 'L2 logistic (OvR)' : LogisticRegression(C = C, penalty = 'l2' ), 'Linear SVC' : SVC(kernel = 'linear' , C = C, probability = True , random_state = 0 ), 'L2 logistic (Multinomial)' : LogisticRegression( C = C, solver = 'lbfgs' , multi_class = 'multinomial' ), 'GPC' : GaussianProcessClassifier(kernel) } n_classifiers = len (classifiers) plt . figure(figsize = ( 3 * 2 , n_classifiers * 2 )) plt . subplots_adjust(bottom =. 2 , top =. 95 ) xx = np . linspace( 3 , 9 , 100 ) yy = np . linspace( 1 , 5 , 100 ) . T xx, yy = np . meshgrid(xx, yy) Xfull = np . c_[xx . ravel(), yy . ravel()] for index, (name, classifier) in enumerate (classifiers . items()): classifier . fit(X, y) y_pred = classifier . predict(X) classif_rate = np . mean(y_pred . ravel() == y . ravel()) * 100 print ( \"classif_rate for %s : %f \" % (name, classif_rate)) # View probabilities= probas = classifier . predict_proba(Xfull) n_classes = np . unique(y_pred) . size for k in range (n_classes): plt . subplot(n_classifiers, n_classes, index * n_classes + k + 1 ) plt . title( \"Class %d \" % k) if k == 0 : plt . ylabel(name) imshow_handle = plt . imshow(probas[:, k] . reshape(( 100 , 100 )), extent = ( 3 , 9 , 1 , 5 ), origin = 'lower' ) plt . xticks(()) plt . yticks(()) idx = (y_pred == k) if idx . any(): plt . scatter(X[idx, 0 ], X[idx, 1 ], marker = 'o' , c = 'k' ) ax = plt . axes([ 0.15 , 0.04 , 0.7 , 0.05 ]) plt . title( \"Probability\" ) plt . colorbar(imshow_handle, cax = ax, orientation = 'horizontal' ) plt . show() classif_rate for GPC : 82.666667 classif_rate for L2 logistic (OvR) : 76.666667 classif_rate for L1 logistic : 79.333333 classif_rate for L2 logistic (Multinomial) : 82.000000 classif_rate for Linear SVC : 82.000000 Decision boundaries 2. Classifier comparison ( source ) A comparison of a several classifiers in scikit-learn on synthetic datasets. The point of this example is to illustrate the nature of decision boundaries of different classifiers. This should be taken with a grain of salt, as the intuition conveyed by these examples does not necessarily carry over to real datasets. Particularly in high-dimensional spaces, data can more easily be separated linearly and the simplicity of classifiers such as naive Bayes and linear SVMs might lead to better generalization than is achieved by other classifiers. The plots show training points in solid colors and testing points semi-transparent. The lower right shows the classification accuracy on the test set. Models: sklearn.tree.DecisionTreeClassifier sklearn.naive_bayes.GaussianNB sklearn.neighbors.KNeighborsClassifier sklearn.gaussian_process.GaussianProcessClassifier sklearn.gaussian_process.kernels.RBF sklearn.svm.SVC sklearn.ensemble.RandomForestClassifier sklearn.ensemble.AdaBoostClassifier sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis sklearn.neural_network.MLPClassifier import numpy as np import matplotlib.pyplot as plt from matplotlib.colors import ListedColormap #============= preprocessing =========================== from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler #============= models==================================== from sklearn.neural_network import MLPClassifier from sklearn.neighbors import KNeighborsClassifier from sklearn.svm import SVC from sklearn.gaussian_process import GaussianProcessClassifier from sklearn.gaussian_process.kernels import RBF from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier from sklearn.naive_bayes import GaussianNB from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis #========== data set===================================================== from sklearn.datasets import make_moons, make_circles, make_classification h = . 02 # step size in the mesh Data preparation X, y = make_classification(n_features = 2 , n_redundant = 0 , n_informative = 2 , random_state = 1 , n_clusters_per_class = 1 ) rng = np . random . RandomState( 2 ) X += 2 * rng . uniform(size = X . shape) linearly_separable = (X, y) datasets = [make_moons(noise = 0.3 , random_state = 0 ), make_circles(noise = 0.2 , factor = 0.5 , random_state = 1 ), linearly_separable ] Assign Classifiers names = [ \"Nearest Neighbors\" , \"Linear SVM\" , \"RBF SVM\" , \"Gaussian Process\" , \"Decision Tree\" , \"Random Forest\" , \"Neural Net\" , \"AdaBoost\" , \"Naive Bayes\" , \"QDA\" ] classifiers = [ KNeighborsClassifier( 3 ), SVC(kernel = \"linear\" , C = 0.025 ), SVC(gamma = 2 , C = 1 ), GaussianProcessClassifier( 1.0 * RBF( 1.0 ), warm_start = True ), DecisionTreeClassifier(max_depth = 5 ), RandomForestClassifier(max_depth = 5 , n_estimators = 10 , max_features = 1 ), MLPClassifier(alpha = 1 ), AdaBoostClassifier(), GaussianNB(), QuadraticDiscriminantAnalysis()] Plot figure = plt . figure(figsize = ( 27 , 9 )) i = 1 # iterate over datasets for ds_cnt, ds in enumerate (datasets): # preprocess dataset, split into training and test part X, y = ds X = StandardScaler() . fit_transform(X) X_train, X_test, y_train, y_test = \\ train_test_split(X, y, test_size =. 4 , random_state = 42 ) x_min, x_max = X[:, 0 ] . min() - . 5 , X[:, 0 ] . max() + . 5 y_min, y_max = X[:, 1 ] . min() - . 5 , X[:, 1 ] . max() + . 5 xx, yy = np . meshgrid(np . arange(x_min, x_max, h), np . arange(y_min, y_max, h)) # just plot the dataset first cm = plt . cm . RdBu cm_bright = ListedColormap([ '#FF0000' , '#0000FF' ]) ax = plt . subplot( len (datasets), len (classifiers) + 1 , i) if ds_cnt == 0 : ax . set_title( \"Input data\" ) # Plot the training points ax . scatter(X_train[:, 0 ], X_train[:, 1 ], c = y_train, cmap = cm_bright) # and testing points ax . scatter(X_test[:, 0 ], X_test[:, 1 ], c = y_test, cmap = cm_bright, alpha = 0.6 ) ax . set_xlim(xx . min(), xx . max()) ax . set_ylim(yy . min(), yy . max()) ax . set_xticks(()) ax . set_yticks(()) i += 1 # iterate over classifiers for name, clf in zip (names, classifiers): ax = plt . subplot( len (datasets), len (classifiers) + 1 , i) clf . fit(X_train, y_train) score = clf . score(X_test, y_test) # Plot the decision boundary. For that, we will assign a color to each # point in the mesh [x_min, x_max]x[y_min, y_max]. if hasattr (clf, \"decision_function\" ): Z = clf . decision_function(np . c_[xx . ravel(), yy . ravel()]) else : Z = clf . predict_proba(np . c_[xx . ravel(), yy . ravel()])[:, 1 ] # Put the result into a color plot Z = Z . reshape(xx . shape) ax . contourf(xx, yy, Z, cmap = cm, alpha =. 8 ) # Plot also the training points ax . scatter(X_train[:, 0 ], X_train[:, 1 ], c = y_train, cmap = cm_bright) # and testing points ax . scatter(X_test[:, 0 ], X_test[:, 1 ], c = y_test, cmap = cm_bright, alpha = 0.6 ) ax . set_xlim(xx . min(), xx . max()) ax . set_ylim(yy . min(), yy . max()) ax . set_xticks(()) ax . set_yticks(()) if ds_cnt == 0 : ax . set_title(name) ax . text(xx . max() - . 3 , yy . min() + . 3 , ( ' %.2f ' % score) . lstrip( '0' ), size = 15 , horizontalalignment = 'right' ) i += 1 plt . tight_layout() plt . show() /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet. % (), ConvergenceWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet. % (), ConvergenceWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet. % (), ConvergenceWarning) Digit Classifier 3. Recognizing hand-written digits( source ) An example showing how the scikit-learn can be used to recognize images of hand-written digits. # Standard scientific Python imports import matplotlib.pyplot as plt # Import datasets, classifiers and performance metrics from sklearn import datasets, svm, metrics The data that we are interested in is made of 8x8 images of digits, let's have a look at the first 4 images, stored in the images attribute of the dataset. If we were working from image files, we could load them using matplotlib.pyplot.imread. Note that each image must have the same size. For these images, we know which digit they represent: it is given in the 'target' of the dataset. The digits dataset digits = datasets . load_digits() To apply a classifier on this data, we need to flatten the image, to turn the data in a (samples, feature) matrix: n_samples = len (digits . images) data = digits . images . reshape((n_samples, - 1 )) Create a classifier: a support vector classifier classifier = svm . SVC(gamma = 0.001 ) We learn the digits on the first half of the digits classifier . fit(data[:n_samples / 2 ], digits . target[:n_samples / 2 ]) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:1: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future if __name__ == '__main__': SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma=0.001, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) Now predict the value of the digit on the second half: expected = digits . target[n_samples / 2 :] predicted = classifier . predict(data[n_samples / 2 :]) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:1: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future if __name__ == '__main__': /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:2: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future from ipykernel import kernelapp as app print ( \"Classification report for classifier %s : \\n %s \\n \" % (classifier, metrics . classification_report(expected, predicted))) print ( \"Confusion matrix: \\n %s \" % metrics . confusion_matrix(expected, predicted)) Classification report for classifier SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma=0.001, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False): precision recall f1-score support 0 1.00 0.99 0.99 88 1 0.99 0.97 0.98 91 2 0.99 0.99 0.99 86 3 0.98 0.87 0.92 91 4 0.99 0.96 0.97 92 5 0.95 0.97 0.96 91 6 0.99 0.99 0.99 91 7 0.96 0.99 0.97 89 8 0.94 1.00 0.97 88 9 0.93 0.98 0.95 92 avg / total 0.97 0.97 0.97 899 Confusion matrix: [[87 0 0 0 1 0 0 0 0 0] [ 0 88 1 0 0 0 0 0 1 1] [ 0 0 85 1 0 0 0 0 0 0] [ 0 0 0 79 0 3 0 4 5 0] [ 0 0 0 0 88 0 0 0 0 4] [ 0 0 0 0 0 88 1 0 0 2] [ 0 1 0 0 0 0 90 0 0 0] [ 0 0 0 0 0 1 0 88 0 0] [ 0 0 0 0 0 0 0 0 88 0] [ 0 0 0 1 0 1 0 0 0 90]] images_and_labels = list ( zip (digits . images, digits . target)) for index, (image, label) in enumerate (images_and_labels[: 4 ]): plt . subplot( 2 , 4 , index + 1 ) plt . axis( 'off' ) plt . imshow(image, cmap = plt . cm . gray_r, interpolation = 'nearest' ) plt . title( 'Training: %i ' % label) images_and_predictions = list ( zip (digits . images[n_samples / 2 :], predicted)) for index, (image, prediction) in enumerate (images_and_predictions[: 4 ]): plt . subplot( 2 , 4 , index + 5 ) plt . axis( 'off' ) plt . imshow(image, cmap = plt . cm . gray_r, interpolation = 'nearest' ) plt . title( 'Prediction: %i ' % prediction) plt . show() /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:10: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future Discriminant Analysis 4. Normal and Shrinkage Linear Discriminant Analysis for classification( source ) from __future__ import division import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_blobs from sklearn.discriminant_analysis import LinearDiscriminantAnalysis n_train = 20 # samples for training n_test = 200 # samples for testing n_averages = 50 # how often to repeat classification n_features_max = 75 # maximum number of features step = 4 # step size for the calculation def generate_data (n_samples, n_features): \"\"\"Generate random blob-ish data with noisy features. This returns an array of input data with shape `(n_samples, n_features)` and an array of `n_samples` target labels. Only one feature contains discriminative information, the other features contain only noise. \"\"\" X, y = make_blobs(n_samples = n_samples, n_features = 1 , centers = [[ - 2 ], [ 2 ]]) # add non-discriminative features if n_features > 1 : X = np . hstack([X, np . random . randn(n_samples, n_features - 1 )]) return X, y acc_clf1, acc_clf2 = [], [] n_features_range = range ( 1 , n_features_max + 1 , step) for n_features in n_features_range: score_clf1, score_clf2 = 0 , 0 for _ in range (n_averages): X, y = generate_data(n_train, n_features) clf1 = LinearDiscriminantAnalysis(solver = 'lsqr' , shrinkage = 'auto' ) . fit(X, y) clf2 = LinearDiscriminantAnalysis(solver = 'lsqr' , shrinkage = None ) . fit(X, y) X, y = generate_data(n_test, n_features) score_clf1 += clf1 . score(X, y) score_clf2 += clf2 . score(X, y) acc_clf1 . append(score_clf1 / n_averages) acc_clf2 . append(score_clf2 / n_averages) features_samples_ratio = np . array(n_features_range) / n_train plt . plot(features_samples_ratio, acc_clf1, linewidth = 2 , label = \"Linear Discriminant Analysis with shrinkage\" , color = 'navy' ) plt . plot(features_samples_ratio, acc_clf2, linewidth = 2 , label = \"Linear Discriminant Analysis\" , color = 'gold' ) plt . xlabel( 'n_features / n_samples' ) plt . ylabel( 'Classification accuracy' ) plt . legend(loc = 1 , prop = { 'size' : 12 }) plt . suptitle( 'Linear Discriminant Analysis vs. \\ shrinkage Linear Discriminant Analysis (1 discriminative feature)' ) plt . show() Discriminant Analysis 5. Linear and Quadratic Discriminant Analysis with covariance ellipsoid ( source ) This example plots the covariance ellipsoids of each class and decision boundary learned by LDA and QDA. The ellipsoids display the double standard deviation for each class. With LDA, the standard deviation is the same for all the classes, while each class has its own standard deviation with QDA. from scipy import linalg import numpy as np import matplotlib.pyplot as plt import matplotlib as mpl from matplotlib import colors from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis colormap cmap = colors . LinearSegmentedColormap( 'red_blue_classes' , { 'red' : [( 0 , 1 , 1 ), ( 1 , 0.7 , 0.7 )], 'green' : [( 0 , 0.7 , 0.7 ), ( 1 , 0.7 , 0.7 )], 'blue' : [( 0 , 0.7 , 0.7 ), ( 1 , 1 , 1 )]}) plt . cm . register_cmap(cmap = cmap) generate datasets def dataset_fixed_cov (): '''Generate 2 Gaussians samples with the same covariance matrix''' n, dim = 300 , 2 np . random . seed( 0 ) C = np . array([[ 0. , - 0.23 ], [ 0.83 , . 23 ]]) X = np . r_[np . dot(np . random . randn(n, dim), C), np . dot(np . random . randn(n, dim), C) + np . array([ 1 , 1 ])] y = np . hstack((np . zeros(n), np . ones(n))) return X, y def dataset_cov (): '''Generate 2 Gaussians samples with different covariance matrices''' n, dim = 300 , 2 np . random . seed( 0 ) C = np . array([[ 0. , - 1. ], [ 2.5 , . 7 ]]) * 2. X = np . r_[np . dot(np . random . randn(n, dim), C), np . dot(np . random . randn(n, dim), C . T) + np . array([ 1 , 4 ])] y = np . hstack((np . zeros(n), np . ones(n))) return X, y plot functions def plot_data (lda, X, y, y_pred, fig_index): splot = plt . subplot( 2 , 2 , fig_index) if fig_index == 1 : plt . title( 'Linear Discriminant Analysis' ) plt . ylabel( 'Data with fixed covariance' ) elif fig_index == 2 : plt . title( 'Quadratic Discriminant Analysis' ) elif fig_index == 3 : plt . ylabel( 'Data with varying covariances' ) tp = (y == y_pred) # True Positive tp0, tp1 = tp[y == 0 ], tp[y == 1 ] X0, X1 = X[y == 0 ], X[y == 1 ] X0_tp, X0_fp = X0[tp0], X0[ ~ tp0] X1_tp, X1_fp = X1[tp1], X1[ ~ tp1] alpha = 0.5 # class 0: dots plt . plot(X0_tp[:, 0 ], X0_tp[:, 1 ], 'o' , alpha = alpha, color = 'red' ) plt . plot(X0_fp[:, 0 ], X0_fp[:, 1 ], '*' , alpha = alpha, color = '#990000' ) # dark red # class 1: dots plt . plot(X1_tp[:, 0 ], X1_tp[:, 1 ], 'o' , alpha = alpha, color = 'blue' ) plt . plot(X1_fp[:, 0 ], X1_fp[:, 1 ], '*' , alpha = alpha, color = '#000099' ) # dark blue # class 0 and 1 : areas nx, ny = 200 , 100 x_min, x_max = plt . xlim() y_min, y_max = plt . ylim() xx, yy = np . meshgrid(np . linspace(x_min, x_max, nx), np . linspace(y_min, y_max, ny)) Z = lda . predict_proba(np . c_[xx . ravel(), yy . ravel()]) Z = Z[:, 1 ] . reshape(xx . shape) plt . pcolormesh(xx, yy, Z, cmap = 'red_blue_classes' , norm = colors . Normalize( 0. , 1. )) plt . contour(xx, yy, Z, [ 0.5 ], linewidths = 2. , colors = 'k' ) # means plt . plot(lda . means_[ 0 ][ 0 ], lda . means_[ 0 ][ 1 ], 'o' , color = 'black' , markersize = 10 ) plt . plot(lda . means_[ 1 ][ 0 ], lda . means_[ 1 ][ 1 ], 'o' , color = 'black' , markersize = 10 ) return splot def plot_ellipse (splot, mean, cov, color): v, w = linalg . eigh(cov) u = w[ 0 ] / linalg . norm(w[ 0 ]) angle = np . arctan(u[ 1 ] / u[ 0 ]) angle = 180 * angle / np . pi # convert to degrees # filled Gaussian at 2 standard deviation ell = mpl . patches . Ellipse(mean, 2 * v[ 0 ] ** 0.5 , 2 * v[ 1 ] ** 0.5 , 180 + angle, facecolor = color, edgecolor = 'yellow' , linewidth = 2 , zorder = 2 ) ell . set_clip_box(splot . bbox) ell . set_alpha( 0.5 ) splot . add_artist(ell) splot . set_xticks(()) splot . set_yticks(()) def plot_lda_cov (lda, splot): plot_ellipse(splot, lda . means_[ 0 ], lda . covariance_, 'red' ) plot_ellipse(splot, lda . means_[ 1 ], lda . covariance_, 'blue' ) def plot_qda_cov (qda, splot): plot_ellipse(splot, qda . means_[ 0 ], qda . covariances_[ 0 ], 'red' ) plot_ellipse(splot, qda . means_[ 1 ], qda . covariances_[ 1 ], 'blue' ) for i, (X, y) in enumerate ([dataset_fixed_cov(), dataset_cov()]): # Linear Discriminant Analysis lda = LinearDiscriminantAnalysis(solver = \"svd\" , store_covariance = True ) y_pred = lda . fit(X, y) . predict(X) splot = plot_data(lda, X, y, y_pred, fig_index = 2 * i + 1 ) plot_lda_cov(lda, splot) plt . axis( 'tight' ) # Quadratic Discriminant Analysis qda = QuadraticDiscriminantAnalysis(store_covariances = True ) y_pred = qda . fit(X, y) . predict(X) splot = plot_data(qda, X, y, y_pred, fig_index = 2 * i + 2 ) plot_qda_cov(qda, splot) plt . axis( 'tight' ) plt . suptitle( 'Linear Discriminant Analysis vs Quadratic Discriminant Analysis' ) plt . show()","title":"Logistic"},{"location":"Classifiers/Logistic/Logistic/#classification","text":"","title":"Classification"},{"location":"Classifiers/Logistic/Logistic/#probability-plot","text":"","title":"Probability plot"},{"location":"Classifiers/Logistic/Logistic/#1-plot-classification-probability-source","text":"Plot the classification probability for different classifiers. We use a 3 class dataset, and we classify it with a Support Vector classifier ( sklearn.svm.SVC ), L1 and L2 penalized logistic regression with either a One-Vs-Rest or multinomial setting ( sklearn.linear_model.LogisticRegression ), and Gaussian process classification ( sklearn.gaussian_process.kernels.RBF ) The logistic regression is not a multiclass classifier out of the box. As a result it can identify only the first class. import matplotlib.pyplot as plt import numpy as np #========Models=================== from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC from sklearn.gaussian_process import GaussianProcessClassifier from sklearn.gaussian_process.kernels import RBF #======= data set =============================== from sklearn import datasets","title":"1. Plot classification probability (source)"},{"location":"Classifiers/Logistic/Logistic/#data","text":"iris = datasets . load_iris() X = iris . data[:, 0 : 2 ] # we only take the first two features for visualization y = iris . target n_features = X . shape[ 1 ] C = 1.0 kernel = 1.0 * RBF([ 1.0 , 1.0 ]) # for GPC # Create different classifiers. The logistic regression cannot do # multiclass out of the box. classifiers = { 'L1 logistic' : LogisticRegression(C = C, penalty = 'l1' ), 'L2 logistic (OvR)' : LogisticRegression(C = C, penalty = 'l2' ), 'Linear SVC' : SVC(kernel = 'linear' , C = C, probability = True , random_state = 0 ), 'L2 logistic (Multinomial)' : LogisticRegression( C = C, solver = 'lbfgs' , multi_class = 'multinomial' ), 'GPC' : GaussianProcessClassifier(kernel) } n_classifiers = len (classifiers) plt . figure(figsize = ( 3 * 2 , n_classifiers * 2 )) plt . subplots_adjust(bottom =. 2 , top =. 95 ) xx = np . linspace( 3 , 9 , 100 ) yy = np . linspace( 1 , 5 , 100 ) . T xx, yy = np . meshgrid(xx, yy) Xfull = np . c_[xx . ravel(), yy . ravel()] for index, (name, classifier) in enumerate (classifiers . items()): classifier . fit(X, y) y_pred = classifier . predict(X) classif_rate = np . mean(y_pred . ravel() == y . ravel()) * 100 print ( \"classif_rate for %s : %f \" % (name, classif_rate)) # View probabilities= probas = classifier . predict_proba(Xfull) n_classes = np . unique(y_pred) . size for k in range (n_classes): plt . subplot(n_classifiers, n_classes, index * n_classes + k + 1 ) plt . title( \"Class %d \" % k) if k == 0 : plt . ylabel(name) imshow_handle = plt . imshow(probas[:, k] . reshape(( 100 , 100 )), extent = ( 3 , 9 , 1 , 5 ), origin = 'lower' ) plt . xticks(()) plt . yticks(()) idx = (y_pred == k) if idx . any(): plt . scatter(X[idx, 0 ], X[idx, 1 ], marker = 'o' , c = 'k' ) ax = plt . axes([ 0.15 , 0.04 , 0.7 , 0.05 ]) plt . title( \"Probability\" ) plt . colorbar(imshow_handle, cax = ax, orientation = 'horizontal' ) plt . show() classif_rate for GPC : 82.666667 classif_rate for L2 logistic (OvR) : 76.666667 classif_rate for L1 logistic : 79.333333 classif_rate for L2 logistic (Multinomial) : 82.000000 classif_rate for Linear SVC : 82.000000","title":"Data"},{"location":"Classifiers/Logistic/Logistic/#decision-boundaries","text":"","title":"Decision boundaries"},{"location":"Classifiers/Logistic/Logistic/#2-classifier-comparison-source","text":"A comparison of a several classifiers in scikit-learn on synthetic datasets. The point of this example is to illustrate the nature of decision boundaries of different classifiers. This should be taken with a grain of salt, as the intuition conveyed by these examples does not necessarily carry over to real datasets. Particularly in high-dimensional spaces, data can more easily be separated linearly and the simplicity of classifiers such as naive Bayes and linear SVMs might lead to better generalization than is achieved by other classifiers. The plots show training points in solid colors and testing points semi-transparent. The lower right shows the classification accuracy on the test set. Models: sklearn.tree.DecisionTreeClassifier sklearn.naive_bayes.GaussianNB sklearn.neighbors.KNeighborsClassifier sklearn.gaussian_process.GaussianProcessClassifier sklearn.gaussian_process.kernels.RBF sklearn.svm.SVC sklearn.ensemble.RandomForestClassifier sklearn.ensemble.AdaBoostClassifier sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis sklearn.neural_network.MLPClassifier import numpy as np import matplotlib.pyplot as plt from matplotlib.colors import ListedColormap #============= preprocessing =========================== from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler #============= models==================================== from sklearn.neural_network import MLPClassifier from sklearn.neighbors import KNeighborsClassifier from sklearn.svm import SVC from sklearn.gaussian_process import GaussianProcessClassifier from sklearn.gaussian_process.kernels import RBF from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier from sklearn.naive_bayes import GaussianNB from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis #========== data set===================================================== from sklearn.datasets import make_moons, make_circles, make_classification h = . 02 # step size in the mesh","title":"2. Classifier comparison (source)"},{"location":"Classifiers/Logistic/Logistic/#data-preparation","text":"X, y = make_classification(n_features = 2 , n_redundant = 0 , n_informative = 2 , random_state = 1 , n_clusters_per_class = 1 ) rng = np . random . RandomState( 2 ) X += 2 * rng . uniform(size = X . shape) linearly_separable = (X, y) datasets = [make_moons(noise = 0.3 , random_state = 0 ), make_circles(noise = 0.2 , factor = 0.5 , random_state = 1 ), linearly_separable ]","title":"Data preparation"},{"location":"Classifiers/Logistic/Logistic/#assign-classifiers","text":"names = [ \"Nearest Neighbors\" , \"Linear SVM\" , \"RBF SVM\" , \"Gaussian Process\" , \"Decision Tree\" , \"Random Forest\" , \"Neural Net\" , \"AdaBoost\" , \"Naive Bayes\" , \"QDA\" ] classifiers = [ KNeighborsClassifier( 3 ), SVC(kernel = \"linear\" , C = 0.025 ), SVC(gamma = 2 , C = 1 ), GaussianProcessClassifier( 1.0 * RBF( 1.0 ), warm_start = True ), DecisionTreeClassifier(max_depth = 5 ), RandomForestClassifier(max_depth = 5 , n_estimators = 10 , max_features = 1 ), MLPClassifier(alpha = 1 ), AdaBoostClassifier(), GaussianNB(), QuadraticDiscriminantAnalysis()]","title":"Assign Classifiers"},{"location":"Classifiers/Logistic/Logistic/#plot","text":"figure = plt . figure(figsize = ( 27 , 9 )) i = 1 # iterate over datasets for ds_cnt, ds in enumerate (datasets): # preprocess dataset, split into training and test part X, y = ds X = StandardScaler() . fit_transform(X) X_train, X_test, y_train, y_test = \\ train_test_split(X, y, test_size =. 4 , random_state = 42 ) x_min, x_max = X[:, 0 ] . min() - . 5 , X[:, 0 ] . max() + . 5 y_min, y_max = X[:, 1 ] . min() - . 5 , X[:, 1 ] . max() + . 5 xx, yy = np . meshgrid(np . arange(x_min, x_max, h), np . arange(y_min, y_max, h)) # just plot the dataset first cm = plt . cm . RdBu cm_bright = ListedColormap([ '#FF0000' , '#0000FF' ]) ax = plt . subplot( len (datasets), len (classifiers) + 1 , i) if ds_cnt == 0 : ax . set_title( \"Input data\" ) # Plot the training points ax . scatter(X_train[:, 0 ], X_train[:, 1 ], c = y_train, cmap = cm_bright) # and testing points ax . scatter(X_test[:, 0 ], X_test[:, 1 ], c = y_test, cmap = cm_bright, alpha = 0.6 ) ax . set_xlim(xx . min(), xx . max()) ax . set_ylim(yy . min(), yy . max()) ax . set_xticks(()) ax . set_yticks(()) i += 1 # iterate over classifiers for name, clf in zip (names, classifiers): ax = plt . subplot( len (datasets), len (classifiers) + 1 , i) clf . fit(X_train, y_train) score = clf . score(X_test, y_test) # Plot the decision boundary. For that, we will assign a color to each # point in the mesh [x_min, x_max]x[y_min, y_max]. if hasattr (clf, \"decision_function\" ): Z = clf . decision_function(np . c_[xx . ravel(), yy . ravel()]) else : Z = clf . predict_proba(np . c_[xx . ravel(), yy . ravel()])[:, 1 ] # Put the result into a color plot Z = Z . reshape(xx . shape) ax . contourf(xx, yy, Z, cmap = cm, alpha =. 8 ) # Plot also the training points ax . scatter(X_train[:, 0 ], X_train[:, 1 ], c = y_train, cmap = cm_bright) # and testing points ax . scatter(X_test[:, 0 ], X_test[:, 1 ], c = y_test, cmap = cm_bright, alpha = 0.6 ) ax . set_xlim(xx . min(), xx . max()) ax . set_ylim(yy . min(), yy . max()) ax . set_xticks(()) ax . set_yticks(()) if ds_cnt == 0 : ax . set_title(name) ax . text(xx . max() - . 3 , yy . min() + . 3 , ( ' %.2f ' % score) . lstrip( '0' ), size = 15 , horizontalalignment = 'right' ) i += 1 plt . tight_layout() plt . show() /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet. % (), ConvergenceWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet. % (), ConvergenceWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet. % (), ConvergenceWarning)","title":"Plot"},{"location":"Classifiers/Logistic/Logistic/#digit-classifier","text":"","title":"Digit Classifier"},{"location":"Classifiers/Logistic/Logistic/#3-recognizing-hand-written-digitssource","text":"An example showing how the scikit-learn can be used to recognize images of hand-written digits. # Standard scientific Python imports import matplotlib.pyplot as plt # Import datasets, classifiers and performance metrics from sklearn import datasets, svm, metrics The data that we are interested in is made of 8x8 images of digits, let's have a look at the first 4 images, stored in the images attribute of the dataset. If we were working from image files, we could load them using matplotlib.pyplot.imread. Note that each image must have the same size. For these images, we know which digit they represent: it is given in the 'target' of the dataset. The digits dataset digits = datasets . load_digits() To apply a classifier on this data, we need to flatten the image, to turn the data in a (samples, feature) matrix: n_samples = len (digits . images) data = digits . images . reshape((n_samples, - 1 )) Create a classifier: a support vector classifier classifier = svm . SVC(gamma = 0.001 ) We learn the digits on the first half of the digits classifier . fit(data[:n_samples / 2 ], digits . target[:n_samples / 2 ]) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:1: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future if __name__ == '__main__': SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma=0.001, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) Now predict the value of the digit on the second half: expected = digits . target[n_samples / 2 :] predicted = classifier . predict(data[n_samples / 2 :]) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:1: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future if __name__ == '__main__': /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:2: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future from ipykernel import kernelapp as app print ( \"Classification report for classifier %s : \\n %s \\n \" % (classifier, metrics . classification_report(expected, predicted))) print ( \"Confusion matrix: \\n %s \" % metrics . confusion_matrix(expected, predicted)) Classification report for classifier SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma=0.001, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False): precision recall f1-score support 0 1.00 0.99 0.99 88 1 0.99 0.97 0.98 91 2 0.99 0.99 0.99 86 3 0.98 0.87 0.92 91 4 0.99 0.96 0.97 92 5 0.95 0.97 0.96 91 6 0.99 0.99 0.99 91 7 0.96 0.99 0.97 89 8 0.94 1.00 0.97 88 9 0.93 0.98 0.95 92 avg / total 0.97 0.97 0.97 899 Confusion matrix: [[87 0 0 0 1 0 0 0 0 0] [ 0 88 1 0 0 0 0 0 1 1] [ 0 0 85 1 0 0 0 0 0 0] [ 0 0 0 79 0 3 0 4 5 0] [ 0 0 0 0 88 0 0 0 0 4] [ 0 0 0 0 0 88 1 0 0 2] [ 0 1 0 0 0 0 90 0 0 0] [ 0 0 0 0 0 1 0 88 0 0] [ 0 0 0 0 0 0 0 0 88 0] [ 0 0 0 1 0 1 0 0 0 90]] images_and_labels = list ( zip (digits . images, digits . target)) for index, (image, label) in enumerate (images_and_labels[: 4 ]): plt . subplot( 2 , 4 , index + 1 ) plt . axis( 'off' ) plt . imshow(image, cmap = plt . cm . gray_r, interpolation = 'nearest' ) plt . title( 'Training: %i ' % label) images_and_predictions = list ( zip (digits . images[n_samples / 2 :], predicted)) for index, (image, prediction) in enumerate (images_and_predictions[: 4 ]): plt . subplot( 2 , 4 , index + 5 ) plt . axis( 'off' ) plt . imshow(image, cmap = plt . cm . gray_r, interpolation = 'nearest' ) plt . title( 'Prediction: %i ' % prediction) plt . show() /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:10: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future","title":"3. Recognizing hand-written digits(source)"},{"location":"Classifiers/Logistic/Logistic/#discriminant-analysis","text":"","title":"Discriminant Analysis"},{"location":"Classifiers/Logistic/Logistic/#4-normal-and-shrinkage-linear-discriminant-analysis-for-classificationsource","text":"from __future__ import division import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_blobs from sklearn.discriminant_analysis import LinearDiscriminantAnalysis n_train = 20 # samples for training n_test = 200 # samples for testing n_averages = 50 # how often to repeat classification n_features_max = 75 # maximum number of features step = 4 # step size for the calculation def generate_data (n_samples, n_features): \"\"\"Generate random blob-ish data with noisy features. This returns an array of input data with shape `(n_samples, n_features)` and an array of `n_samples` target labels. Only one feature contains discriminative information, the other features contain only noise. \"\"\" X, y = make_blobs(n_samples = n_samples, n_features = 1 , centers = [[ - 2 ], [ 2 ]]) # add non-discriminative features if n_features > 1 : X = np . hstack([X, np . random . randn(n_samples, n_features - 1 )]) return X, y acc_clf1, acc_clf2 = [], [] n_features_range = range ( 1 , n_features_max + 1 , step) for n_features in n_features_range: score_clf1, score_clf2 = 0 , 0 for _ in range (n_averages): X, y = generate_data(n_train, n_features) clf1 = LinearDiscriminantAnalysis(solver = 'lsqr' , shrinkage = 'auto' ) . fit(X, y) clf2 = LinearDiscriminantAnalysis(solver = 'lsqr' , shrinkage = None ) . fit(X, y) X, y = generate_data(n_test, n_features) score_clf1 += clf1 . score(X, y) score_clf2 += clf2 . score(X, y) acc_clf1 . append(score_clf1 / n_averages) acc_clf2 . append(score_clf2 / n_averages) features_samples_ratio = np . array(n_features_range) / n_train plt . plot(features_samples_ratio, acc_clf1, linewidth = 2 , label = \"Linear Discriminant Analysis with shrinkage\" , color = 'navy' ) plt . plot(features_samples_ratio, acc_clf2, linewidth = 2 , label = \"Linear Discriminant Analysis\" , color = 'gold' ) plt . xlabel( 'n_features / n_samples' ) plt . ylabel( 'Classification accuracy' ) plt . legend(loc = 1 , prop = { 'size' : 12 }) plt . suptitle( 'Linear Discriminant Analysis vs. \\ shrinkage Linear Discriminant Analysis (1 discriminative feature)' ) plt . show()","title":"4. Normal and Shrinkage Linear Discriminant Analysis for classification(source)"},{"location":"Classifiers/Logistic/Logistic/#discriminant-analysis_1","text":"","title":"Discriminant Analysis"},{"location":"Classifiers/Logistic/Logistic/#5-linear-and-quadratic-discriminant-analysis-with-covariance-ellipsoid-source","text":"This example plots the covariance ellipsoids of each class and decision boundary learned by LDA and QDA. The ellipsoids display the double standard deviation for each class. With LDA, the standard deviation is the same for all the classes, while each class has its own standard deviation with QDA. from scipy import linalg import numpy as np import matplotlib.pyplot as plt import matplotlib as mpl from matplotlib import colors from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis colormap cmap = colors . LinearSegmentedColormap( 'red_blue_classes' , { 'red' : [( 0 , 1 , 1 ), ( 1 , 0.7 , 0.7 )], 'green' : [( 0 , 0.7 , 0.7 ), ( 1 , 0.7 , 0.7 )], 'blue' : [( 0 , 0.7 , 0.7 ), ( 1 , 1 , 1 )]}) plt . cm . register_cmap(cmap = cmap) generate datasets def dataset_fixed_cov (): '''Generate 2 Gaussians samples with the same covariance matrix''' n, dim = 300 , 2 np . random . seed( 0 ) C = np . array([[ 0. , - 0.23 ], [ 0.83 , . 23 ]]) X = np . r_[np . dot(np . random . randn(n, dim), C), np . dot(np . random . randn(n, dim), C) + np . array([ 1 , 1 ])] y = np . hstack((np . zeros(n), np . ones(n))) return X, y def dataset_cov (): '''Generate 2 Gaussians samples with different covariance matrices''' n, dim = 300 , 2 np . random . seed( 0 ) C = np . array([[ 0. , - 1. ], [ 2.5 , . 7 ]]) * 2. X = np . r_[np . dot(np . random . randn(n, dim), C), np . dot(np . random . randn(n, dim), C . T) + np . array([ 1 , 4 ])] y = np . hstack((np . zeros(n), np . ones(n))) return X, y plot functions def plot_data (lda, X, y, y_pred, fig_index): splot = plt . subplot( 2 , 2 , fig_index) if fig_index == 1 : plt . title( 'Linear Discriminant Analysis' ) plt . ylabel( 'Data with fixed covariance' ) elif fig_index == 2 : plt . title( 'Quadratic Discriminant Analysis' ) elif fig_index == 3 : plt . ylabel( 'Data with varying covariances' ) tp = (y == y_pred) # True Positive tp0, tp1 = tp[y == 0 ], tp[y == 1 ] X0, X1 = X[y == 0 ], X[y == 1 ] X0_tp, X0_fp = X0[tp0], X0[ ~ tp0] X1_tp, X1_fp = X1[tp1], X1[ ~ tp1] alpha = 0.5 # class 0: dots plt . plot(X0_tp[:, 0 ], X0_tp[:, 1 ], 'o' , alpha = alpha, color = 'red' ) plt . plot(X0_fp[:, 0 ], X0_fp[:, 1 ], '*' , alpha = alpha, color = '#990000' ) # dark red # class 1: dots plt . plot(X1_tp[:, 0 ], X1_tp[:, 1 ], 'o' , alpha = alpha, color = 'blue' ) plt . plot(X1_fp[:, 0 ], X1_fp[:, 1 ], '*' , alpha = alpha, color = '#000099' ) # dark blue # class 0 and 1 : areas nx, ny = 200 , 100 x_min, x_max = plt . xlim() y_min, y_max = plt . ylim() xx, yy = np . meshgrid(np . linspace(x_min, x_max, nx), np . linspace(y_min, y_max, ny)) Z = lda . predict_proba(np . c_[xx . ravel(), yy . ravel()]) Z = Z[:, 1 ] . reshape(xx . shape) plt . pcolormesh(xx, yy, Z, cmap = 'red_blue_classes' , norm = colors . Normalize( 0. , 1. )) plt . contour(xx, yy, Z, [ 0.5 ], linewidths = 2. , colors = 'k' ) # means plt . plot(lda . means_[ 0 ][ 0 ], lda . means_[ 0 ][ 1 ], 'o' , color = 'black' , markersize = 10 ) plt . plot(lda . means_[ 1 ][ 0 ], lda . means_[ 1 ][ 1 ], 'o' , color = 'black' , markersize = 10 ) return splot def plot_ellipse (splot, mean, cov, color): v, w = linalg . eigh(cov) u = w[ 0 ] / linalg . norm(w[ 0 ]) angle = np . arctan(u[ 1 ] / u[ 0 ]) angle = 180 * angle / np . pi # convert to degrees # filled Gaussian at 2 standard deviation ell = mpl . patches . Ellipse(mean, 2 * v[ 0 ] ** 0.5 , 2 * v[ 1 ] ** 0.5 , 180 + angle, facecolor = color, edgecolor = 'yellow' , linewidth = 2 , zorder = 2 ) ell . set_clip_box(splot . bbox) ell . set_alpha( 0.5 ) splot . add_artist(ell) splot . set_xticks(()) splot . set_yticks(()) def plot_lda_cov (lda, splot): plot_ellipse(splot, lda . means_[ 0 ], lda . covariance_, 'red' ) plot_ellipse(splot, lda . means_[ 1 ], lda . covariance_, 'blue' ) def plot_qda_cov (qda, splot): plot_ellipse(splot, qda . means_[ 0 ], qda . covariances_[ 0 ], 'red' ) plot_ellipse(splot, qda . means_[ 1 ], qda . covariances_[ 1 ], 'blue' ) for i, (X, y) in enumerate ([dataset_fixed_cov(), dataset_cov()]): # Linear Discriminant Analysis lda = LinearDiscriminantAnalysis(solver = \"svd\" , store_covariance = True ) y_pred = lda . fit(X, y) . predict(X) splot = plot_data(lda, X, y, y_pred, fig_index = 2 * i + 1 ) plot_lda_cov(lda, splot) plt . axis( 'tight' ) # Quadratic Discriminant Analysis qda = QuadraticDiscriminantAnalysis(store_covariances = True ) y_pred = qda . fit(X, y) . predict(X) splot = plot_data(qda, X, y, y_pred, fig_index = 2 * i + 2 ) plot_qda_cov(qda, splot) plt . axis( 'tight' ) plt . suptitle( 'Linear Discriminant Analysis vs Quadratic Discriminant Analysis' ) plt . show()","title":"5. Linear and Quadratic Discriminant Analysis with covariance ellipsoid (source)"},{"location":"Classifiers/SVM/svm/","text":"Support Vector Mechanics (SVM) Introduction from sklearn import svm X = [[ 0 , 0 ], [ 1 , 1 ]] y = [ 0 , 1 ] clf = svm . SVC() Fit clf . fit(X, y) SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) Predict After being fitted, the model can then be used to predict new values: clf . predict([[ 2. , 2. ]]) array([1]) SVMs decision function depends on some subset of the training data, called the support vectors. Some properties of these support vectors can be found in members support_vectors_, support_ and n_support: Support vectors clf . support_vectors_ array([[0., 0.], [1., 1.]]) # get indices of support vectors clf . support_ array([0, 1], dtype=int32) # get number of support vectors for each class clf . n_support_ array([1, 1], dtype=int32) Examples: Plot Iris SVM 1. Plot different SVM classifiers in the iris dataset ( source ) Comparison of different linear SVM classifiers on a 2D projection of the iris dataset. We only consider the first 2 features of this dataset: Sepal length Sepal width This example shows how to plot the decision surface for four SVM classifiers with different kernels. The linear models LinearSVC() and SVC(kernel='linear') yield slightly different decision boundaries. This can be a consequence of the following differences: LinearSVC minimizes the squared hinge loss while SVC minimizes the regular hinge loss. LinearSVC uses the One-vs-All (also known as One-vs-Rest) multiclass reduction while SVC uses the One-vs-One multiclass reduction. Both linear models have linear decision boundaries (intersecting hyperplanes) while the non-linear kernel models (polynomial or Gaussian RBF) have more flexible non-linear decision boundaries with shapes that depend on the kind of kernel and its parameters. .. NOTE:: while plotting the decision function of classifiers for toy 2D datasets can help get an intuitive understanding of their respective expressive power, be aware that those intuitions don't always generalize to more realistic high-dimensional problems. % matplotlib inline import numpy as np import matplotlib.pyplot as plt #=====model and data set ========== from sklearn import svm, datasets Data Set # import some data to play with iris = datasets . load_iris() iris . data[ 0 : 10 ] array([[5.1, 3.5, 1.4, 0.2], [4.9, 3. , 1.4, 0.2], [4.7, 3.2, 1.3, 0.2], [4.6, 3.1, 1.5, 0.2], [5. , 3.6, 1.4, 0.2], [5.4, 3.9, 1.7, 0.4], [4.6, 3.4, 1.4, 0.3], [5. , 3.4, 1.5, 0.2], [4.4, 2.9, 1.4, 0.2], [4.9, 3.1, 1.5, 0.1]]) iris . target[ 0 : 10 ] array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) We only take the first two features. We could avoid this ugly slicing by using a two-dim dataset X = iris . data[:, : 2 ] y = iris . target Train the model h = . 02 # step size in the mesh We create an instance of SVM and fit out data. We do not scale our data since we want to plot the support vectors C = 1.0 # SVM regularization parameter svc = svm . SVC(kernel = 'linear' , C = C) . fit(X, y) rbf_svc = svm . SVC(kernel = 'rbf' , gamma = 0.7 , C = C) . fit(X, y) poly_svc = svm . SVC(kernel = 'poly' , degree = 3 , C = C) . fit(X, y) lin_svc = svm . LinearSVC(C = C) . fit(X, y) Plot # create a mesh to plot in x_min, x_max = X[:, 0 ] . min() - 1 , X[:, 0 ] . max() + 1 y_min, y_max = X[:, 1 ] . min() - 1 , X[:, 1 ] . max() + 1 xx, yy = np . meshgrid(np . arange(x_min, x_max, h), np . arange(y_min, y_max, h)) plt . figure(figsize = [ 12 , 12 ]) # title for the plots titles = [ 'SVC with linear kernel' , 'LinearSVC (linear kernel)' , 'SVC with RBF kernel' , 'SVC with polynomial (degree 3) kernel' ] for i, clf in enumerate ((svc, lin_svc, rbf_svc, poly_svc)): # Plot the decision boundary. For that, we will assign a color to each # point in the mesh [x_min, m_max]x[y_min, y_max]. plt . subplot( 2 , 2 , i + 1 ) plt . subplots_adjust(wspace = 0.4 , hspace = 0.4 ) Z = clf . predict(np . c_[xx . ravel(), yy . ravel()]) # Put the result into a color plot Z = Z . reshape(xx . shape) plt . contourf(xx, yy, Z, cmap = plt . cm . Paired, alpha = 0.8 ) # Plot also the training points plt . scatter(X[:, 0 ], X[:, 1 ], c = y, cmap = plt . cm . Paired) plt . xlabel( 'Sepal length' ) plt . ylabel( 'Sepal width' ) plt . xlim(xx . min(), xx . max()) plt . ylim(yy . min(), yy . max()) plt . xticks(()) plt . yticks(()) plt . title(titles[i]) plt . show() 2. SVM with custom kernel ( source ) Simple usage of Support Vector Machines to classify a sample. It will plot the decision surface and the support vectors. import numpy as np import matplotlib.pyplot as plt from sklearn import svm, datasets Data Set # import some data to play with iris = datasets . load_iris() X = iris . data[:, : 2 ] # we only take the first two features. We could # avoid this ugly slicing by using a two-dim dataset Y = iris . target Define Kernel def my_kernel (X, Y): \"\"\" We create a custom kernel: (2 0) k(X, Y) = X ( ) Y.T (0 1) \"\"\" M = np . array([[ 2 , 0 ], [ 0 , 1.0 ]]) return np . dot(np . dot(X, M), Y . T) Fit the model # we create an instance of SVM and fit out data. clf = svm . SVC(kernel = my_kernel) clf . fit(X, Y) SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape='ovr', degree=3, gamma='auto', kernel=&lt;function my_kernel at 0x7f3ae7845620&gt;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) Plot h = . 02 # step size in the mesh # Plot the decision boundary. For that, we will assign a color to each # point in the mesh [x_min, m_max]x[y_min, y_max]. x_min, x_max = X[:, 0 ] . min() - 1 , X[:, 0 ] . max() + 1 y_min, y_max = X[:, 1 ] . min() - 1 , X[:, 1 ] . max() + 1 xx, yy = np . meshgrid(np . arange(x_min, x_max, h), np . arange(y_min, y_max, h)) Z = clf . predict(np . c_[xx . ravel(), yy . ravel()]) # Put the result into a color plot Z = Z . reshape(xx . shape) plt . figure(figsize = [ 8 , 8 ]) plt . pcolormesh(xx, yy, Z, cmap = plt . cm . Paired) # Plot also the training points plt . scatter(X[:, 0 ], X[:, 1 ], c = Y, cmap = plt . cm . Paired) plt . title( '3-Class classification using Support Vector Machine with custom' ' kernel' ) plt . axis( 'tight' ) plt . show() 3. One-class SVM with non-linear kernel (RBF) ( source ) An example using a one-class SVM for novelty detection. One-class SVM is an unsupervised algorithm that learns a decision function for novelty detection: classifying new data as similar or different to the training set. import numpy as np import matplotlib.pyplot as plt import matplotlib.font_manager from sklearn import svm Generate train data # Generate train data X = 0.3 * np . random . randn( 100 , 2 ) X_train = np . r_[X + 2 , X - 2 ] # Generate some regular novel observations X = 0.3 * np . random . randn( 20 , 2 ) X_test = np . r_[X + 2 , X - 2 ] # Generate some abnormal novel observations X_outliers = np . random . uniform(low =- 4 , high = 4 , size = ( 20 , 2 )) Fit the Model (sklearn.svm.OneClassSVM) xx, yy = np . meshgrid(np . linspace( - 5 , 5 , 500 ), np . linspace( - 5 , 5 , 500 )) # fit the model clf = svm . OneClassSVM(nu = 0.1 , kernel = \"rbf\" , gamma = 0.1 ) clf . fit(X_train) #predict y_pred_train = clf . predict(X_train) y_pred_test = clf . predict(X_test) y_pred_outliers = clf . predict(X_outliers) #Error n_error_train = y_pred_train[y_pred_train == - 1 ] . size n_error_test = y_pred_test[y_pred_test == - 1 ] . size n_error_outliers = y_pred_outliers[y_pred_outliers == 1 ] . size # plot the line, the points, and the nearest vectors to the plane Z = clf . decision_function(np . c_[xx . ravel(), yy . ravel()]) Z = Z . reshape(xx . shape) Plot plt . figure(figsize = [ 12 , 12 ]) plt . title( \"Novelty Detection\" ) plt . contourf(xx, yy, Z, levels = np . linspace(Z . min(), 0 , 7 ), cmap = plt . cm . PuBu) a = plt . contour(xx, yy, Z, levels = [ 0 ], linewidths = 2 , colors = 'darkred' ) plt . contourf(xx, yy, Z, levels = [ 0 , Z . max()], colors = 'palevioletred' ) s = 40 b1 = plt . scatter(X_train[:, 0 ], X_train[:, 1 ], c = 'white' , s = s) b2 = plt . scatter(X_test[:, 0 ], X_test[:, 1 ], c = 'blueviolet' , s = s) c = plt . scatter(X_outliers[:, 0 ], X_outliers[:, 1 ], c = 'gold' , s = s) plt . axis( 'tight' ) plt . xlim(( - 5 , 5 )) plt . ylim(( - 5 , 5 )) plt . legend([a . collections[ 0 ], b1, b2, c], [ \"learned frontier\" , \"training observations\" , \"new regular observations\" , \"new abnormal observations\" ], loc = \"upper left\" , prop = matplotlib . font_manager . FontProperties(size = 11 )) plt . xlabel( \"error train: %d /200 ; errors novel regular: %d /40 ; \" \"errors novel abnormal: %d /40\" % (n_error_train, n_error_test, n_error_outliers)) plt . show() 4. RBF SVM parameters ( source ) This example illustrates the effect of the parameters gamma and C of the Radial Basis Function (RBF) kernel SVM. Intuitively, the gamma parameter defines how far the influence of a single training example reaches, with low values meaning 'far' and high values meaning 'close'. The gamma parameters can be seen as the inverse of the radius of influence of samples selected by the model as support vectors. The C parameter trades off misclassification of training examples against simplicity of the decision surface. A low C makes the decision surface smooth, while a high C aims at classifying all training examples correctly by giving the model freedom to select more samples as support vectors. The first plot is a visualization of the decision function for a variety of parameter values on a simplified classification problem involving only 2 input features and 2 possible target classes (binary classification). Note that this kind of plot is not possible to do for problems with more features or target classes. The second plot is a heatmap of the classifier's cross-validation accuracy as a function of C and gamma . For this example we explore a relatively large grid for illustration purposes. In practice, a logarithmic grid from :math: 10^{-3} to :math: 10^3 is usually sufficient. If the best parameters lie on the boundaries of the grid, it can be extended in that direction in a subsequent search. Note that the heat map plot has a special colorbar with a midpoint value close to the score values of the best performing models so as to make it easy to tell them appart in the blink of an eye. The behavior of the model is very sensitive to the gamma parameter. If gamma is too large, the radius of the area of influence of the support vectors only includes the support vector itself and no amount of regularization with C will be able to prevent overfitting. When gamma is very small, the model is too constrained and cannot capture the complexity or \"shape\" of the data. The region of influence of any selected support vector would include the whole training set. The resulting model will behave similarly to a linear model with a set of hyperplanes that separate the centers of high density of any pair of two classes. For intermediate values, we can see on the second plot that good models can be found on a diagonal of C and gamma . Smooth models (lower gamma values) can be made more complex by selecting a larger number of support vectors (larger C values) hence the diagonal of good performing models. Finally one can also observe that for some intermediate values of gamma we get equally performing models when C becomes very large: it is not necessary to regularize by limiting the number of support vectors. The radius of the RBF kernel alone acts as a good structural regularizer. In practice though it might still be interesting to limit the number of support vectors with a lower value of C so as to favor models that use less memory and that are faster to predict. We should also note that small differences in scores results from the random splits of the cross-validation procedure. Those spurious variations can be smoothed out by increasing the number of CV iterations n_iter at the expense of compute time. Increasing the value number of C_range and gamma_range steps will increase the resolution of the hyper-parameter heat map. import numpy as np import matplotlib.pyplot as plt from matplotlib.colors import Normalize #======== preprocessing ========== from sklearn.preprocessing import StandardScaler #===models===================== from sklearn.svm import SVC from sklearn.cross_validation import StratifiedShuffleSplit from sklearn.model_selection import GridSearchCV #======== data ========================== from sklearn.datasets import load_iris /home/bitnami/anaconda3/envs/caseolap/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20. \"This module will be removed in 0.20.\", DeprecationWarning) Utility function to move the midpoint of a colormap to be around the values of interest. class MidpointNormalize (Normalize): def __init__ ( self , vmin = None , vmax = None , midpoint = None , clip = False ): self . midpoint = midpoint Normalize . __init__ ( self , vmin, vmax, clip) def __call__ ( self , value, clip = None ): x, y = [ self . vmin, self . midpoint, self . vmax], [ 0 , 0.5 , 1 ] return np . ma . masked_array(np . interp(value, x, y)) Load and prepare data set dataset for grid search iris = load_iris() X = iris . data y = iris . target Dataset for decision function visualization: we only keep the first two features in X and sub-sample the dataset to keep only 2 classes and make it a binary classification problem. X_2d = X[:, : 2 ] X_2d = X_2d[y > 0 ] y_2d = y[y > 0 ] y_2d -= 1 It is usually a good idea to scale the data for SVM training. We are cheating a bit in this example in scaling all of the data, instead of fitting the transformation on the training set and just applying it on the test set. scaler = StandardScaler() X = scaler . fit_transform(X) X_2d = scaler . fit_transform(X_2d) Train classifiers, For an initial search, a logarithmic grid with basis 10 is often helpful. Using a basis of 2, a finer tuning can be achieved but at a much higher cost. C_range = np . logspace( - 2 , 10 , 13 ) gamma_range = np . logspace( - 9 , 3 , 13 ) param_grid = dict (gamma = gamma_range, C = C_range) cv = StratifiedShuffleSplit(n_splits = 5 , test_size = 0.2 , random_state = 42 ) grid = GridSearchCV(SVC(), param_grid = param_grid, cv = cv) grid . fit(X, y) print ( \"The best parameters are %s with a score of %0.2f \" % (grid . best_params_, grid . best_score_)) --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-40-3e73776bfe50&gt; in &lt;module&gt;() 2 gamma_range = np.logspace(-9, 3, 13) 3 param_grid = dict(gamma=gamma_range, C=C_range) ----&gt; 4 cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42) 5 grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv) 6 grid.fit(X, y) TypeError: __init__() got an unexpected keyword argument 'n_splits' Now we need to fit a classifier for all parameters in the 2d version (we use a smaller set of parameters here because it takes a while to train) C_2d_range = [ 1e-2 , 1 , 1e2 ] gamma_2d_range = [ 1e-1 , 1 , 1e1 ] classifiers = [] for C in C_2d_range: for gamma in gamma_2d_range: clf = SVC(C = C, gamma = gamma) clf . fit(X_2d, y_2d) classifiers . append((C, gamma, clf)) visualization draw visualization of parameter effects plt . figure(figsize = ( 8 , 6 )) xx, yy = np . meshgrid(np . linspace( - 3 , 3 , 200 ), np . linspace( - 3 , 3 , 200 )) for (k, (C, gamma, clf)) in enumerate (classifiers): # evaluate decision function in a grid Z = clf . decision_function(np . c_[xx . ravel(), yy . ravel()]) Z = Z . reshape(xx . shape) # visualize decision function for these parameters plt . subplot( len (C_2d_range), len (gamma_2d_range), k + 1 ) plt . title( \"gamma=10^ %d , C=10^ %d \" % (np . log10(gamma), np . log10(C)), size = 'medium' ) # visualize parameter's effect on decision function plt . pcolormesh(xx, yy, - Z, cmap = plt . cm . RdBu) plt . scatter(X_2d[:, 0 ], X_2d[:, 1 ], c = y_2d, cmap = plt . cm . RdBu_r) plt . xticks(()) plt . yticks(()) plt . axis( 'tight' ) # plot the scores of the grid # grid_scores_ contains parameter settings and scores # We extract just the scores scores = [x[ 1 ] for x in grid . grid_scores_] scores = np . array(scores) . reshape( len (C_range), len (gamma_range)) # Draw heatmap of the validation accuracy as a function of gamma and C # # The score are encoded as colors with the hot colormap which varies from dark # red to bright yellow. As the most interesting scores are all located in the # 0.92 to 0.97 range we use a custom normalizer to set the mid-point to 0.92 so # as to make it easier to visualize the small variations of score values in the # interesting range while not brutally collapsing all the low score values to # the same color. plt . figure(figsize = ( 8 , 6 )) plt . subplots_adjust(left =. 2 , right = 0.95 , bottom = 0.15 , top = 0.95 ) plt . imshow(scores, interpolation = 'nearest' , cmap = plt . cm . hot, norm = MidpointNormalize(vmin = 0.2 , midpoint = 0.92 )) plt . xlabel( 'gamma' ) plt . ylabel( 'C' ) plt . colorbar() plt . xticks(np . arange( len (gamma_range)), gamma_range, rotation = 45 ) plt . yticks(np . arange( len (C_range)), C_range) plt . title( 'Validation accuracy' ) plt . show() --------------------------------------------------------------------------- NameError Traceback (most recent call last) &lt;ipython-input-45-d7c9f3ffab83&gt; in &lt;module&gt;() 21 # grid_scores_ contains parameter settings and scores 22 # We extract just the scores ---&gt; 23 scores = [x[1] for x in grid.grid_scores_] 24 scores = np.array(scores).reshape(len(C_range), len(gamma_range)) 25 NameError: name 'grid' is not defined 5. SVM: Separating hyperplane for unbalanced classes ( source ) Find the optimal separating hyperplane using an SVC for classes that are unbalanced. We first find the separating plane with a plain SVC and then plot (dashed) the separating hyperplane with automatically correction for unbalanced classes. This example will also work by replacing SVC(kernel=\"linear\") with SGDClassifier(loss=\"hinge\") . Setting the loss parameter of the :class: SGDClassifier equal to hinge will yield behaviour such as that of a SVC with a linear kernel. For example try instead of the SVC :: clf = SGDClassifier(n_iter=100, alpha=0.01) import numpy as np import matplotlib.pyplot as plt from sklearn import svm from sklearn.linear_model import SGDClassifier Data # we create 40 separable points rng = np . random . RandomState( 0 ) n_samples_1 = 1000 n_samples_2 = 100 X = np . r_[ 1.5 * rng . randn(n_samples_1, 2 ), 0.5 * rng . randn(n_samples_2, 2 ) + [ 2 , 2 ]] y = [ 0 ] * (n_samples_1) + [ 1 ] * (n_samples_2) Model # fit the model and get the separating hyperplane clf = svm . SVC(kernel = 'linear' , C = 1.0 ) clf . fit(X, y) w = clf . coef_[ 0 ] a = - w[ 0 ] / w[ 1 ] xx = np . linspace( - 5 , 5 ) yy = a * xx - clf . intercept_[ 0 ] / w[ 1 ] Get the separating hyperplane using weighted classes # get the separating hyperplane using weighted classes wclf = svm . SVC(kernel = 'linear' , class_weight = { 1 : 10 }) wclf . fit(X, y) ww = wclf . coef_[ 0 ] wa = - ww[ 0 ] / ww[ 1 ] wyy = wa * xx - wclf . intercept_[ 0 ] / ww[ 1 ] Plot # plot separating hyperplanes and samples h0 = plt . plot(xx, yy, 'k-' , label = 'no weights' ) h1 = plt . plot(xx, wyy, 'k--' , label = 'with weights' ) plt . scatter(X[:, 0 ], X[:, 1 ], c = y, cmap = plt . cm . Paired) plt . legend() plt . axis( 'tight' ) plt . show() 6. SVM: Maximum margin separating hyperplane ( source ) Plot the maximum margin separating hyperplane within a two-class separable dataset using a Support Vector Machine classifier with linear kernel. import numpy as np import matplotlib.pyplot as plt from sklearn import svm data # we create 40 separable points np . random . seed( 0 ) X = np . r_[np . random . randn( 20 , 2 ) - [ 2 , 2 ], np . random . randn( 20 , 2 ) + [ 2 , 2 ]] Y = [ 0 ] * 20 + [ 1 ] * 20 Model # fit the model clf = svm . SVC(kernel = 'linear' ) clf . fit(X, Y) # get the separating hyperplane w = clf . coef_[ 0 ] a = - w[ 0 ] / w[ 1 ] xx = np . linspace( - 5 , 5 ) yy = a * xx - (clf . intercept_[ 0 ]) / w[ 1 ] Separating hyperplane # plot the parallels to the separating hyperplane that pass through the # support vectors b = clf . support_vectors_[ 0 ] yy_down = a * xx + (b[ 1 ] - a * b[ 0 ]) b = clf . support_vectors_[ - 1 ] yy_up = a * xx + (b[ 1 ] - a * b[ 0 ]) Plot # plot the line, the points, and the nearest vectors to the plane plt . plot(xx, yy, 'k-' ) plt . plot(xx, yy_down, 'k--' ) plt . plot(xx, yy_up, 'k--' ) plt . scatter(clf . support_vectors_[:, 0 ], clf . support_vectors_[:, 1 ], s = 80 , facecolors = 'none' ) plt . scatter(X[:, 0 ], X[:, 1 ], c = Y, cmap = plt . cm . Paired) plt . axis( 'tight' ) plt . show() 7. SVM-Anova: SVM with univariate feature selection ( source ) This example shows how to perform univariate feature selection before running a SVC (support vector classifier) to improve the classification scores. import numpy as np import matplotlib.pyplot as plt from sklearn import svm, datasets, feature_selection from sklearn.cross_validation import cross_val_score from sklearn.pipeline import Pipeline Data # Import some data to play with digits = datasets . load_digits() y = digits . target # Throw away data, to be in the curse of dimension settings y = y[: 200 ] X = digits . data[: 200 ] n_samples = len (y) X = X . reshape((n_samples, - 1 )) # add 200 non-informative features X = np . hstack((X, 2 * np . random . random((n_samples, 200 )))) Feature-selection transform # Create a feature-selection transform and transform = feature_selection . SelectPercentile(feature_selection . f_classif) instance of SVM # Instance of SVM that we combine together to have an full-blown estimator clf = Pipeline([( 'anova' , transform), ( 'svc' , svm . SVC(C = 1.0 ))]) cross-validation score as a function of percentile of features #the cross-validation score as a function of percentile of features score_means = list () score_stds = list () percentiles = ( 1 , 3 , 6 , 10 , 15 , 20 , 30 , 40 , 60 , 80 , 100 ) for percentile in percentiles: clf . set_params(anova__percentile = percentile) # Compute cross-validation score using 1 CPU this_scores = cross_val_score(clf, X, y, n_jobs = 1 ) score_means . append(this_scores . mean()) score_stds . append(this_scores . std()) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 7 8 15 16 23 24 31 32 39 40 47 48 56 63] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 8 15 16 23 31 32 39 40 48 56] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 8 15 16 23 31 32 39 40 48 56] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 7 8 15 16 23 24 31 32 39 40 47 48 56 63] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 8 15 16 23 31 32 39 40 48 56] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 8 15 16 23 31 32 39 40 48 56] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 7 8 15 16 23 24 31 32 39 40 47 48 56 63] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 8 15 16 23 31 32 39 40 48 56] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 8 15 16 23 31 32 39 40 48 56] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 7 8 15 16 23 24 31 32 39 40 47 48 56 63] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 8 15 16 23 31 32 39 40 48 56] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 8 15 16 23 31 32 39 40 48 56] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 7 8 15 16 23 24 31 32 39 40 47 48 56 63] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 8 15 16 23 31 32 39 40 48 56] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 8 15 16 23 31 32 39 40 48 56] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 7 8 15 16 23 24 31 32 39 40 47 48 56 63] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 8 15 16 23 31 32 39 40 48 56] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 8 15 16 23 31 32 39 40 48 56] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 7 8 15 16 23 24 31 32 39 40 47 48 56 63] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 8 15 16 23 31 32 39 40 48 56] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 8 15 16 23 31 32 39 40 48 56] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 7 8 15 16 23 24 31 32 39 40 47 48 56 63] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 8 15 16 23 31 32 39 40 48 56] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 8 15 16 23 31 32 39 40 48 56] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 7 8 15 16 23 24 31 32 39 40 47 48 56 63] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 8 15 16 23 31 32 39 40 48 56] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 8 15 16 23 31 32 39 40 48 56] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 7 8 15 16 23 24 31 32 39 40 47 48 56 63] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 8 15 16 23 31 32 39 40 48 56] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 8 15 16 23 31 32 39 40 48 56] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 7 8 15 16 23 24 31 32 39 40 47 48 56 63] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 8 15 16 23 31 32 39 40 48 56] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 8 15 16 23 31 32 39 40 48 56] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw Plot plt . errorbar(percentiles, score_means, np . array(score_stds)) plt . title( 'Performance of the SVM-Anova varying the percentile of features selected' ) plt . xlabel( 'Percentile' ) plt . ylabel( 'Prediction rate' ) plt . axis( 'tight' ) plt . show() 8. SVM-Kernels ( source ) Three different types of SVM-Kernels are displayed below. The polynomial and RBF are especially useful when the data-points are not linearly separable. import numpy as np import matplotlib.pyplot as plt from sklearn import svm Data # Our dataset and targets X = np . c_[( . 4 , -. 7 ), ( - 1.5 , - 1 ), ( - 1.4 , -. 9 ), ( - 1.3 , - 1.2 ), ( - 1.1 , -. 2 ), ( - 1.2 , -. 4 ), ( -. 5 , 1.2 ), ( - 1.5 , 2.1 ), ( 1 , 1 ), # -- ( 1.3 , . 8 ), ( 1.2 , . 5 ), ( . 2 , - 2 ), ( . 5 , - 2.4 ), ( . 2 , - 2.3 ), ( 0 , - 2.7 ), ( 1.3 , 2.1 )] . T Y = [ 0 ] * 8 + [ 1 ] * 8 plot # figure number fignum = 1 # fit the model for kernel in ( 'linear' , 'poly' , 'rbf' ): clf = svm . SVC(kernel = kernel, gamma = 2 ) clf . fit(X, Y) # plot the line, the points, and the nearest vectors to the plane plt . figure(fignum, figsize = ( 4 , 3 )) plt . clf() plt . scatter(clf . support_vectors_[:, 0 ], clf . support_vectors_[:, 1 ], s = 80 , facecolors = 'none' , zorder = 10 ) plt . scatter(X[:, 0 ], X[:, 1 ], c = Y, zorder = 10 , cmap = plt . cm . Paired) plt . axis( 'tight' ) x_min = - 3 x_max = 3 y_min = - 3 y_max = 3 XX, YY = np . mgrid[x_min:x_max: 200j , y_min:y_max: 200j ] Z = clf . decision_function(np . c_[XX . ravel(), YY . ravel()]) # Put the result into a color plot Z = Z . reshape(XX . shape) plt . figure(fignum, figsize = ( 4 , 3 )) plt . pcolormesh(XX, YY, Z > 0 , cmap = plt . cm . Paired) plt . contour(XX, YY, Z, colors = [ 'k' , 'k' , 'k' ], linestyles = [ '--' , '-' , '--' ], levels = [ -. 5 , 0 , . 5 ]) plt . xlim(x_min, x_max) plt . ylim(y_min, y_max) plt . xticks(()) plt . yticks(()) fignum = fignum + 1 plt . show() 9. SVM Margins Example ( source ) The plots below illustrate the effect the parameter C has on the separation line. A large value of C basically tells our model that we do not have that much faith in our data's distribution, and will only consider points close to line of separation. A small value of C includes more/all the observations, allowing the margins to be calculated using all the data in the area. import numpy as np import matplotlib.pyplot as plt from sklearn import svm Data # we create 40 separable points np . random . seed( 0 ) X = np . r_[np . random . randn( 20 , 2 ) - [ 2 , 2 ], np . random . randn( 20 , 2 ) + [ 2 , 2 ]] Y = [ 0 ] * 20 + [ 1 ] * 20 Plot # figure number fignum = 1 # fit the model for name, penalty in (( 'unreg' , 1 ), ( 'reg' , 0.05 )): clf = svm . SVC(kernel = 'linear' , C = penalty) clf . fit(X, Y) # get the separating hyperplane w = clf . coef_[ 0 ] a = - w[ 0 ] / w[ 1 ] xx = np . linspace( - 5 , 5 ) yy = a * xx - (clf . intercept_[ 0 ]) / w[ 1 ] # plot the parallels to the separating hyperplane that pass through the # support vectors margin = 1 / np . sqrt(np . sum(clf . coef_ ** 2 )) yy_down = yy + a * margin yy_up = yy - a * margin # plot the line, the points, and the nearest vectors to the plane plt . figure(fignum, figsize = ( 4 , 3 )) plt . clf() plt . plot(xx, yy, 'k-' ) plt . plot(xx, yy_down, 'k--' ) plt . plot(xx, yy_up, 'k--' ) plt . scatter(clf . support_vectors_[:, 0 ], clf . support_vectors_[:, 1 ], s = 80 , facecolors = 'none' , zorder = 10 ) plt . scatter(X[:, 0 ], X[:, 1 ], c = Y, zorder = 10 , cmap = plt . cm . Paired) plt . axis( 'tight' ) x_min = - 4.8 x_max = 4.2 y_min = - 6 y_max = 6 XX, YY = np . mgrid[x_min:x_max: 200j , y_min:y_max: 200j ] Z = clf . predict(np . c_[XX . ravel(), YY . ravel()]) # Put the result into a color plot Z = Z . reshape(XX . shape) plt . figure(fignum, figsize = ( 4 , 3 )) plt . pcolormesh(XX, YY, Z, cmap = plt . cm . Paired) plt . xlim(x_min, x_max) plt . ylim(y_min, y_max) plt . xticks(()) plt . yticks(()) fignum = fignum + 1 plt . show() 10. Non-linear SVM ( source ) Perform binary classification using non-linear SVC with RBF kernel. The target to predict is a XOR of the inputs. The color map illustrates the decision function learned by the SVC. import numpy as np import matplotlib.pyplot as plt from sklearn import svm Data xx, yy = np . meshgrid(np . linspace( - 3 , 3 , 500 ), np . linspace( - 3 , 3 , 500 )) np . random . seed( 0 ) X = np . random . randn( 300 , 2 ) Y = np . logical_xor(X[:, 0 ] > 0 , X[:, 1 ] > 0 ) Model # fit the model clf = svm . NuSVC() clf . fit(X, Y) # plot the decision function for each datapoint on the grid Z = clf . decision_function(np . c_[xx . ravel(), yy . ravel()]) Z = Z . reshape(xx . shape) Plot plt . imshow(Z, interpolation = 'nearest' , extent = (xx . min(), xx . max(), yy . min(), yy . max()), aspect = 'auto' , origin = 'lower' , cmap = plt . cm . PuOr_r) contours = plt . contour(xx, yy, Z, levels = [ 0 ], linewidths = 2 , linetypes = '--' ) plt . scatter(X[:, 0 ], X[:, 1 ], s = 30 , c = Y, cmap = plt . cm . Paired) plt . xticks(()) plt . yticks(()) plt . axis([ - 3 , 3 , - 3 , 3 ]) plt . show() 11. Support Vector Regression (SVR) using linear and non-linear kernels ( source ) Toy example of 1D regression using linear, polynomial and RBF kernels. import numpy as np from sklearn.svm import SVR import matplotlib.pyplot as plt Data # Generate sample data X = np . sort( 5 * np . random . rand( 40 , 1 ), axis = 0 ) y = np . sin(X) . ravel() # Add noise to targets y[:: 5 ] += 3 * ( 0.5 - np . random . rand( 8 )) Model # Fit regression model svr_rbf = SVR(kernel = 'rbf' , C = 1e3 , gamma = 0.1 ) svr_lin = SVR(kernel = 'linear' , C = 1e3 ) svr_poly = SVR(kernel = 'poly' , C = 1e3 , degree = 2 ) y_rbf = svr_rbf . fit(X, y) . predict(X) y_lin = svr_lin . fit(X, y) . predict(X) y_poly = svr_poly . fit(X, y) . predict(X) Plot # look at the results lw = 2 plt . scatter(X, y, color = 'darkorange' , label = 'data' ) plt . hold( 'on' ) plt . plot(X, y_rbf, color = 'navy' , lw = lw, label = 'RBF model' ) plt . plot(X, y_lin, color = 'c' , lw = lw, label = 'Linear model' ) plt . plot(X, y_poly, color = 'cornflowerblue' , lw = lw, label = 'Polynomial model' ) plt . xlabel( 'data' ) plt . ylabel( 'target' ) plt . title( 'Support Vector Regression' ) plt . legend() plt . show() 13. Scaling the regularization parameter for SVCs ( source ) The following example illustrates the effect of scaling the regularization parameter when using :ref: svm for classification . For SVC classification, we are interested in a risk minimization for the equation: $ C \\sum_{i=1, n} \\mathcal{L} (f(x_i), y_i) + \\Omega (w)$ where - `C` is used to set the amount of regularization - `\\mathcal{L}` is a `loss` function of our samples and our model parameters. - `\\Omega` is a `penalty` function of our model parameters If we consider the loss function to be the individual error per sample, then the data-fit term, or the sum of the error for each sample, will increase as we add more samples. The penalization term, however, will not increase. When using, for example, :ref: cross validation <cross_validation> , to set the amount of regularization with C , there will be a different amount of samples between the main problem and the smaller problems within the folds of the cross validation. Since our loss function is dependent on the amount of samples, the latter will influence the selected value of C . The question that arises is How do we optimally adjust C to account for the different amount of training samples? The figures below are used to illustrate the effect of scaling our C to compensate for the change in the number of samples, in the case of using an l1 penalty, as well as the l2 penalty. l1-penalty case In the l1 case, theory says that prediction consistency (i.e. that under given hypothesis, the estimator learned predicts as well as a model knowing the true distribution) is not possible because of the bias of the l1 . It does say, however, that model consistency, in terms of finding the right set of non-zero parameters as well as their signs, can be achieved by scaling C1 . l2-penalty case The theory says that in order to achieve prediction consistency, the penalty parameter should be kept constant as the number of samples grow. Simulations The two figures below plot the values of C on the x-axis and the corresponding cross-validation scores on the y-axis , for several different fractions of a generated data-set. In the l1 penalty case, the cross-validation-error correlates best with the test-error, when scaling our C with the number of samples, n , which can be seen in the first figure. For the l2 penalty case, the best result comes from the case where C is not scaled. Two separate datasets are used for the two different plots. The reason behind this is the `l1` case works better on sparse data, while `l2` is better suited to the non-sparse case. import numpy as np import matplotlib.pyplot as plt from sklearn.svm import LinearSVC from sklearn.model_selection import ShuffleSplit from sklearn.model_selection import GridSearchCV from sklearn.utils import check_random_state from sklearn import datasets Data,Model,Plot rnd = check_random_state( 1 ) # set up dataset n_samples = 100 n_features = 300 # l1 data (only 5 informative features) X_1, y_1 = datasets . make_classification(n_samples = n_samples, n_features = n_features, n_informative = 5 , random_state = 1 ) # l2 data: non sparse, but less features y_2 = np . sign( . 5 - rnd . rand(n_samples)) X_2 = rnd . randn(n_samples, n_features / 5 ) + y_2[:, np . newaxis] X_2 += 5 * rnd . randn(n_samples, n_features / 5 ) clf_sets = [(LinearSVC(penalty = 'l1' , loss = 'squared_hinge' , dual = False , tol = 1e-3 ), np . logspace( - 2.3 , - 1.3 , 10 ), X_1, y_1), (LinearSVC(penalty = 'l2' , loss = 'squared_hinge' , dual = True , tol = 1e-4 ), np . logspace( - 4.5 , - 2 , 10 ), X_2, y_2)] colors = [ 'navy' , 'cyan' , 'darkorange' ] lw = 2 for fignum, (clf, cs, X, y) in enumerate (clf_sets): # set up the plot for each regressor plt . figure(fignum, figsize = ( 9 , 10 )) for k, train_size in enumerate (np . linspace( 0.3 , 0.7 , 3 )[:: - 1 ]): param_grid = dict (C = cs) # To get nice curve, we need a large number of iterations to # reduce the variance grid = GridSearchCV(clf, refit = False , param_grid = param_grid, cv = ShuffleSplit(train_size = train_size, n_iter = 250 , random_state = 1 )) grid . fit(X, y) scores = [x[ 1 ] for x in grid . grid_scores_] scales = [( 1 , 'No scaling' ), ((n_samples * train_size), '1/n_samples' ), ] for subplotnum, (scaler, name) in enumerate (scales): plt . subplot( 2 , 1 , subplotnum + 1 ) plt . xlabel( 'C' ) plt . ylabel( 'CV Score' ) grid_cs = cs * float (scaler) # scale the C's plt . semilogx(grid_cs, scores, label = \"fraction %.2f \" % train_size, color = colors[k], lw = lw) plt . title( 'scaling= %s , penalty= %s , loss= %s ' % (name, clf . penalty, clf . loss)) plt . legend(loc = \"best\" ) plt . show() /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:14: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:15: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-83-3265a6814293&gt; in &lt;module&gt;() 35 grid = GridSearchCV(clf, refit=False, param_grid=param_grid, 36 cv=ShuffleSplit(train_size=train_size, n_iter=250, ---&gt; 37 random_state=1)) 38 grid.fit(X, y) 39 scores = [x[1] for x in grid.grid_scores_] TypeError: __init__() got an unexpected keyword argument 'n_iter' 14. SVM: Weighted samples ( source ) Plot decision function of a weighted dataset, where the size of points is proportional to its weight. The sample weighting rescales the C parameter, which means that the classifier puts more emphasis on getting these points right. The effect might often be subtle. To emphasize the effect here, we particularly weight outliers, making the deformation of the decision boundary very visible. import numpy as np import matplotlib.pyplot as plt from sklearn import svm def plot_decision_function (classifier, sample_weight, axis, title): # plot the decision function xx, yy = np . meshgrid(np . linspace( - 4 , 5 , 500 ), np . linspace( - 4 , 5 , 500 )) Z = classifier . decision_function(np . c_[xx . ravel(), yy . ravel()]) Z = Z . reshape(xx . shape) # plot the line, the points, and the nearest vectors to the plane axis . contourf(xx, yy, Z, alpha = 0.75 , cmap = plt . cm . bone) axis . scatter(X[:, 0 ], X[:, 1 ], c = y, s = 100 * sample_weight, alpha = 0.9 , cmap = plt . cm . bone) axis . axis( 'off' ) axis . set_title(title) Data # we create 20 points np . random . seed( 0 ) X = np . r_[np . random . randn( 10 , 2 ) + [ 1 , 1 ], np . random . randn( 10 , 2 )] y = [ 1 ] * 10 + [ - 1 ] * 10 sample_weight_last_ten = abs (np . random . randn( len (X))) sample_weight_constant = np . ones( len (X)) # and bigger weights to some outliers sample_weight_last_ten[ 15 :] *= 5 sample_weight_last_ten[ 9 ] *= 15 Model # for reference, first fit without class weights # fit the model clf_weights = svm . SVC() clf_weights . fit(X, y, sample_weight = sample_weight_last_ten) clf_no_weights = svm . SVC() clf_no_weights . fit(X, y) SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma='auto', kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) Plot fig, axes = plt . subplots( 1 , 2 , figsize = ( 14 , 6 )) plot_decision_function(clf_no_weights, sample_weight_constant, axes[ 0 ], \"Constant weights\" ) plot_decision_function(clf_weights, sample_weight_last_ten, axes[ 1 ], \"Modified weights\" ) plt . show() &lt;matplotlib.figure.Figure at 0x114969e80&gt;","title":"SVM"},{"location":"Classifiers/SVM/svm/#support-vector-mechanics-svm","text":"","title":"Support Vector Mechanics (SVM)"},{"location":"Classifiers/SVM/svm/#introduction","text":"from sklearn import svm X = [[ 0 , 0 ], [ 1 , 1 ]] y = [ 0 , 1 ] clf = svm . SVC()","title":"Introduction"},{"location":"Classifiers/SVM/svm/#fit","text":"clf . fit(X, y) SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False)","title":"Fit"},{"location":"Classifiers/SVM/svm/#predict","text":"After being fitted, the model can then be used to predict new values: clf . predict([[ 2. , 2. ]]) array([1]) SVMs decision function depends on some subset of the training data, called the support vectors. Some properties of these support vectors can be found in members support_vectors_, support_ and n_support:","title":"Predict"},{"location":"Classifiers/SVM/svm/#support-vectors","text":"clf . support_vectors_ array([[0., 0.], [1., 1.]]) # get indices of support vectors clf . support_ array([0, 1], dtype=int32) # get number of support vectors for each class clf . n_support_ array([1, 1], dtype=int32)","title":"Support vectors"},{"location":"Classifiers/SVM/svm/#examples","text":"","title":"Examples:"},{"location":"Classifiers/SVM/svm/#plot-iris-svm","text":"","title":"Plot Iris SVM"},{"location":"Classifiers/SVM/svm/#1-plot-different-svm-classifiers-in-the-iris-dataset-source","text":"Comparison of different linear SVM classifiers on a 2D projection of the iris dataset. We only consider the first 2 features of this dataset: Sepal length Sepal width This example shows how to plot the decision surface for four SVM classifiers with different kernels. The linear models LinearSVC() and SVC(kernel='linear') yield slightly different decision boundaries. This can be a consequence of the following differences: LinearSVC minimizes the squared hinge loss while SVC minimizes the regular hinge loss. LinearSVC uses the One-vs-All (also known as One-vs-Rest) multiclass reduction while SVC uses the One-vs-One multiclass reduction. Both linear models have linear decision boundaries (intersecting hyperplanes) while the non-linear kernel models (polynomial or Gaussian RBF) have more flexible non-linear decision boundaries with shapes that depend on the kind of kernel and its parameters. .. NOTE:: while plotting the decision function of classifiers for toy 2D datasets can help get an intuitive understanding of their respective expressive power, be aware that those intuitions don't always generalize to more realistic high-dimensional problems. % matplotlib inline import numpy as np import matplotlib.pyplot as plt #=====model and data set ========== from sklearn import svm, datasets","title":"1. Plot different SVM classifiers in the iris dataset (source)"},{"location":"Classifiers/SVM/svm/#data-set","text":"# import some data to play with iris = datasets . load_iris() iris . data[ 0 : 10 ] array([[5.1, 3.5, 1.4, 0.2], [4.9, 3. , 1.4, 0.2], [4.7, 3.2, 1.3, 0.2], [4.6, 3.1, 1.5, 0.2], [5. , 3.6, 1.4, 0.2], [5.4, 3.9, 1.7, 0.4], [4.6, 3.4, 1.4, 0.3], [5. , 3.4, 1.5, 0.2], [4.4, 2.9, 1.4, 0.2], [4.9, 3.1, 1.5, 0.1]]) iris . target[ 0 : 10 ] array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) We only take the first two features. We could avoid this ugly slicing by using a two-dim dataset X = iris . data[:, : 2 ] y = iris . target","title":"Data Set"},{"location":"Classifiers/SVM/svm/#train-the-model","text":"h = . 02 # step size in the mesh We create an instance of SVM and fit out data. We do not scale our data since we want to plot the support vectors C = 1.0 # SVM regularization parameter svc = svm . SVC(kernel = 'linear' , C = C) . fit(X, y) rbf_svc = svm . SVC(kernel = 'rbf' , gamma = 0.7 , C = C) . fit(X, y) poly_svc = svm . SVC(kernel = 'poly' , degree = 3 , C = C) . fit(X, y) lin_svc = svm . LinearSVC(C = C) . fit(X, y)","title":"Train the model"},{"location":"Classifiers/SVM/svm/#plot","text":"# create a mesh to plot in x_min, x_max = X[:, 0 ] . min() - 1 , X[:, 0 ] . max() + 1 y_min, y_max = X[:, 1 ] . min() - 1 , X[:, 1 ] . max() + 1 xx, yy = np . meshgrid(np . arange(x_min, x_max, h), np . arange(y_min, y_max, h)) plt . figure(figsize = [ 12 , 12 ]) # title for the plots titles = [ 'SVC with linear kernel' , 'LinearSVC (linear kernel)' , 'SVC with RBF kernel' , 'SVC with polynomial (degree 3) kernel' ] for i, clf in enumerate ((svc, lin_svc, rbf_svc, poly_svc)): # Plot the decision boundary. For that, we will assign a color to each # point in the mesh [x_min, m_max]x[y_min, y_max]. plt . subplot( 2 , 2 , i + 1 ) plt . subplots_adjust(wspace = 0.4 , hspace = 0.4 ) Z = clf . predict(np . c_[xx . ravel(), yy . ravel()]) # Put the result into a color plot Z = Z . reshape(xx . shape) plt . contourf(xx, yy, Z, cmap = plt . cm . Paired, alpha = 0.8 ) # Plot also the training points plt . scatter(X[:, 0 ], X[:, 1 ], c = y, cmap = plt . cm . Paired) plt . xlabel( 'Sepal length' ) plt . ylabel( 'Sepal width' ) plt . xlim(xx . min(), xx . max()) plt . ylim(yy . min(), yy . max()) plt . xticks(()) plt . yticks(()) plt . title(titles[i]) plt . show()","title":"Plot"},{"location":"Classifiers/SVM/svm/#2-svm-with-custom-kernel-source","text":"Simple usage of Support Vector Machines to classify a sample. It will plot the decision surface and the support vectors. import numpy as np import matplotlib.pyplot as plt from sklearn import svm, datasets","title":"2. SVM with custom kernel (source)"},{"location":"Classifiers/SVM/svm/#data-set_1","text":"# import some data to play with iris = datasets . load_iris() X = iris . data[:, : 2 ] # we only take the first two features. We could # avoid this ugly slicing by using a two-dim dataset Y = iris . target","title":"Data Set"},{"location":"Classifiers/SVM/svm/#define-kernel","text":"def my_kernel (X, Y): \"\"\" We create a custom kernel: (2 0) k(X, Y) = X ( ) Y.T (0 1) \"\"\" M = np . array([[ 2 , 0 ], [ 0 , 1.0 ]]) return np . dot(np . dot(X, M), Y . T)","title":"Define Kernel"},{"location":"Classifiers/SVM/svm/#fit-the-model","text":"# we create an instance of SVM and fit out data. clf = svm . SVC(kernel = my_kernel) clf . fit(X, Y) SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape='ovr', degree=3, gamma='auto', kernel=&lt;function my_kernel at 0x7f3ae7845620&gt;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False)","title":"Fit the model"},{"location":"Classifiers/SVM/svm/#plot_1","text":"h = . 02 # step size in the mesh # Plot the decision boundary. For that, we will assign a color to each # point in the mesh [x_min, m_max]x[y_min, y_max]. x_min, x_max = X[:, 0 ] . min() - 1 , X[:, 0 ] . max() + 1 y_min, y_max = X[:, 1 ] . min() - 1 , X[:, 1 ] . max() + 1 xx, yy = np . meshgrid(np . arange(x_min, x_max, h), np . arange(y_min, y_max, h)) Z = clf . predict(np . c_[xx . ravel(), yy . ravel()]) # Put the result into a color plot Z = Z . reshape(xx . shape) plt . figure(figsize = [ 8 , 8 ]) plt . pcolormesh(xx, yy, Z, cmap = plt . cm . Paired) # Plot also the training points plt . scatter(X[:, 0 ], X[:, 1 ], c = Y, cmap = plt . cm . Paired) plt . title( '3-Class classification using Support Vector Machine with custom' ' kernel' ) plt . axis( 'tight' ) plt . show()","title":"Plot"},{"location":"Classifiers/SVM/svm/#3-one-class-svm-with-non-linear-kernel-rbf-source","text":"An example using a one-class SVM for novelty detection. One-class SVM is an unsupervised algorithm that learns a decision function for novelty detection: classifying new data as similar or different to the training set. import numpy as np import matplotlib.pyplot as plt import matplotlib.font_manager from sklearn import svm","title":"3. One-class SVM with non-linear kernel (RBF) (source)"},{"location":"Classifiers/SVM/svm/#generate-train-data","text":"# Generate train data X = 0.3 * np . random . randn( 100 , 2 ) X_train = np . r_[X + 2 , X - 2 ] # Generate some regular novel observations X = 0.3 * np . random . randn( 20 , 2 ) X_test = np . r_[X + 2 , X - 2 ] # Generate some abnormal novel observations X_outliers = np . random . uniform(low =- 4 , high = 4 , size = ( 20 , 2 ))","title":"Generate train data"},{"location":"Classifiers/SVM/svm/#fit-the-model-sklearnsvmoneclasssvm","text":"xx, yy = np . meshgrid(np . linspace( - 5 , 5 , 500 ), np . linspace( - 5 , 5 , 500 )) # fit the model clf = svm . OneClassSVM(nu = 0.1 , kernel = \"rbf\" , gamma = 0.1 ) clf . fit(X_train) #predict y_pred_train = clf . predict(X_train) y_pred_test = clf . predict(X_test) y_pred_outliers = clf . predict(X_outliers) #Error n_error_train = y_pred_train[y_pred_train == - 1 ] . size n_error_test = y_pred_test[y_pred_test == - 1 ] . size n_error_outliers = y_pred_outliers[y_pred_outliers == 1 ] . size # plot the line, the points, and the nearest vectors to the plane Z = clf . decision_function(np . c_[xx . ravel(), yy . ravel()]) Z = Z . reshape(xx . shape)","title":"Fit the Model (sklearn.svm.OneClassSVM)"},{"location":"Classifiers/SVM/svm/#plot_2","text":"plt . figure(figsize = [ 12 , 12 ]) plt . title( \"Novelty Detection\" ) plt . contourf(xx, yy, Z, levels = np . linspace(Z . min(), 0 , 7 ), cmap = plt . cm . PuBu) a = plt . contour(xx, yy, Z, levels = [ 0 ], linewidths = 2 , colors = 'darkred' ) plt . contourf(xx, yy, Z, levels = [ 0 , Z . max()], colors = 'palevioletred' ) s = 40 b1 = plt . scatter(X_train[:, 0 ], X_train[:, 1 ], c = 'white' , s = s) b2 = plt . scatter(X_test[:, 0 ], X_test[:, 1 ], c = 'blueviolet' , s = s) c = plt . scatter(X_outliers[:, 0 ], X_outliers[:, 1 ], c = 'gold' , s = s) plt . axis( 'tight' ) plt . xlim(( - 5 , 5 )) plt . ylim(( - 5 , 5 )) plt . legend([a . collections[ 0 ], b1, b2, c], [ \"learned frontier\" , \"training observations\" , \"new regular observations\" , \"new abnormal observations\" ], loc = \"upper left\" , prop = matplotlib . font_manager . FontProperties(size = 11 )) plt . xlabel( \"error train: %d /200 ; errors novel regular: %d /40 ; \" \"errors novel abnormal: %d /40\" % (n_error_train, n_error_test, n_error_outliers)) plt . show()","title":"Plot"},{"location":"Classifiers/SVM/svm/#4-rbf-svm-parameters-source","text":"This example illustrates the effect of the parameters gamma and C of the Radial Basis Function (RBF) kernel SVM. Intuitively, the gamma parameter defines how far the influence of a single training example reaches, with low values meaning 'far' and high values meaning 'close'. The gamma parameters can be seen as the inverse of the radius of influence of samples selected by the model as support vectors. The C parameter trades off misclassification of training examples against simplicity of the decision surface. A low C makes the decision surface smooth, while a high C aims at classifying all training examples correctly by giving the model freedom to select more samples as support vectors. The first plot is a visualization of the decision function for a variety of parameter values on a simplified classification problem involving only 2 input features and 2 possible target classes (binary classification). Note that this kind of plot is not possible to do for problems with more features or target classes. The second plot is a heatmap of the classifier's cross-validation accuracy as a function of C and gamma . For this example we explore a relatively large grid for illustration purposes. In practice, a logarithmic grid from :math: 10^{-3} to :math: 10^3 is usually sufficient. If the best parameters lie on the boundaries of the grid, it can be extended in that direction in a subsequent search. Note that the heat map plot has a special colorbar with a midpoint value close to the score values of the best performing models so as to make it easy to tell them appart in the blink of an eye. The behavior of the model is very sensitive to the gamma parameter. If gamma is too large, the radius of the area of influence of the support vectors only includes the support vector itself and no amount of regularization with C will be able to prevent overfitting. When gamma is very small, the model is too constrained and cannot capture the complexity or \"shape\" of the data. The region of influence of any selected support vector would include the whole training set. The resulting model will behave similarly to a linear model with a set of hyperplanes that separate the centers of high density of any pair of two classes. For intermediate values, we can see on the second plot that good models can be found on a diagonal of C and gamma . Smooth models (lower gamma values) can be made more complex by selecting a larger number of support vectors (larger C values) hence the diagonal of good performing models. Finally one can also observe that for some intermediate values of gamma we get equally performing models when C becomes very large: it is not necessary to regularize by limiting the number of support vectors. The radius of the RBF kernel alone acts as a good structural regularizer. In practice though it might still be interesting to limit the number of support vectors with a lower value of C so as to favor models that use less memory and that are faster to predict. We should also note that small differences in scores results from the random splits of the cross-validation procedure. Those spurious variations can be smoothed out by increasing the number of CV iterations n_iter at the expense of compute time. Increasing the value number of C_range and gamma_range steps will increase the resolution of the hyper-parameter heat map. import numpy as np import matplotlib.pyplot as plt from matplotlib.colors import Normalize #======== preprocessing ========== from sklearn.preprocessing import StandardScaler #===models===================== from sklearn.svm import SVC from sklearn.cross_validation import StratifiedShuffleSplit from sklearn.model_selection import GridSearchCV #======== data ========================== from sklearn.datasets import load_iris /home/bitnami/anaconda3/envs/caseolap/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20. \"This module will be removed in 0.20.\", DeprecationWarning) Utility function to move the midpoint of a colormap to be around the values of interest. class MidpointNormalize (Normalize): def __init__ ( self , vmin = None , vmax = None , midpoint = None , clip = False ): self . midpoint = midpoint Normalize . __init__ ( self , vmin, vmax, clip) def __call__ ( self , value, clip = None ): x, y = [ self . vmin, self . midpoint, self . vmax], [ 0 , 0.5 , 1 ] return np . ma . masked_array(np . interp(value, x, y)) Load and prepare data set dataset for grid search iris = load_iris() X = iris . data y = iris . target Dataset for decision function visualization: we only keep the first two features in X and sub-sample the dataset to keep only 2 classes and make it a binary classification problem. X_2d = X[:, : 2 ] X_2d = X_2d[y > 0 ] y_2d = y[y > 0 ] y_2d -= 1 It is usually a good idea to scale the data for SVM training. We are cheating a bit in this example in scaling all of the data, instead of fitting the transformation on the training set and just applying it on the test set. scaler = StandardScaler() X = scaler . fit_transform(X) X_2d = scaler . fit_transform(X_2d) Train classifiers, For an initial search, a logarithmic grid with basis 10 is often helpful. Using a basis of 2, a finer tuning can be achieved but at a much higher cost. C_range = np . logspace( - 2 , 10 , 13 ) gamma_range = np . logspace( - 9 , 3 , 13 ) param_grid = dict (gamma = gamma_range, C = C_range) cv = StratifiedShuffleSplit(n_splits = 5 , test_size = 0.2 , random_state = 42 ) grid = GridSearchCV(SVC(), param_grid = param_grid, cv = cv) grid . fit(X, y) print ( \"The best parameters are %s with a score of %0.2f \" % (grid . best_params_, grid . best_score_)) --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-40-3e73776bfe50&gt; in &lt;module&gt;() 2 gamma_range = np.logspace(-9, 3, 13) 3 param_grid = dict(gamma=gamma_range, C=C_range) ----&gt; 4 cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42) 5 grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv) 6 grid.fit(X, y) TypeError: __init__() got an unexpected keyword argument 'n_splits' Now we need to fit a classifier for all parameters in the 2d version (we use a smaller set of parameters here because it takes a while to train) C_2d_range = [ 1e-2 , 1 , 1e2 ] gamma_2d_range = [ 1e-1 , 1 , 1e1 ] classifiers = [] for C in C_2d_range: for gamma in gamma_2d_range: clf = SVC(C = C, gamma = gamma) clf . fit(X_2d, y_2d) classifiers . append((C, gamma, clf)) visualization draw visualization of parameter effects plt . figure(figsize = ( 8 , 6 )) xx, yy = np . meshgrid(np . linspace( - 3 , 3 , 200 ), np . linspace( - 3 , 3 , 200 )) for (k, (C, gamma, clf)) in enumerate (classifiers): # evaluate decision function in a grid Z = clf . decision_function(np . c_[xx . ravel(), yy . ravel()]) Z = Z . reshape(xx . shape) # visualize decision function for these parameters plt . subplot( len (C_2d_range), len (gamma_2d_range), k + 1 ) plt . title( \"gamma=10^ %d , C=10^ %d \" % (np . log10(gamma), np . log10(C)), size = 'medium' ) # visualize parameter's effect on decision function plt . pcolormesh(xx, yy, - Z, cmap = plt . cm . RdBu) plt . scatter(X_2d[:, 0 ], X_2d[:, 1 ], c = y_2d, cmap = plt . cm . RdBu_r) plt . xticks(()) plt . yticks(()) plt . axis( 'tight' ) # plot the scores of the grid # grid_scores_ contains parameter settings and scores # We extract just the scores scores = [x[ 1 ] for x in grid . grid_scores_] scores = np . array(scores) . reshape( len (C_range), len (gamma_range)) # Draw heatmap of the validation accuracy as a function of gamma and C # # The score are encoded as colors with the hot colormap which varies from dark # red to bright yellow. As the most interesting scores are all located in the # 0.92 to 0.97 range we use a custom normalizer to set the mid-point to 0.92 so # as to make it easier to visualize the small variations of score values in the # interesting range while not brutally collapsing all the low score values to # the same color. plt . figure(figsize = ( 8 , 6 )) plt . subplots_adjust(left =. 2 , right = 0.95 , bottom = 0.15 , top = 0.95 ) plt . imshow(scores, interpolation = 'nearest' , cmap = plt . cm . hot, norm = MidpointNormalize(vmin = 0.2 , midpoint = 0.92 )) plt . xlabel( 'gamma' ) plt . ylabel( 'C' ) plt . colorbar() plt . xticks(np . arange( len (gamma_range)), gamma_range, rotation = 45 ) plt . yticks(np . arange( len (C_range)), C_range) plt . title( 'Validation accuracy' ) plt . show() --------------------------------------------------------------------------- NameError Traceback (most recent call last) &lt;ipython-input-45-d7c9f3ffab83&gt; in &lt;module&gt;() 21 # grid_scores_ contains parameter settings and scores 22 # We extract just the scores ---&gt; 23 scores = [x[1] for x in grid.grid_scores_] 24 scores = np.array(scores).reshape(len(C_range), len(gamma_range)) 25 NameError: name 'grid' is not defined","title":"4. RBF SVM parameters (source)"},{"location":"Classifiers/SVM/svm/#5-svm-separating-hyperplane-for-unbalanced-classes-source","text":"Find the optimal separating hyperplane using an SVC for classes that are unbalanced. We first find the separating plane with a plain SVC and then plot (dashed) the separating hyperplane with automatically correction for unbalanced classes. This example will also work by replacing SVC(kernel=\"linear\") with SGDClassifier(loss=\"hinge\") . Setting the loss parameter of the :class: SGDClassifier equal to hinge will yield behaviour such as that of a SVC with a linear kernel. For example try instead of the SVC :: clf = SGDClassifier(n_iter=100, alpha=0.01) import numpy as np import matplotlib.pyplot as plt from sklearn import svm from sklearn.linear_model import SGDClassifier","title":"5. SVM: Separating hyperplane for unbalanced classes (source)"},{"location":"Classifiers/SVM/svm/#data","text":"# we create 40 separable points rng = np . random . RandomState( 0 ) n_samples_1 = 1000 n_samples_2 = 100 X = np . r_[ 1.5 * rng . randn(n_samples_1, 2 ), 0.5 * rng . randn(n_samples_2, 2 ) + [ 2 , 2 ]] y = [ 0 ] * (n_samples_1) + [ 1 ] * (n_samples_2)","title":"Data"},{"location":"Classifiers/SVM/svm/#model","text":"# fit the model and get the separating hyperplane clf = svm . SVC(kernel = 'linear' , C = 1.0 ) clf . fit(X, y) w = clf . coef_[ 0 ] a = - w[ 0 ] / w[ 1 ] xx = np . linspace( - 5 , 5 ) yy = a * xx - clf . intercept_[ 0 ] / w[ 1 ]","title":"Model"},{"location":"Classifiers/SVM/svm/#get-the-separating-hyperplane-using-weighted-classes","text":"# get the separating hyperplane using weighted classes wclf = svm . SVC(kernel = 'linear' , class_weight = { 1 : 10 }) wclf . fit(X, y) ww = wclf . coef_[ 0 ] wa = - ww[ 0 ] / ww[ 1 ] wyy = wa * xx - wclf . intercept_[ 0 ] / ww[ 1 ]","title":"Get the separating hyperplane using weighted classes"},{"location":"Classifiers/SVM/svm/#plot_3","text":"# plot separating hyperplanes and samples h0 = plt . plot(xx, yy, 'k-' , label = 'no weights' ) h1 = plt . plot(xx, wyy, 'k--' , label = 'with weights' ) plt . scatter(X[:, 0 ], X[:, 1 ], c = y, cmap = plt . cm . Paired) plt . legend() plt . axis( 'tight' ) plt . show()","title":"Plot"},{"location":"Classifiers/SVM/svm/#6-svm-maximum-margin-separating-hyperplane-source","text":"Plot the maximum margin separating hyperplane within a two-class separable dataset using a Support Vector Machine classifier with linear kernel. import numpy as np import matplotlib.pyplot as plt from sklearn import svm data # we create 40 separable points np . random . seed( 0 ) X = np . r_[np . random . randn( 20 , 2 ) - [ 2 , 2 ], np . random . randn( 20 , 2 ) + [ 2 , 2 ]] Y = [ 0 ] * 20 + [ 1 ] * 20 Model # fit the model clf = svm . SVC(kernel = 'linear' ) clf . fit(X, Y) # get the separating hyperplane w = clf . coef_[ 0 ] a = - w[ 0 ] / w[ 1 ] xx = np . linspace( - 5 , 5 ) yy = a * xx - (clf . intercept_[ 0 ]) / w[ 1 ] Separating hyperplane # plot the parallels to the separating hyperplane that pass through the # support vectors b = clf . support_vectors_[ 0 ] yy_down = a * xx + (b[ 1 ] - a * b[ 0 ]) b = clf . support_vectors_[ - 1 ] yy_up = a * xx + (b[ 1 ] - a * b[ 0 ]) Plot # plot the line, the points, and the nearest vectors to the plane plt . plot(xx, yy, 'k-' ) plt . plot(xx, yy_down, 'k--' ) plt . plot(xx, yy_up, 'k--' ) plt . scatter(clf . support_vectors_[:, 0 ], clf . support_vectors_[:, 1 ], s = 80 , facecolors = 'none' ) plt . scatter(X[:, 0 ], X[:, 1 ], c = Y, cmap = plt . cm . Paired) plt . axis( 'tight' ) plt . show()","title":"6. SVM: Maximum margin separating hyperplane (source)"},{"location":"Classifiers/SVM/svm/#7-svm-anova-svm-with-univariate-feature-selection-source","text":"This example shows how to perform univariate feature selection before running a SVC (support vector classifier) to improve the classification scores. import numpy as np import matplotlib.pyplot as plt from sklearn import svm, datasets, feature_selection from sklearn.cross_validation import cross_val_score from sklearn.pipeline import Pipeline Data # Import some data to play with digits = datasets . load_digits() y = digits . target # Throw away data, to be in the curse of dimension settings y = y[: 200 ] X = digits . data[: 200 ] n_samples = len (y) X = X . reshape((n_samples, - 1 )) # add 200 non-informative features X = np . hstack((X, 2 * np . random . random((n_samples, 200 )))) Feature-selection transform # Create a feature-selection transform and transform = feature_selection . SelectPercentile(feature_selection . f_classif) instance of SVM # Instance of SVM that we combine together to have an full-blown estimator clf = Pipeline([( 'anova' , transform), ( 'svc' , svm . SVC(C = 1.0 ))]) cross-validation score as a function of percentile of features #the cross-validation score as a function of percentile of features score_means = list () score_stds = list () percentiles = ( 1 , 3 , 6 , 10 , 15 , 20 , 30 , 40 , 60 , 80 , 100 ) for percentile in percentiles: clf . set_params(anova__percentile = percentile) # Compute cross-validation score using 1 CPU this_scores = cross_val_score(clf, X, y, n_jobs = 1 ) score_means . append(this_scores . mean()) score_stds . append(this_scores . std()) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 7 8 15 16 23 24 31 32 39 40 47 48 56 63] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 8 15 16 23 31 32 39 40 48 56] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 8 15 16 23 31 32 39 40 48 56] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 7 8 15 16 23 24 31 32 39 40 47 48 56 63] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 8 15 16 23 31 32 39 40 48 56] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 8 15 16 23 31 32 39 40 48 56] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 7 8 15 16 23 24 31 32 39 40 47 48 56 63] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 8 15 16 23 31 32 39 40 48 56] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 8 15 16 23 31 32 39 40 48 56] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 7 8 15 16 23 24 31 32 39 40 47 48 56 63] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 8 15 16 23 31 32 39 40 48 56] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 8 15 16 23 31 32 39 40 48 56] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 7 8 15 16 23 24 31 32 39 40 47 48 56 63] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 8 15 16 23 31 32 39 40 48 56] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 8 15 16 23 31 32 39 40 48 56] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 7 8 15 16 23 24 31 32 39 40 47 48 56 63] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 8 15 16 23 31 32 39 40 48 56] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 8 15 16 23 31 32 39 40 48 56] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 7 8 15 16 23 24 31 32 39 40 47 48 56 63] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 8 15 16 23 31 32 39 40 48 56] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 8 15 16 23 31 32 39 40 48 56] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 7 8 15 16 23 24 31 32 39 40 47 48 56 63] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 8 15 16 23 31 32 39 40 48 56] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 8 15 16 23 31 32 39 40 48 56] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 7 8 15 16 23 24 31 32 39 40 47 48 56 63] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 8 15 16 23 31 32 39 40 48 56] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 8 15 16 23 31 32 39 40 48 56] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 7 8 15 16 23 24 31 32 39 40 47 48 56 63] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 8 15 16 23 31 32 39 40 48 56] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 8 15 16 23 31 32 39 40 48 56] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 7 8 15 16 23 24 31 32 39 40 47 48 56 63] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 8 15 16 23 31 32 39 40 48 56] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 8 15 16 23 31 32 39 40 48 56] are constant. UserWarning) /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide f = msb / msw Plot plt . errorbar(percentiles, score_means, np . array(score_stds)) plt . title( 'Performance of the SVM-Anova varying the percentile of features selected' ) plt . xlabel( 'Percentile' ) plt . ylabel( 'Prediction rate' ) plt . axis( 'tight' ) plt . show()","title":"7. SVM-Anova: SVM with univariate feature selection (source)"},{"location":"Classifiers/SVM/svm/#8-svm-kernels-source","text":"Three different types of SVM-Kernels are displayed below. The polynomial and RBF are especially useful when the data-points are not linearly separable. import numpy as np import matplotlib.pyplot as plt from sklearn import svm Data # Our dataset and targets X = np . c_[( . 4 , -. 7 ), ( - 1.5 , - 1 ), ( - 1.4 , -. 9 ), ( - 1.3 , - 1.2 ), ( - 1.1 , -. 2 ), ( - 1.2 , -. 4 ), ( -. 5 , 1.2 ), ( - 1.5 , 2.1 ), ( 1 , 1 ), # -- ( 1.3 , . 8 ), ( 1.2 , . 5 ), ( . 2 , - 2 ), ( . 5 , - 2.4 ), ( . 2 , - 2.3 ), ( 0 , - 2.7 ), ( 1.3 , 2.1 )] . T Y = [ 0 ] * 8 + [ 1 ] * 8 plot # figure number fignum = 1 # fit the model for kernel in ( 'linear' , 'poly' , 'rbf' ): clf = svm . SVC(kernel = kernel, gamma = 2 ) clf . fit(X, Y) # plot the line, the points, and the nearest vectors to the plane plt . figure(fignum, figsize = ( 4 , 3 )) plt . clf() plt . scatter(clf . support_vectors_[:, 0 ], clf . support_vectors_[:, 1 ], s = 80 , facecolors = 'none' , zorder = 10 ) plt . scatter(X[:, 0 ], X[:, 1 ], c = Y, zorder = 10 , cmap = plt . cm . Paired) plt . axis( 'tight' ) x_min = - 3 x_max = 3 y_min = - 3 y_max = 3 XX, YY = np . mgrid[x_min:x_max: 200j , y_min:y_max: 200j ] Z = clf . decision_function(np . c_[XX . ravel(), YY . ravel()]) # Put the result into a color plot Z = Z . reshape(XX . shape) plt . figure(fignum, figsize = ( 4 , 3 )) plt . pcolormesh(XX, YY, Z > 0 , cmap = plt . cm . Paired) plt . contour(XX, YY, Z, colors = [ 'k' , 'k' , 'k' ], linestyles = [ '--' , '-' , '--' ], levels = [ -. 5 , 0 , . 5 ]) plt . xlim(x_min, x_max) plt . ylim(y_min, y_max) plt . xticks(()) plt . yticks(()) fignum = fignum + 1 plt . show()","title":"8. SVM-Kernels (source)"},{"location":"Classifiers/SVM/svm/#9-svm-margins-example-source","text":"The plots below illustrate the effect the parameter C has on the separation line. A large value of C basically tells our model that we do not have that much faith in our data's distribution, and will only consider points close to line of separation. A small value of C includes more/all the observations, allowing the margins to be calculated using all the data in the area. import numpy as np import matplotlib.pyplot as plt from sklearn import svm Data # we create 40 separable points np . random . seed( 0 ) X = np . r_[np . random . randn( 20 , 2 ) - [ 2 , 2 ], np . random . randn( 20 , 2 ) + [ 2 , 2 ]] Y = [ 0 ] * 20 + [ 1 ] * 20 Plot # figure number fignum = 1 # fit the model for name, penalty in (( 'unreg' , 1 ), ( 'reg' , 0.05 )): clf = svm . SVC(kernel = 'linear' , C = penalty) clf . fit(X, Y) # get the separating hyperplane w = clf . coef_[ 0 ] a = - w[ 0 ] / w[ 1 ] xx = np . linspace( - 5 , 5 ) yy = a * xx - (clf . intercept_[ 0 ]) / w[ 1 ] # plot the parallels to the separating hyperplane that pass through the # support vectors margin = 1 / np . sqrt(np . sum(clf . coef_ ** 2 )) yy_down = yy + a * margin yy_up = yy - a * margin # plot the line, the points, and the nearest vectors to the plane plt . figure(fignum, figsize = ( 4 , 3 )) plt . clf() plt . plot(xx, yy, 'k-' ) plt . plot(xx, yy_down, 'k--' ) plt . plot(xx, yy_up, 'k--' ) plt . scatter(clf . support_vectors_[:, 0 ], clf . support_vectors_[:, 1 ], s = 80 , facecolors = 'none' , zorder = 10 ) plt . scatter(X[:, 0 ], X[:, 1 ], c = Y, zorder = 10 , cmap = plt . cm . Paired) plt . axis( 'tight' ) x_min = - 4.8 x_max = 4.2 y_min = - 6 y_max = 6 XX, YY = np . mgrid[x_min:x_max: 200j , y_min:y_max: 200j ] Z = clf . predict(np . c_[XX . ravel(), YY . ravel()]) # Put the result into a color plot Z = Z . reshape(XX . shape) plt . figure(fignum, figsize = ( 4 , 3 )) plt . pcolormesh(XX, YY, Z, cmap = plt . cm . Paired) plt . xlim(x_min, x_max) plt . ylim(y_min, y_max) plt . xticks(()) plt . yticks(()) fignum = fignum + 1 plt . show()","title":"9. SVM Margins Example (source)"},{"location":"Classifiers/SVM/svm/#10-non-linear-svm-source","text":"Perform binary classification using non-linear SVC with RBF kernel. The target to predict is a XOR of the inputs. The color map illustrates the decision function learned by the SVC. import numpy as np import matplotlib.pyplot as plt from sklearn import svm Data xx, yy = np . meshgrid(np . linspace( - 3 , 3 , 500 ), np . linspace( - 3 , 3 , 500 )) np . random . seed( 0 ) X = np . random . randn( 300 , 2 ) Y = np . logical_xor(X[:, 0 ] > 0 , X[:, 1 ] > 0 ) Model # fit the model clf = svm . NuSVC() clf . fit(X, Y) # plot the decision function for each datapoint on the grid Z = clf . decision_function(np . c_[xx . ravel(), yy . ravel()]) Z = Z . reshape(xx . shape) Plot plt . imshow(Z, interpolation = 'nearest' , extent = (xx . min(), xx . max(), yy . min(), yy . max()), aspect = 'auto' , origin = 'lower' , cmap = plt . cm . PuOr_r) contours = plt . contour(xx, yy, Z, levels = [ 0 ], linewidths = 2 , linetypes = '--' ) plt . scatter(X[:, 0 ], X[:, 1 ], s = 30 , c = Y, cmap = plt . cm . Paired) plt . xticks(()) plt . yticks(()) plt . axis([ - 3 , 3 , - 3 , 3 ]) plt . show()","title":"10. Non-linear SVM (source)"},{"location":"Classifiers/SVM/svm/#11-support-vector-regression-svr-using-linear-and-non-linear-kernels-source","text":"Toy example of 1D regression using linear, polynomial and RBF kernels. import numpy as np from sklearn.svm import SVR import matplotlib.pyplot as plt Data # Generate sample data X = np . sort( 5 * np . random . rand( 40 , 1 ), axis = 0 ) y = np . sin(X) . ravel() # Add noise to targets y[:: 5 ] += 3 * ( 0.5 - np . random . rand( 8 )) Model # Fit regression model svr_rbf = SVR(kernel = 'rbf' , C = 1e3 , gamma = 0.1 ) svr_lin = SVR(kernel = 'linear' , C = 1e3 ) svr_poly = SVR(kernel = 'poly' , C = 1e3 , degree = 2 ) y_rbf = svr_rbf . fit(X, y) . predict(X) y_lin = svr_lin . fit(X, y) . predict(X) y_poly = svr_poly . fit(X, y) . predict(X) Plot # look at the results lw = 2 plt . scatter(X, y, color = 'darkorange' , label = 'data' ) plt . hold( 'on' ) plt . plot(X, y_rbf, color = 'navy' , lw = lw, label = 'RBF model' ) plt . plot(X, y_lin, color = 'c' , lw = lw, label = 'Linear model' ) plt . plot(X, y_poly, color = 'cornflowerblue' , lw = lw, label = 'Polynomial model' ) plt . xlabel( 'data' ) plt . ylabel( 'target' ) plt . title( 'Support Vector Regression' ) plt . legend() plt . show()","title":"11. Support Vector Regression (SVR) using linear and non-linear kernels (source)"},{"location":"Classifiers/SVM/svm/#13-scaling-the-regularization-parameter-for-svcs-source","text":"The following example illustrates the effect of scaling the regularization parameter when using :ref: svm for classification . For SVC classification, we are interested in a risk minimization for the equation: $ C \\sum_{i=1, n} \\mathcal{L} (f(x_i), y_i) + \\Omega (w)$ where - `C` is used to set the amount of regularization - `\\mathcal{L}` is a `loss` function of our samples and our model parameters. - `\\Omega` is a `penalty` function of our model parameters If we consider the loss function to be the individual error per sample, then the data-fit term, or the sum of the error for each sample, will increase as we add more samples. The penalization term, however, will not increase. When using, for example, :ref: cross validation <cross_validation> , to set the amount of regularization with C , there will be a different amount of samples between the main problem and the smaller problems within the folds of the cross validation. Since our loss function is dependent on the amount of samples, the latter will influence the selected value of C . The question that arises is How do we optimally adjust C to account for the different amount of training samples? The figures below are used to illustrate the effect of scaling our C to compensate for the change in the number of samples, in the case of using an l1 penalty, as well as the l2 penalty.","title":"13. Scaling the regularization parameter for SVCs (source)"},{"location":"Classifiers/SVM/svm/#l1-penalty-case","text":"In the l1 case, theory says that prediction consistency (i.e. that under given hypothesis, the estimator learned predicts as well as a model knowing the true distribution) is not possible because of the bias of the l1 . It does say, however, that model consistency, in terms of finding the right set of non-zero parameters as well as their signs, can be achieved by scaling C1 .","title":"l1-penalty case"},{"location":"Classifiers/SVM/svm/#l2-penalty-case","text":"The theory says that in order to achieve prediction consistency, the penalty parameter should be kept constant as the number of samples grow.","title":"l2-penalty case"},{"location":"Classifiers/SVM/svm/#simulations","text":"The two figures below plot the values of C on the x-axis and the corresponding cross-validation scores on the y-axis , for several different fractions of a generated data-set. In the l1 penalty case, the cross-validation-error correlates best with the test-error, when scaling our C with the number of samples, n , which can be seen in the first figure. For the l2 penalty case, the best result comes from the case where C is not scaled. Two separate datasets are used for the two different plots. The reason behind this is the `l1` case works better on sparse data, while `l2` is better suited to the non-sparse case. import numpy as np import matplotlib.pyplot as plt from sklearn.svm import LinearSVC from sklearn.model_selection import ShuffleSplit from sklearn.model_selection import GridSearchCV from sklearn.utils import check_random_state from sklearn import datasets Data,Model,Plot rnd = check_random_state( 1 ) # set up dataset n_samples = 100 n_features = 300 # l1 data (only 5 informative features) X_1, y_1 = datasets . make_classification(n_samples = n_samples, n_features = n_features, n_informative = 5 , random_state = 1 ) # l2 data: non sparse, but less features y_2 = np . sign( . 5 - rnd . rand(n_samples)) X_2 = rnd . randn(n_samples, n_features / 5 ) + y_2[:, np . newaxis] X_2 += 5 * rnd . randn(n_samples, n_features / 5 ) clf_sets = [(LinearSVC(penalty = 'l1' , loss = 'squared_hinge' , dual = False , tol = 1e-3 ), np . logspace( - 2.3 , - 1.3 , 10 ), X_1, y_1), (LinearSVC(penalty = 'l2' , loss = 'squared_hinge' , dual = True , tol = 1e-4 ), np . logspace( - 4.5 , - 2 , 10 ), X_2, y_2)] colors = [ 'navy' , 'cyan' , 'darkorange' ] lw = 2 for fignum, (clf, cs, X, y) in enumerate (clf_sets): # set up the plot for each regressor plt . figure(fignum, figsize = ( 9 , 10 )) for k, train_size in enumerate (np . linspace( 0.3 , 0.7 , 3 )[:: - 1 ]): param_grid = dict (C = cs) # To get nice curve, we need a large number of iterations to # reduce the variance grid = GridSearchCV(clf, refit = False , param_grid = param_grid, cv = ShuffleSplit(train_size = train_size, n_iter = 250 , random_state = 1 )) grid . fit(X, y) scores = [x[ 1 ] for x in grid . grid_scores_] scales = [( 1 , 'No scaling' ), ((n_samples * train_size), '1/n_samples' ), ] for subplotnum, (scaler, name) in enumerate (scales): plt . subplot( 2 , 1 , subplotnum + 1 ) plt . xlabel( 'C' ) plt . ylabel( 'CV Score' ) grid_cs = cs * float (scaler) # scale the C's plt . semilogx(grid_cs, scores, label = \"fraction %.2f \" % train_size, color = colors[k], lw = lw) plt . title( 'scaling= %s , penalty= %s , loss= %s ' % (name, clf . penalty, clf . loss)) plt . legend(loc = \"best\" ) plt . show() /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:14: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:15: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-83-3265a6814293&gt; in &lt;module&gt;() 35 grid = GridSearchCV(clf, refit=False, param_grid=param_grid, 36 cv=ShuffleSplit(train_size=train_size, n_iter=250, ---&gt; 37 random_state=1)) 38 grid.fit(X, y) 39 scores = [x[1] for x in grid.grid_scores_] TypeError: __init__() got an unexpected keyword argument 'n_iter'","title":"Simulations"},{"location":"Classifiers/SVM/svm/#14-svm-weighted-samples-source","text":"Plot decision function of a weighted dataset, where the size of points is proportional to its weight. The sample weighting rescales the C parameter, which means that the classifier puts more emphasis on getting these points right. The effect might often be subtle. To emphasize the effect here, we particularly weight outliers, making the deformation of the decision boundary very visible. import numpy as np import matplotlib.pyplot as plt from sklearn import svm def plot_decision_function (classifier, sample_weight, axis, title): # plot the decision function xx, yy = np . meshgrid(np . linspace( - 4 , 5 , 500 ), np . linspace( - 4 , 5 , 500 )) Z = classifier . decision_function(np . c_[xx . ravel(), yy . ravel()]) Z = Z . reshape(xx . shape) # plot the line, the points, and the nearest vectors to the plane axis . contourf(xx, yy, Z, alpha = 0.75 , cmap = plt . cm . bone) axis . scatter(X[:, 0 ], X[:, 1 ], c = y, s = 100 * sample_weight, alpha = 0.9 , cmap = plt . cm . bone) axis . axis( 'off' ) axis . set_title(title) Data # we create 20 points np . random . seed( 0 ) X = np . r_[np . random . randn( 10 , 2 ) + [ 1 , 1 ], np . random . randn( 10 , 2 )] y = [ 1 ] * 10 + [ - 1 ] * 10 sample_weight_last_ten = abs (np . random . randn( len (X))) sample_weight_constant = np . ones( len (X)) # and bigger weights to some outliers sample_weight_last_ten[ 15 :] *= 5 sample_weight_last_ten[ 9 ] *= 15 Model # for reference, first fit without class weights # fit the model clf_weights = svm . SVC() clf_weights . fit(X, y, sample_weight = sample_weight_last_ten) clf_no_weights = svm . SVC() clf_no_weights . fit(X, y) SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma='auto', kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) Plot fig, axes = plt . subplots( 1 , 2 , figsize = ( 14 , 6 )) plot_decision_function(clf_no_weights, sample_weight_constant, axes[ 0 ], \"Constant weights\" ) plot_decision_function(clf_weights, sample_weight_last_ten, axes[ 1 ], \"Modified weights\" ) plt . show() &lt;matplotlib.figure.Figure at 0x114969e80&gt;","title":"14. SVM: Weighted samples (source)"},{"location":"Classifiers/Tree/Tree/","text":"Decision Tree Introduction Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. from sklearn import tree % matplotlib inline X = [[ 0 , 0 ], [ 1 , 1 ]] Y = [ 0 , 1 ] clf = tree . DecisionTreeClassifier() clf = clf . fit(X, Y) clf . predict([[ 2. , 2. ]]) array([1]) clf . predict_proba([[ 2. , 2. ]]) array([[0., 1.]]) DecisionTreeClassifier is capable of both binary (where the labels are [-1, 1]) classification and multiclass (where the labels are [0, ..., K-1]) classification. Iris Tree Using the Iris dataset, we can construct a tree as follows: from sklearn.datasets import load_iris from sklearn import tree iris = load_iris() clf = tree . DecisionTreeClassifier() clf = clf . fit(iris . data, iris . target) from sklearn.externals.six import StringIO with open ( \"iris.dot\" , 'w' ) as f: f = tree . export_graphviz(clf, out_file = f) import os os . unlink( 'iris.dot' ) from sklearn.externals.six import StringIO import pydot dot_data = StringIO() tree . export_graphviz(clf, out_file = dot_data) graph = pydot . graph_from_dot_data(dot_data . getvalue()) graph . write_pdf( \"iris.pdf\" ) --------------------------------------------------------------------------- ModuleNotFoundError Traceback (most recent call last) &lt;ipython-input-12-52bcb3d743e1&gt; in &lt;module&gt;() 1 from sklearn.externals.six import StringIO ----&gt; 2 import pydot 3 dot_data = StringIO() 4 tree.export_graphviz(clf, out_file=dot_data) 5 graph = pydot.graph_from_dot_data(dot_data.getvalue()) ModuleNotFoundError: No module named 'pydot' from IPython.display import Image dot_data = StringIO() tree . export_graphviz(clf, out_file = dot_data, feature_names = iris . feature_names, class_names = iris . target_names, filled = True , rounded = True , special_characters = True ) graph = pydot . graph_from_dot_data(dot_data . getvalue()) Image(graph . create_png()) --------------------------------------------------------------------------- NameError Traceback (most recent call last) &lt;ipython-input-13-d9934c394935&gt; in &lt;module&gt;() 6 filled=True, rounded=True, 7 special_characters=True) ----&gt; 8 graph = pydot.graph_from_dot_data(dot_data.getvalue()) 9 Image(graph.create_png()) NameError: name 'pydot' is not defined from IPython.display import Image Image(filename = 'iris.png' ) --------------------------------------------------------------------------- FileNotFoundError Traceback (most recent call last) &lt;ipython-input-14-53607bd2a2e8&gt; in &lt;module&gt;() 1 from IPython.display import Image ----&gt; 2 Image(filename='iris.png') ~/anaconda3/envs/caseolap/lib/python3.6/site-packages/IPython/core/display.py in __init__(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata) 1132 self.unconfined = unconfined 1133 super(Image, self).__init__(data=data, url=url, filename=filename, -&gt; 1134 metadata=metadata) 1135 1136 if self.width is None and self.metadata.get('width', {}): ~/anaconda3/envs/caseolap/lib/python3.6/site-packages/IPython/core/display.py in __init__(self, data, url, filename, metadata) 606 self.metadata = {} 607 --&gt; 608 self.reload() 609 self._check_data() 610 ~/anaconda3/envs/caseolap/lib/python3.6/site-packages/IPython/core/display.py in reload(self) 1163 \"\"\"Reload the raw data from file or URL.\"\"\" 1164 if self.embed: -&gt; 1165 super(Image,self).reload() 1166 if self.retina: 1167 self._retina_shape() ~/anaconda3/envs/caseolap/lib/python3.6/site-packages/IPython/core/display.py in reload(self) 631 \"\"\"Reload the raw data from file or URL.\"\"\" 632 if self.filename is not None: --&gt; 633 with open(self.filename, self._read_flags) as f: 634 self.data = f.read() 635 elif self.url is not None: FileNotFoundError: [Errno 2] No such file or directory: 'iris.png' clf . predict(iris . data[: 1 , :]) array([0]) clf . predict_proba(iris . data[: 1 , :]) array([[1., 0., 0.]]) Examples 1. Plot the decision surface of a decision tree on the iris dataset ( source ) Plot the decision surface of a decision tree trained on pairs of features of the iris dataset. For each pair of iris features, the decision tree learns decision boundaries made of combinations of simple thresholding rules inferred from the training samples. import numpy as np import matplotlib.pyplot as plt #=======model================ from sklearn.tree import DecisionTreeClassifier #======= data ================================= from sklearn.datasets import load_iris Data and Parameter # Parameters n_classes = 3 plot_colors = \"bry\" plot_step = 0.02 # Load data iris = load_iris() Plot for pairidx, pair in enumerate ([[ 0 , 1 ], [ 0 , 2 ], [ 0 , 3 ], [ 1 , 2 ], [ 1 , 3 ], [ 2 , 3 ]]): # We only take the two corresponding features X = iris . data[:, pair] y = iris . target # Train clf = DecisionTreeClassifier() . fit(X, y) # Plot the decision boundary plt . subplot( 2 , 3 , pairidx + 1 ) x_min, x_max = X[:, 0 ] . min() - 1 , X[:, 0 ] . max() + 1 y_min, y_max = X[:, 1 ] . min() - 1 , X[:, 1 ] . max() + 1 xx, yy = np . meshgrid(np . arange(x_min, x_max, plot_step), np . arange(y_min, y_max, plot_step)) Z = clf . predict(np . c_[xx . ravel(), yy . ravel()]) Z = Z . reshape(xx . shape) cs = plt . contourf(xx, yy, Z, cmap = plt . cm . Paired) plt . xlabel(iris . feature_names[pair[ 0 ]]) plt . ylabel(iris . feature_names[pair[ 1 ]]) plt . axis( \"tight\" ) # Plot the training points for i, color in zip ( range (n_classes), plot_colors): idx = np . where(y == i) plt . scatter(X[idx, 0 ], X[idx, 1 ], c = color, label = iris . target_names[i], cmap = plt . cm . Paired) plt . axis( \"tight\" ) plt . suptitle( \"Decision surface of a decision tree using paired features\" ) plt . legend() plt . show() 2. Understanding the decision tree structure ( source ) The decision tree structure can be analysed to gain further insight on the relation between the features and the target to predict. In this example, we show how to retrieve: the binary tree structure; the depth of each node and whether or not it's a leaf; the nodes that were reached by a sample using the decision_path method; the leaf that was reached by a sample using the apply method; the rules that were used to predict a sample; the decision path shared by a group of samples. import numpy as np #======= preprocessing& model selection============ from sklearn.model_selection import train_test_split #======== model ================ from sklearn.tree import DecisionTreeClassifier #========= data ============== from sklearn.datasets import load_iris iris = load_iris() X = iris . data y = iris . target X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0 ) estimator = DecisionTreeClassifier(max_leaf_nodes = 3 , random_state = 0 ) estimator . fit(X_train, y_train) DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None, max_features=None, max_leaf_nodes=3, min_impurity_split=1e-07, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=0, splitter='best') The decision estimator has an attribute called tree_ which stores the entire tree structure and allows access to low level attributes. The binary tree tree_ is represented as a number of parallel arrays. The i-th element of each array holds information about the node i . Node 0 is the tree's root. NOTE: Some of the arrays only apply to either leaves or split nodes, resp. In this case the values of nodes of the other type are arbitrary!* Among those arrays, we have: - left_child, id of the left child of the node - right_child, id of the right child of the node - feature, feature used for splitting the node - threshold, threshold value at the node Using those arrays, we can parse the tree structure: n_nodes = estimator . tree_ . node_count children_left = estimator . tree_ . children_left children_right = estimator . tree_ . children_right feature = estimator . tree_ . feature threshold = estimator . tree_ . threshold The tree structure can be traversed to compute various properties such as the depth of each node and whether or not it is a leaf. node_depth = np . zeros(shape = n_nodes) is_leaves = np . zeros(shape = n_nodes, dtype = bool ) stack = [( 0 , - 1 )] # seed is the root node id and its parent depth while len (stack) > 0 : node_id, parent_depth = stack . pop() node_depth[node_id] = parent_depth + 1 # If we have a test node if (children_left[node_id] != children_right[node_id]): stack . append((children_left[node_id], parent_depth + 1 )) stack . append((children_right[node_id], parent_depth + 1 )) else : is_leaves[node_id] = True print ( \"The binary tree structure has %s nodes and has \" \"the following tree structure:\" % n_nodes) The binary tree structure has 5 nodes and has the following tree structure: for i in range (n_nodes): if is_leaves[i]: print ( \" %s node= %s leaf node.\" % (node_depth[i] * \" \\t \" , i)) else : print ( \" %s node= %s test node: go to node %s if X[:, %s ] <= %s s else to \" \"node %s .\" % (node_depth[i] * \" \\t \" , i, children_left[i], feature[i], threshold[i], children_right[i], )) print () node=0 test node: go to node 1 if X[:, 3] &lt;= 0.800000011921s else to node 2. node=1 leaf node. node=2 test node: go to node 3 if X[:, 2] &lt;= 4.94999980927s else to node 4. node=3 leaf node. node=4 leaf node. /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:7: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:3: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future app.launch_new_instance() First let's retrieve the decision path of each sample. The decision_path method allows to retrieve the node indicator functions. A non zero element of indicator matrix at the position (i, j) indicates that the sample i goes through the node j. node_indicator = estimator . decision_path(X_test) Similarly, we can also have the leaves ids reached by each sample. leave_id = estimator . apply(X_test) Now, it's possible to get the tests that were used to predict a sample or a group of samples. First, let's make it for the sample. sample_id = 0 node_index = node_indicator . indices[node_indicator . indptr[sample_id]: node_indicator . indptr[sample_id + 1 ]] print ( 'Rules used to predict sample %s : ' % sample_id) for node_id in node_index: if leave_id[sample_id] != node_id: continue if (X_test[sample_id, feature[node_id]] <= threshold[node_id]): threshold_sign = \"<=\" else : threshold_sign = \">\" print ( \"decision id node %s : (X[ %s , %s ] (= %s ) %s %s )\" % (node_id, sample_id, feature[node_id], X_test[i, feature[node_id]], threshold_sign, threshold[node_id])) # For a group of samples, we have the following common node. sample_ids = [ 0 , 1 ] common_nodes = (node_indicator . toarray()[sample_ids] . sum(axis = 0 ) == len (sample_ids)) common_node_id = np . arange(n_nodes)[common_nodes] print ( \" \\n The following samples %s share the node %s in the tree\" % (sample_ids, common_node_id)) print ( \"It is %s %% of all nodes.\" % ( 100 * len (common_node_id) / n_nodes,)) Rules used to predict sample 0: decision id node 4 : (X[0, -2] (= 1.5) &gt; -2.0) The following samples [0, 1] share the node [0 2] in the tree It is 40.0 % of all nodes. 3. Decision Tree Regression ( source ) \"\"\" =================================================================== Decision Tree Regression =================================================================== A 1D regression with decision tree. The :ref:`decision trees <tree>` is used to fit a sine curve with addition noisy observation. As a result, it learns local linear regressions approximating the sine curve. We can see that if the maximum depth of the tree (controlled by the `max_depth` parameter) is set too high, the decision trees learn too fine details of the training data and learn from the noise, i.e. they overfit. \"\"\" # print(__doc__) # Import the necessary modules and libraries import numpy as np from sklearn.tree import DecisionTreeRegressor import matplotlib.pyplot as plt Data # Create a random dataset rng = np . random . RandomState( 1 ) X = np . sort( 5 * rng . rand( 80 , 1 ), axis = 0 ) y = np . sin(X) . ravel() y[:: 5 ] += 3 * ( 0.5 - rng . rand( 16 )) Model # Fit regression model regr_1 = DecisionTreeRegressor(max_depth = 2 ) regr_2 = DecisionTreeRegressor(max_depth = 5 ) regr_1 . fit(X, y) regr_2 . fit(X, y) DecisionTreeRegressor(criterion='mse', max_depth=5, max_features=None, max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=None, splitter='best') Predict # Predict X_test = np . arange( 0.0 , 5.0 , 0.01 )[:, np . newaxis] y_1 = regr_1 . predict(X_test) y_2 = regr_2 . predict(X_test) Plot # Plot the results plt . figure() plt . scatter(X, y, c = \"darkorange\" , label = \"data\" ) plt . plot(X_test, y_1, color = \"cornflowerblue\" , label = \"max_depth=2\" , linewidth = 2 ) plt . plot(X_test, y_2, color = \"yellowgreen\" , label = \"max_depth=5\" , linewidth = 2 ) plt . xlabel( \"data\" ) plt . ylabel( \"target\" ) plt . title( \"Decision Tree Regression\" ) plt . legend() plt . show() 4. Multi-output Decision Tree Regression ( source ) An example to illustrate multi-output regression with decision tree. The decision trees is used to predict simultaneously the noisy x and y observations of a circle given a single underlying feature. As a result, it learns local linear regressions approximating the circle. We can see that if the maximum depth of the tree (controlled by the max_depth parameter) is set too high, the decision trees learn too fine details of the training data and learn from the noise, i.e. they overfit. import numpy as np import matplotlib.pyplot as plt from sklearn.tree import DecisionTreeRegressor Data: Create a random dataset rng = np . random . RandomState( 1 ) X = np . sort( 200 * rng . rand( 100 , 1 ) - 100 , axis = 0 ) y = np . array([np . pi * np . sin(X) . ravel(), np . pi * np . cos(X) . ravel()]) . T y[:: 5 , :] += ( 0.5 - rng . rand( 20 , 2 )) Model: Fit regression model regr_1 = DecisionTreeRegressor(max_depth = 2 ) regr_2 = DecisionTreeRegressor(max_depth = 5 ) regr_3 = DecisionTreeRegressor(max_depth = 8 ) regr_1 . fit(X, y) regr_2 . fit(X, y) regr_3 . fit(X, y) DecisionTreeRegressor(criterion='mse', max_depth=8, max_features=None, max_leaf_nodes=None, min_impurity_split=1e-07, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=None, splitter='best') Predict X_test = np . arange( - 100.0 , 100.0 , 0.01 )[:, np . newaxis] y_1 = regr_1 . predict(X_test) y_2 = regr_2 . predict(X_test) y_3 = regr_3 . predict(X_test) Plot plt . figure() s = 50 plt . scatter(y[:, 0 ], y[:, 1 ], c = \"navy\" , s = s, label = \"data\" ) plt . scatter(y_1[:, 0 ], y_1[:, 1 ], c = \"cornflowerblue\" , s = s, label = \"max_depth=2\" ) plt . scatter(y_2[:, 0 ], y_2[:, 1 ], c = \"c\" , s = s, label = \"max_depth=5\" ) plt . scatter(y_3[:, 0 ], y_3[:, 1 ], c = \"orange\" , s = s, label = \"max_depth=8\" ) plt . xlim([ - 6 , 6 ]) plt . ylim([ - 6 , 6 ]) plt . xlabel( \"target 1\" ) plt . ylabel( \"target 2\" ) plt . title( \"Multi-output Decision Tree Regression\" ) plt . legend() plt . show()","title":"Decision Tree"},{"location":"Classifiers/Tree/Tree/#decision-tree","text":"","title":"Decision Tree"},{"location":"Classifiers/Tree/Tree/#introduction","text":"Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. from sklearn import tree % matplotlib inline X = [[ 0 , 0 ], [ 1 , 1 ]] Y = [ 0 , 1 ] clf = tree . DecisionTreeClassifier() clf = clf . fit(X, Y) clf . predict([[ 2. , 2. ]]) array([1]) clf . predict_proba([[ 2. , 2. ]]) array([[0., 1.]]) DecisionTreeClassifier is capable of both binary (where the labels are [-1, 1]) classification and multiclass (where the labels are [0, ..., K-1]) classification.","title":"Introduction"},{"location":"Classifiers/Tree/Tree/#iris-tree","text":"Using the Iris dataset, we can construct a tree as follows: from sklearn.datasets import load_iris from sklearn import tree iris = load_iris() clf = tree . DecisionTreeClassifier() clf = clf . fit(iris . data, iris . target) from sklearn.externals.six import StringIO with open ( \"iris.dot\" , 'w' ) as f: f = tree . export_graphviz(clf, out_file = f) import os os . unlink( 'iris.dot' ) from sklearn.externals.six import StringIO import pydot dot_data = StringIO() tree . export_graphviz(clf, out_file = dot_data) graph = pydot . graph_from_dot_data(dot_data . getvalue()) graph . write_pdf( \"iris.pdf\" ) --------------------------------------------------------------------------- ModuleNotFoundError Traceback (most recent call last) &lt;ipython-input-12-52bcb3d743e1&gt; in &lt;module&gt;() 1 from sklearn.externals.six import StringIO ----&gt; 2 import pydot 3 dot_data = StringIO() 4 tree.export_graphviz(clf, out_file=dot_data) 5 graph = pydot.graph_from_dot_data(dot_data.getvalue()) ModuleNotFoundError: No module named 'pydot' from IPython.display import Image dot_data = StringIO() tree . export_graphviz(clf, out_file = dot_data, feature_names = iris . feature_names, class_names = iris . target_names, filled = True , rounded = True , special_characters = True ) graph = pydot . graph_from_dot_data(dot_data . getvalue()) Image(graph . create_png()) --------------------------------------------------------------------------- NameError Traceback (most recent call last) &lt;ipython-input-13-d9934c394935&gt; in &lt;module&gt;() 6 filled=True, rounded=True, 7 special_characters=True) ----&gt; 8 graph = pydot.graph_from_dot_data(dot_data.getvalue()) 9 Image(graph.create_png()) NameError: name 'pydot' is not defined from IPython.display import Image Image(filename = 'iris.png' ) --------------------------------------------------------------------------- FileNotFoundError Traceback (most recent call last) &lt;ipython-input-14-53607bd2a2e8&gt; in &lt;module&gt;() 1 from IPython.display import Image ----&gt; 2 Image(filename='iris.png') ~/anaconda3/envs/caseolap/lib/python3.6/site-packages/IPython/core/display.py in __init__(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata) 1132 self.unconfined = unconfined 1133 super(Image, self).__init__(data=data, url=url, filename=filename, -&gt; 1134 metadata=metadata) 1135 1136 if self.width is None and self.metadata.get('width', {}): ~/anaconda3/envs/caseolap/lib/python3.6/site-packages/IPython/core/display.py in __init__(self, data, url, filename, metadata) 606 self.metadata = {} 607 --&gt; 608 self.reload() 609 self._check_data() 610 ~/anaconda3/envs/caseolap/lib/python3.6/site-packages/IPython/core/display.py in reload(self) 1163 \"\"\"Reload the raw data from file or URL.\"\"\" 1164 if self.embed: -&gt; 1165 super(Image,self).reload() 1166 if self.retina: 1167 self._retina_shape() ~/anaconda3/envs/caseolap/lib/python3.6/site-packages/IPython/core/display.py in reload(self) 631 \"\"\"Reload the raw data from file or URL.\"\"\" 632 if self.filename is not None: --&gt; 633 with open(self.filename, self._read_flags) as f: 634 self.data = f.read() 635 elif self.url is not None: FileNotFoundError: [Errno 2] No such file or directory: 'iris.png' clf . predict(iris . data[: 1 , :]) array([0]) clf . predict_proba(iris . data[: 1 , :]) array([[1., 0., 0.]])","title":"Iris Tree"},{"location":"Classifiers/Tree/Tree/#examples","text":"","title":"Examples"},{"location":"Classifiers/Tree/Tree/#1-plot-the-decision-surface-of-a-decision-tree-on-the-iris-dataset-source","text":"Plot the decision surface of a decision tree trained on pairs of features of the iris dataset. For each pair of iris features, the decision tree learns decision boundaries made of combinations of simple thresholding rules inferred from the training samples. import numpy as np import matplotlib.pyplot as plt #=======model================ from sklearn.tree import DecisionTreeClassifier #======= data ================================= from sklearn.datasets import load_iris Data and Parameter # Parameters n_classes = 3 plot_colors = \"bry\" plot_step = 0.02 # Load data iris = load_iris() Plot for pairidx, pair in enumerate ([[ 0 , 1 ], [ 0 , 2 ], [ 0 , 3 ], [ 1 , 2 ], [ 1 , 3 ], [ 2 , 3 ]]): # We only take the two corresponding features X = iris . data[:, pair] y = iris . target # Train clf = DecisionTreeClassifier() . fit(X, y) # Plot the decision boundary plt . subplot( 2 , 3 , pairidx + 1 ) x_min, x_max = X[:, 0 ] . min() - 1 , X[:, 0 ] . max() + 1 y_min, y_max = X[:, 1 ] . min() - 1 , X[:, 1 ] . max() + 1 xx, yy = np . meshgrid(np . arange(x_min, x_max, plot_step), np . arange(y_min, y_max, plot_step)) Z = clf . predict(np . c_[xx . ravel(), yy . ravel()]) Z = Z . reshape(xx . shape) cs = plt . contourf(xx, yy, Z, cmap = plt . cm . Paired) plt . xlabel(iris . feature_names[pair[ 0 ]]) plt . ylabel(iris . feature_names[pair[ 1 ]]) plt . axis( \"tight\" ) # Plot the training points for i, color in zip ( range (n_classes), plot_colors): idx = np . where(y == i) plt . scatter(X[idx, 0 ], X[idx, 1 ], c = color, label = iris . target_names[i], cmap = plt . cm . Paired) plt . axis( \"tight\" ) plt . suptitle( \"Decision surface of a decision tree using paired features\" ) plt . legend() plt . show()","title":"1. Plot the decision surface of a decision tree on the iris dataset (source)"},{"location":"Classifiers/Tree/Tree/#2-understanding-the-decision-tree-structure-source","text":"The decision tree structure can be analysed to gain further insight on the relation between the features and the target to predict. In this example, we show how to retrieve: the binary tree structure; the depth of each node and whether or not it's a leaf; the nodes that were reached by a sample using the decision_path method; the leaf that was reached by a sample using the apply method; the rules that were used to predict a sample; the decision path shared by a group of samples. import numpy as np #======= preprocessing& model selection============ from sklearn.model_selection import train_test_split #======== model ================ from sklearn.tree import DecisionTreeClassifier #========= data ============== from sklearn.datasets import load_iris iris = load_iris() X = iris . data y = iris . target X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0 ) estimator = DecisionTreeClassifier(max_leaf_nodes = 3 , random_state = 0 ) estimator . fit(X_train, y_train) DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None, max_features=None, max_leaf_nodes=3, min_impurity_split=1e-07, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=0, splitter='best') The decision estimator has an attribute called tree_ which stores the entire tree structure and allows access to low level attributes. The binary tree tree_ is represented as a number of parallel arrays. The i-th element of each array holds information about the node i . Node 0 is the tree's root. NOTE: Some of the arrays only apply to either leaves or split nodes, resp. In this case the values of nodes of the other type are arbitrary!* Among those arrays, we have: - left_child, id of the left child of the node - right_child, id of the right child of the node - feature, feature used for splitting the node - threshold, threshold value at the node Using those arrays, we can parse the tree structure: n_nodes = estimator . tree_ . node_count children_left = estimator . tree_ . children_left children_right = estimator . tree_ . children_right feature = estimator . tree_ . feature threshold = estimator . tree_ . threshold The tree structure can be traversed to compute various properties such as the depth of each node and whether or not it is a leaf. node_depth = np . zeros(shape = n_nodes) is_leaves = np . zeros(shape = n_nodes, dtype = bool ) stack = [( 0 , - 1 )] # seed is the root node id and its parent depth while len (stack) > 0 : node_id, parent_depth = stack . pop() node_depth[node_id] = parent_depth + 1 # If we have a test node if (children_left[node_id] != children_right[node_id]): stack . append((children_left[node_id], parent_depth + 1 )) stack . append((children_right[node_id], parent_depth + 1 )) else : is_leaves[node_id] = True print ( \"The binary tree structure has %s nodes and has \" \"the following tree structure:\" % n_nodes) The binary tree structure has 5 nodes and has the following tree structure: for i in range (n_nodes): if is_leaves[i]: print ( \" %s node= %s leaf node.\" % (node_depth[i] * \" \\t \" , i)) else : print ( \" %s node= %s test node: go to node %s if X[:, %s ] <= %s s else to \" \"node %s .\" % (node_depth[i] * \" \\t \" , i, children_left[i], feature[i], threshold[i], children_right[i], )) print () node=0 test node: go to node 1 if X[:, 3] &lt;= 0.800000011921s else to node 2. node=1 leaf node. node=2 test node: go to node 3 if X[:, 2] &lt;= 4.94999980927s else to node 4. node=3 leaf node. node=4 leaf node. /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:7: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:3: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future app.launch_new_instance() First let's retrieve the decision path of each sample. The decision_path method allows to retrieve the node indicator functions. A non zero element of indicator matrix at the position (i, j) indicates that the sample i goes through the node j. node_indicator = estimator . decision_path(X_test) Similarly, we can also have the leaves ids reached by each sample. leave_id = estimator . apply(X_test) Now, it's possible to get the tests that were used to predict a sample or a group of samples. First, let's make it for the sample. sample_id = 0 node_index = node_indicator . indices[node_indicator . indptr[sample_id]: node_indicator . indptr[sample_id + 1 ]] print ( 'Rules used to predict sample %s : ' % sample_id) for node_id in node_index: if leave_id[sample_id] != node_id: continue if (X_test[sample_id, feature[node_id]] <= threshold[node_id]): threshold_sign = \"<=\" else : threshold_sign = \">\" print ( \"decision id node %s : (X[ %s , %s ] (= %s ) %s %s )\" % (node_id, sample_id, feature[node_id], X_test[i, feature[node_id]], threshold_sign, threshold[node_id])) # For a group of samples, we have the following common node. sample_ids = [ 0 , 1 ] common_nodes = (node_indicator . toarray()[sample_ids] . sum(axis = 0 ) == len (sample_ids)) common_node_id = np . arange(n_nodes)[common_nodes] print ( \" \\n The following samples %s share the node %s in the tree\" % (sample_ids, common_node_id)) print ( \"It is %s %% of all nodes.\" % ( 100 * len (common_node_id) / n_nodes,)) Rules used to predict sample 0: decision id node 4 : (X[0, -2] (= 1.5) &gt; -2.0) The following samples [0, 1] share the node [0 2] in the tree It is 40.0 % of all nodes.","title":"2. Understanding the decision tree structure (source)"},{"location":"Classifiers/Tree/Tree/#3-decision-tree-regression-source","text":"\"\"\" =================================================================== Decision Tree Regression =================================================================== A 1D regression with decision tree. The :ref:`decision trees <tree>` is used to fit a sine curve with addition noisy observation. As a result, it learns local linear regressions approximating the sine curve. We can see that if the maximum depth of the tree (controlled by the `max_depth` parameter) is set too high, the decision trees learn too fine details of the training data and learn from the noise, i.e. they overfit. \"\"\" # print(__doc__) # Import the necessary modules and libraries import numpy as np from sklearn.tree import DecisionTreeRegressor import matplotlib.pyplot as plt Data # Create a random dataset rng = np . random . RandomState( 1 ) X = np . sort( 5 * rng . rand( 80 , 1 ), axis = 0 ) y = np . sin(X) . ravel() y[:: 5 ] += 3 * ( 0.5 - rng . rand( 16 )) Model # Fit regression model regr_1 = DecisionTreeRegressor(max_depth = 2 ) regr_2 = DecisionTreeRegressor(max_depth = 5 ) regr_1 . fit(X, y) regr_2 . fit(X, y) DecisionTreeRegressor(criterion='mse', max_depth=5, max_features=None, max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=None, splitter='best') Predict # Predict X_test = np . arange( 0.0 , 5.0 , 0.01 )[:, np . newaxis] y_1 = regr_1 . predict(X_test) y_2 = regr_2 . predict(X_test) Plot # Plot the results plt . figure() plt . scatter(X, y, c = \"darkorange\" , label = \"data\" ) plt . plot(X_test, y_1, color = \"cornflowerblue\" , label = \"max_depth=2\" , linewidth = 2 ) plt . plot(X_test, y_2, color = \"yellowgreen\" , label = \"max_depth=5\" , linewidth = 2 ) plt . xlabel( \"data\" ) plt . ylabel( \"target\" ) plt . title( \"Decision Tree Regression\" ) plt . legend() plt . show()","title":"3. Decision Tree Regression (source)"},{"location":"Classifiers/Tree/Tree/#4-multi-output-decision-tree-regression-source","text":"An example to illustrate multi-output regression with decision tree. The decision trees is used to predict simultaneously the noisy x and y observations of a circle given a single underlying feature. As a result, it learns local linear regressions approximating the circle. We can see that if the maximum depth of the tree (controlled by the max_depth parameter) is set too high, the decision trees learn too fine details of the training data and learn from the noise, i.e. they overfit. import numpy as np import matplotlib.pyplot as plt from sklearn.tree import DecisionTreeRegressor Data: Create a random dataset rng = np . random . RandomState( 1 ) X = np . sort( 200 * rng . rand( 100 , 1 ) - 100 , axis = 0 ) y = np . array([np . pi * np . sin(X) . ravel(), np . pi * np . cos(X) . ravel()]) . T y[:: 5 , :] += ( 0.5 - rng . rand( 20 , 2 )) Model: Fit regression model regr_1 = DecisionTreeRegressor(max_depth = 2 ) regr_2 = DecisionTreeRegressor(max_depth = 5 ) regr_3 = DecisionTreeRegressor(max_depth = 8 ) regr_1 . fit(X, y) regr_2 . fit(X, y) regr_3 . fit(X, y) DecisionTreeRegressor(criterion='mse', max_depth=8, max_features=None, max_leaf_nodes=None, min_impurity_split=1e-07, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=None, splitter='best') Predict X_test = np . arange( - 100.0 , 100.0 , 0.01 )[:, np . newaxis] y_1 = regr_1 . predict(X_test) y_2 = regr_2 . predict(X_test) y_3 = regr_3 . predict(X_test) Plot plt . figure() s = 50 plt . scatter(y[:, 0 ], y[:, 1 ], c = \"navy\" , s = s, label = \"data\" ) plt . scatter(y_1[:, 0 ], y_1[:, 1 ], c = \"cornflowerblue\" , s = s, label = \"max_depth=2\" ) plt . scatter(y_2[:, 0 ], y_2[:, 1 ], c = \"c\" , s = s, label = \"max_depth=5\" ) plt . scatter(y_3[:, 0 ], y_3[:, 1 ], c = \"orange\" , s = s, label = \"max_depth=8\" ) plt . xlim([ - 6 , 6 ]) plt . ylim([ - 6 , 6 ]) plt . xlabel( \"target 1\" ) plt . ylabel( \"target 2\" ) plt . title( \"Multi-output Decision Tree Regression\" ) plt . legend() plt . show()","title":"4. Multi-output Decision Tree Regression (source)"},{"location":"DataStructure/ds/","text":"Fundamental Data Structure : List, Tuple and Dictionary List To initiate a blank List. l = [] To find the type of the object. type (l) list To create a list from scratch. L = [ 1 , 2 , 3 , 4 , 5 , 6 ] Indexing of list. L[ 0 ],L[ 1 ],L[ 5 ] (1, 2, 6) Revers indexing is also possible. L[ - 1 ],L[ - 2 ],L[ - 3 ] (6, 5, 4) To find the length of list. len (L) 6 To add the element from last. L . append( 12 ) L [1, 2, 3, 4, 5, 6, 12] To find the sum of the elements (if they are of same types like int. double etc) sum (L) 33 To find maximum and minimum of the list max (L), min (L) (12, 1) To create a list of heterogeneous element types. L = [ 1 , 2.0 , 3 , 4 , 5 , \"Apple\" ] To find the type of elements of a list. type (L[ 1 ]), type (L[ 5 ]) (float, str) To create a list of list. L = [[ 1 , 2 , 3 ],[ 3 , 4 , 5 ],[ 5 , 7 , 9 ]] To find list inside a list. L[ 0 ] [1, 2, 3] L[ 0 ][ 1 ] 2 To add list L1 = [ 1 , 2 , 3 ] ; L2 = [ 2 , 4 , 6 ] L1 + L2, set (L1 + L2) ([1, 2, 3, 2, 4, 6], {1, 2, 3, 4, 6}) To create array for algebraic operations import numpy as np L1 = np . array([ 1 , 2 , 3 ]); L2 = np . array([ 2 , 4 , 6 ]) L1 + L2 array([3, 6, 9]) To iterate over the element of list L = [i for i in range ( 10 )] L [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] To create a random List import random as random L = [random . uniform( 0 , 1 ) for k in range ( 10 )] L [0.43053490647858217, 0.4042754547551368, 0.4825961844418639, 0.07969436319497114, 0.5359042493237792, 0.521045536638314, 0.9657067090287339, 0.11515389590934644, 0.20310645682803774, 0.3875919385817722] To create a random list of lists LL = [[random . uniform( 0 , 1 ) for k in range ( 3 )] for l in range ( 3 )] LL [[0.27628847122005273, 0.8897450734835115, 0.18989122408874082], [0.13835865058469599, 0.7262672634594681, 0.10898554004253247], [0.42276719493338266, 0.11377775037902738, 0.4901096855852801]] To save the data to json file import json with open ( 'data/mylist.json' , 'w' ) as f1: json . dump(LL,f1) Tuples To define a tuples from scratch t = ( 2 , 3 , 4 , 5 ) Find type type (t) tuple Indexing t[ 1 ] 3 Dictionary To initiate a dictionary D = dict () DD = {} Create a dictionary from scratch D = { \"fruit\" : 'apple' , \"vegetable\" : 'carrot' , \"rice\" : 2.0 , 'milk' : 10 ,} What are keys? D . keys() dict_keys(['fruit', 'vegetable', 'rice', 'milk']) What are values? D . values() dict_values(['apple', 'carrot', 2.0, 10]) Indexing D[ 'fruit' ] 'apple' Iteration over key and values for key,value in D . items(): print (key,value) fruit apple vegetable carrot rice 2.0 milk 10 Update a dictionary D . update({ \"salt\" : 2.0 }) D {'fruit': 'apple', 'vegetable': 'carrot', 'rice': 2.0, 'milk': 10, 'salt': 2.0} Create a list form a Dictionary list (D) ['fruit', 'vegetable', 'rice', 'milk', 'salt'] Create a list of keys list (D . keys()) ['fruit', 'vegetable', 'rice', 'milk', 'salt'] Create a list of values list (D . values()) ['apple', 'carrot', 2.0, 10, 2.0] Create a random Dictionary DD = {} for item in [ 'A' , 'B' , 'C' , 'D' , 'E' ]: DD . update({item:[random . uniform( 0 , 1 ) for k in range ( 10 )]}) DD {'A': [0.8623300586958146, 0.9817282451404751, 0.918013419185538, 0.7163654763224003, 0.9605939306786828, 0.10535569850024595, 0.11017993829505879, 0.7967874445465515, 0.40100560974033395, 0.6683804538904957], 'B': [0.9108032733225849, 0.5126845596833859, 0.2889475226297349, 0.4361419616905007, 0.9162781988261498, 0.6417420997937421, 0.5703303219382578, 0.8317203028864074, 0.9987773067590386, 0.19901433153401582], 'C': [0.6877286885216957, 0.16565933820204293, 0.25063345210121424, 0.31595887595060124, 0.03522116131022823, 0.5286776181365936, 0.8154337189974739, 0.8202821745739262, 0.0672014040433101, 0.12327287509980445], 'D': [0.4836330819912691, 0.8546497284804153, 0.14752285825255218, 0.5918584543549938, 0.14518319590340412, 0.025762251428333438, 0.016788596008689316, 0.009725555304236244, 0.8177641188673302, 0.5450138847266498], 'E': [0.6456541452062622, 0.7662672636891902, 0.04445215914793821, 0.3159171150800496, 0.9400712936126994, 0.6085210458061509, 0.6029509689621034, 0.34555270993185316, 0.7452915466172698, 0.03229045002223074]} Save a dictionary to a json file import json with open ( 'data/mydic.json' , 'w' ) as f2: json . dump(DD,f2)","title":"DataStructure"},{"location":"DataStructure/ds/#fundamental-data-structure","text":"","title":"Fundamental Data Structure :"},{"location":"DataStructure/ds/#list-tuple-and-dictionary","text":"","title":"List, Tuple and Dictionary"},{"location":"DataStructure/ds/#list","text":"To initiate a blank List. l = [] To find the type of the object. type (l) list To create a list from scratch. L = [ 1 , 2 , 3 , 4 , 5 , 6 ] Indexing of list. L[ 0 ],L[ 1 ],L[ 5 ] (1, 2, 6) Revers indexing is also possible. L[ - 1 ],L[ - 2 ],L[ - 3 ] (6, 5, 4) To find the length of list. len (L) 6 To add the element from last. L . append( 12 ) L [1, 2, 3, 4, 5, 6, 12] To find the sum of the elements (if they are of same types like int. double etc) sum (L) 33 To find maximum and minimum of the list max (L), min (L) (12, 1) To create a list of heterogeneous element types. L = [ 1 , 2.0 , 3 , 4 , 5 , \"Apple\" ] To find the type of elements of a list. type (L[ 1 ]), type (L[ 5 ]) (float, str) To create a list of list. L = [[ 1 , 2 , 3 ],[ 3 , 4 , 5 ],[ 5 , 7 , 9 ]] To find list inside a list. L[ 0 ] [1, 2, 3] L[ 0 ][ 1 ] 2 To add list L1 = [ 1 , 2 , 3 ] ; L2 = [ 2 , 4 , 6 ] L1 + L2, set (L1 + L2) ([1, 2, 3, 2, 4, 6], {1, 2, 3, 4, 6}) To create array for algebraic operations import numpy as np L1 = np . array([ 1 , 2 , 3 ]); L2 = np . array([ 2 , 4 , 6 ]) L1 + L2 array([3, 6, 9]) To iterate over the element of list L = [i for i in range ( 10 )] L [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] To create a random List import random as random L = [random . uniform( 0 , 1 ) for k in range ( 10 )] L [0.43053490647858217, 0.4042754547551368, 0.4825961844418639, 0.07969436319497114, 0.5359042493237792, 0.521045536638314, 0.9657067090287339, 0.11515389590934644, 0.20310645682803774, 0.3875919385817722] To create a random list of lists LL = [[random . uniform( 0 , 1 ) for k in range ( 3 )] for l in range ( 3 )] LL [[0.27628847122005273, 0.8897450734835115, 0.18989122408874082], [0.13835865058469599, 0.7262672634594681, 0.10898554004253247], [0.42276719493338266, 0.11377775037902738, 0.4901096855852801]] To save the data to json file import json with open ( 'data/mylist.json' , 'w' ) as f1: json . dump(LL,f1)","title":"List"},{"location":"DataStructure/ds/#tuples","text":"To define a tuples from scratch t = ( 2 , 3 , 4 , 5 ) Find type type (t) tuple Indexing t[ 1 ] 3","title":"Tuples"},{"location":"DataStructure/ds/#dictionary","text":"To initiate a dictionary D = dict () DD = {} Create a dictionary from scratch D = { \"fruit\" : 'apple' , \"vegetable\" : 'carrot' , \"rice\" : 2.0 , 'milk' : 10 ,} What are keys? D . keys() dict_keys(['fruit', 'vegetable', 'rice', 'milk']) What are values? D . values() dict_values(['apple', 'carrot', 2.0, 10]) Indexing D[ 'fruit' ] 'apple' Iteration over key and values for key,value in D . items(): print (key,value) fruit apple vegetable carrot rice 2.0 milk 10 Update a dictionary D . update({ \"salt\" : 2.0 }) D {'fruit': 'apple', 'vegetable': 'carrot', 'rice': 2.0, 'milk': 10, 'salt': 2.0} Create a list form a Dictionary list (D) ['fruit', 'vegetable', 'rice', 'milk', 'salt'] Create a list of keys list (D . keys()) ['fruit', 'vegetable', 'rice', 'milk', 'salt'] Create a list of values list (D . values()) ['apple', 'carrot', 2.0, 10, 2.0] Create a random Dictionary DD = {} for item in [ 'A' , 'B' , 'C' , 'D' , 'E' ]: DD . update({item:[random . uniform( 0 , 1 ) for k in range ( 10 )]}) DD {'A': [0.8623300586958146, 0.9817282451404751, 0.918013419185538, 0.7163654763224003, 0.9605939306786828, 0.10535569850024595, 0.11017993829505879, 0.7967874445465515, 0.40100560974033395, 0.6683804538904957], 'B': [0.9108032733225849, 0.5126845596833859, 0.2889475226297349, 0.4361419616905007, 0.9162781988261498, 0.6417420997937421, 0.5703303219382578, 0.8317203028864074, 0.9987773067590386, 0.19901433153401582], 'C': [0.6877286885216957, 0.16565933820204293, 0.25063345210121424, 0.31595887595060124, 0.03522116131022823, 0.5286776181365936, 0.8154337189974739, 0.8202821745739262, 0.0672014040433101, 0.12327287509980445], 'D': [0.4836330819912691, 0.8546497284804153, 0.14752285825255218, 0.5918584543549938, 0.14518319590340412, 0.025762251428333438, 0.016788596008689316, 0.009725555304236244, 0.8177641188673302, 0.5450138847266498], 'E': [0.6456541452062622, 0.7662672636891902, 0.04445215914793821, 0.3159171150800496, 0.9400712936126994, 0.6085210458061509, 0.6029509689621034, 0.34555270993185316, 0.7452915466172698, 0.03229045002223074]} Save a dictionary to a json file import json with open ( 'data/mydic.json' , 'w' ) as f2: json . dump(DD,f2)","title":"Dictionary"},{"location":"FunctionClass/fnc/","text":"Functions and Classes import numpy as np Class Circle Define a function which will take radious as input and provides area as output for a circle. def area (r): A = np . pi * r ** 2 return A Claculate the area of a sample circle of radius 10. area( 10 ) 314.1592653589793 Define a function which will take radious as input and provides circumference as output for a circle. def circumference (r): C = 2 * np . pi * r return C Claculate the circumference of a sample circle of radius 10. circumference( 10 ) 62.83185307179586 Lets build a class implementing above constants and functions class Circle (): def __init__ ( self , r): self . r = r def area ( self ): A = np . pi * self . r ** 2 return A def circumference ( self ): C = 2 * np . pi * self . r return C Test using examples Circle( 5 ) . area() 78.53981633974483 CC = Circle( 5 ) CC . area(),CC . circumference() (78.53981633974483, 31.41592653589793) CC . r 5 for r in [ 2 , 3 , 6 , 24 , 25 , 46 , 567 ]: CC = Circle(r) print ( \"radius: \" , r,\\ \"area : \" , CC . area(),\\ \"circumf : \" , CC . circumference()) radius: 2 area : 12.566370614359172 circumf : 12.566370614359172 radius: 3 area : 28.274333882308138 circumf : 18.84955592153876 radius: 6 area : 113.09733552923255 circumf : 37.69911184307752 radius: 24 area : 1809.5573684677208 circumf : 150.79644737231007 radius: 25 area : 1963.4954084936207 circumf : 157.07963267948966 radius: 46 area : 6647.610054996002 circumf : 289.02652413026095 radius: 567 area : 1009987.480609929 circumf : 3562.5660691708254 Class Gravity def gravity (m1,m2,d): F = (m1 * m2) / d ** 2 return F gravity( 5 , 4 , 10 ) 0.2 Lets create a class for Gravity calculation class Newton (): def __init__ ( self ,value_of_G, value_of_g, supplied_info): self . G = value_of_G self . info = supplied_info self . g = value_of_g def gravity ( self ,m1,m2,d): F = self . G * (m1 * m2) / d ** 2 print ( self . info) return F def gravity_pot ( self ,m1): F = m1 * self . g return F N1 = Newton(value_of_G = 6.7 , value_of_g = 9.8 ,\\ supplied_info = \"great job\" ) N1 . G, N1 . g,N1 . gravity( 2 , 3 , 13 ),N1 . gravity_pot( 12 ) great job (6.7, 9.8, 0.2378698224852071, 117.60000000000001) N1 . gravity(m1 = 11 ,m2 = 12 ,d = 3 ) great job 98.26666666666667","title":"Functions and Class"},{"location":"FunctionClass/fnc/#functions-and-classes","text":"import numpy as np","title":"Functions and Classes"},{"location":"FunctionClass/fnc/#class-circle","text":"Define a function which will take radious as input and provides area as output for a circle. def area (r): A = np . pi * r ** 2 return A Claculate the area of a sample circle of radius 10. area( 10 ) 314.1592653589793 Define a function which will take radious as input and provides circumference as output for a circle. def circumference (r): C = 2 * np . pi * r return C Claculate the circumference of a sample circle of radius 10. circumference( 10 ) 62.83185307179586 Lets build a class implementing above constants and functions class Circle (): def __init__ ( self , r): self . r = r def area ( self ): A = np . pi * self . r ** 2 return A def circumference ( self ): C = 2 * np . pi * self . r return C Test using examples Circle( 5 ) . area() 78.53981633974483 CC = Circle( 5 ) CC . area(),CC . circumference() (78.53981633974483, 31.41592653589793) CC . r 5 for r in [ 2 , 3 , 6 , 24 , 25 , 46 , 567 ]: CC = Circle(r) print ( \"radius: \" , r,\\ \"area : \" , CC . area(),\\ \"circumf : \" , CC . circumference()) radius: 2 area : 12.566370614359172 circumf : 12.566370614359172 radius: 3 area : 28.274333882308138 circumf : 18.84955592153876 radius: 6 area : 113.09733552923255 circumf : 37.69911184307752 radius: 24 area : 1809.5573684677208 circumf : 150.79644737231007 radius: 25 area : 1963.4954084936207 circumf : 157.07963267948966 radius: 46 area : 6647.610054996002 circumf : 289.02652413026095 radius: 567 area : 1009987.480609929 circumf : 3562.5660691708254","title":"Class Circle"},{"location":"FunctionClass/fnc/#class-gravity","text":"def gravity (m1,m2,d): F = (m1 * m2) / d ** 2 return F gravity( 5 , 4 , 10 ) 0.2 Lets create a class for Gravity calculation class Newton (): def __init__ ( self ,value_of_G, value_of_g, supplied_info): self . G = value_of_G self . info = supplied_info self . g = value_of_g def gravity ( self ,m1,m2,d): F = self . G * (m1 * m2) / d ** 2 print ( self . info) return F def gravity_pot ( self ,m1): F = m1 * self . g return F N1 = Newton(value_of_G = 6.7 , value_of_g = 9.8 ,\\ supplied_info = \"great job\" ) N1 . G, N1 . g,N1 . gravity( 2 , 3 , 13 ),N1 . gravity_pot( 12 ) great job (6.7, 9.8, 0.2378698224852071, 117.60000000000001) N1 . gravity(m1 = 11 ,m2 = 12 ,d = 3 ) great job 98.26666666666667","title":"Class Gravity"},{"location":"GettingStarted/anaconda/","text":"Installing Python To install Anaconda Python follow the instruction at Anaconda Distribution Website . Based on the operating system select the proper version of the Anaconda package and install it in your PC. After you successfully install the proper version, you will get anaconda application in you PC which will look like the figure below: Best way to start with is the \"Jupyter notebook\". Lunch the jupyter notebook to start with Python. Note- Linux: For Linux user, it could be little bit tricky. SOme time it becomes hard to locate anaconda path to the environment so you need to point the python you want to use. Please, run the command below to point the python: bash export PATH=/home/ubuntu/anaconda3/bin:$PATH There is 'base' or 'anaconda3' environment by defult. You can find the list of available environmet by typing following command on the terminal bash conda env list To start the 'base' environment type bash source activate base To install new package for example 'jupyter notebook' type bash pip install jupyter notebook After sucessfully installing Jupyter notebook, tye following to start it bash Jupyter notebook Note - Cloud For running Jupyter notebook in AWS cloud, it is important to open the \"8888\" to \"8889\" with TCP rule with IP \"0.0.0.0\" and allow to be opend from anywhere. Once port is open, type following to bash jupyter notebook --ip=0.0.0.0 --no-browser","title":"Guide to starting Python"},{"location":"GettingStarted/anaconda/#installing-python","text":"To install Anaconda Python follow the instruction at Anaconda Distribution Website . Based on the operating system select the proper version of the Anaconda package and install it in your PC. After you successfully install the proper version, you will get anaconda application in you PC which will look like the figure below: Best way to start with is the \"Jupyter notebook\". Lunch the jupyter notebook to start with Python.","title":"Installing Python"},{"location":"GettingStarted/anaconda/#note-linux","text":"For Linux user, it could be little bit tricky. SOme time it becomes hard to locate anaconda path to the environment so you need to point the python you want to use. Please, run the command below to point the python: bash export PATH=/home/ubuntu/anaconda3/bin:$PATH There is 'base' or 'anaconda3' environment by defult. You can find the list of available environmet by typing following command on the terminal bash conda env list To start the 'base' environment type bash source activate base To install new package for example 'jupyter notebook' type bash pip install jupyter notebook After sucessfully installing Jupyter notebook, tye following to start it bash Jupyter notebook","title":"Note- Linux:"},{"location":"GettingStarted/anaconda/#note-cloud","text":"For running Jupyter notebook in AWS cloud, it is important to open the \"8888\" to \"8889\" with TCP rule with IP \"0.0.0.0\" and allow to be opend from anywhere. Once port is open, type following to bash jupyter notebook --ip=0.0.0.0 --no-browser","title":"Note - Cloud"},{"location":"GettingStarted/env/","text":"Python Environment Basics To avoid errors later, it's best to update all the packages in the default environment. Open the Anaconda Prompt application. In the prompt, run the following commands: conda upgrade conda conda upgrade --all If you are seeing the following \"conda command not found\" and are using ZShell, you have to do the following: export PATH = \"/Users/username/anaconda/bin: $PATH \" or update above command line to your .zsh_config file. Once you have Anaconda installed, managing packages is fairly straightforward. To install a package, type conda install package_name in your terminal. For example, to install numpy, type conda install numpy. You can install multiple packages at the same time. Something like conda install numpy scipy pandas will install all those packages simultaneously. It's also possible to specify which version of a package you want by adding the version number such as conda install numpy = 1 .10. Conda also automatically installs dependencies for you. For example scipy depends on numpy, it uses and requires numpy. If you install just scipy (conda install scipy), Conda will also install numpy if it isn't already installed. Most of the commands are pretty intuitive. To uninstall, use conda remove package_name To update a package conda update package_name If you want to update all packages in an environment, which is often useful, use conda update --all And finally, to list installed packages, it's conda list If you don't know the exact name of the package you're looking for, you can try searching with conda search search_term For example, I know I want to install Beautiful Soup, but I'm not sure of the exact package name. So, I try conda search beautifulsoup Environments Conda can be used to create environments to isolate your projects. To create an environment, use conda create -n env_name list of packages in your terminal Here -n env_name sets the name of your environment (-n for name) and list of packages is the list of packages you want installed in the environment. For example, to create an environment named my_env and install numpy in it, type conda create -n my_env numpy When creating an environment, you can specify which version of Python to install in the environment. This is useful when you're working with code in both Python 2.x and Python 3.x. To create an environment with a specific Python version, do something like conda create -n py3 python = 3 or conda create -n py2 python = 2 I actually have both of these environments on my personal computer. I use them as general environments not tied to any specific project, but rather for general work with each Python version easily accessible. These commands will install the most recent version of Python 3 and 2, respectively. To install a specific version, use conda create -n py python = 3 .3 for Python 3.3. Once you have an environment created, use source activate my_env to enter it on OSX/Linux. On Windows, use activate my_env When you're in the environment, you'll see the environment name in the terminal prompt. Something like (my_env) ~ $. The environment has only a few packages installed by default, plus the ones you installed when creating it. You can check this out with conda list. Installing packages in the environment is the same as before: conda install package_name Only this time, the specific packages you install will only be available when you're in the environment. To leave the environment, type source deactivate ( on OSX/Linux ) On Windows, use deactivate Saving and loading environments A really useful feature is sharing environments so others can install all the packages used in your code, with the correct versions. You can save the packages to a YAML file with conda env export > environment.yaml The first part conda env export writes out all the packages in the environment, including the Python version. Above you can see the name of the environment and all the dependencies (along with versions) are listed. The second part of the export command, > environment.yaml writes the exported text to a YAML file environment.yaml . This file can now be shared and others will be able to create the same environment you used for the project. To create an environment from an environment file use conda env create -f environment.yaml This will create a new environment with the same name listed in environment.yaml . Listing environments If you forget what your environments are named (happens to me sometimes), use conda env list to list out all the environments you've created. You should see a list of environments, there will be an asterisk next to the environment you're currently in. The default environment, the environment used when you aren't in one, is called root . Removing environments If there are environments you don't use anymore, conda env remove -n env_name will remove the specified environment (here, named env_name ). Using environments One thing that\u2019s helped me tremendously is having separate environments for Python 2 and Python 3. I used conda create -n py2 python = 2 and conda create -n py3 python = 3 to create two separate environments, py2 and py3 . Now I have a general use environment for each Python version. In each of those environments, I've installed most of the standard data science packages (numpy, scipy, pandas, etc.) I\u2019ve also found it useful to create environments for each project I\u2019m working on. It works great for non-data related projects too like web apps with Flask. For example, I have an environment for my personal blog using Pelican . Sharing environments When sharing your code on GitHub, it's good practice to make an environment file and include it in the repository. This will make it easier for people to install all the dependencies for your code. I also usually include a pip requirements.txt file using pip freeze ( learn more here ) for people not using conda. More to learn To learn more about conda and how it fits in the Python ecosystem, check out this article by Jake Vanderplas: Conda myths and misconceptions. And here's the conda documentation you can reference later.","title":"Setting up Python Environment"},{"location":"GettingStarted/env/#python-environment","text":"","title":"Python Environment"},{"location":"GettingStarted/env/#basics","text":"To avoid errors later, it's best to update all the packages in the default environment. Open the Anaconda Prompt application. In the prompt, run the following commands: conda upgrade conda conda upgrade --all If you are seeing the following \"conda command not found\" and are using ZShell, you have to do the following: export PATH = \"/Users/username/anaconda/bin: $PATH \" or update above command line to your .zsh_config file. Once you have Anaconda installed, managing packages is fairly straightforward. To install a package, type conda install package_name in your terminal. For example, to install numpy, type conda install numpy. You can install multiple packages at the same time. Something like conda install numpy scipy pandas will install all those packages simultaneously. It's also possible to specify which version of a package you want by adding the version number such as conda install numpy = 1 .10. Conda also automatically installs dependencies for you. For example scipy depends on numpy, it uses and requires numpy. If you install just scipy (conda install scipy), Conda will also install numpy if it isn't already installed. Most of the commands are pretty intuitive. To uninstall, use conda remove package_name To update a package conda update package_name If you want to update all packages in an environment, which is often useful, use conda update --all And finally, to list installed packages, it's conda list If you don't know the exact name of the package you're looking for, you can try searching with conda search search_term For example, I know I want to install Beautiful Soup, but I'm not sure of the exact package name. So, I try conda search beautifulsoup","title":"Basics"},{"location":"GettingStarted/env/#environments","text":"Conda can be used to create environments to isolate your projects. To create an environment, use conda create -n env_name list of packages in your terminal Here -n env_name sets the name of your environment (-n for name) and list of packages is the list of packages you want installed in the environment. For example, to create an environment named my_env and install numpy in it, type conda create -n my_env numpy When creating an environment, you can specify which version of Python to install in the environment. This is useful when you're working with code in both Python 2.x and Python 3.x. To create an environment with a specific Python version, do something like conda create -n py3 python = 3 or conda create -n py2 python = 2 I actually have both of these environments on my personal computer. I use them as general environments not tied to any specific project, but rather for general work with each Python version easily accessible. These commands will install the most recent version of Python 3 and 2, respectively. To install a specific version, use conda create -n py python = 3 .3 for Python 3.3. Once you have an environment created, use source activate my_env to enter it on OSX/Linux. On Windows, use activate my_env When you're in the environment, you'll see the environment name in the terminal prompt. Something like (my_env) ~ $. The environment has only a few packages installed by default, plus the ones you installed when creating it. You can check this out with conda list. Installing packages in the environment is the same as before: conda install package_name Only this time, the specific packages you install will only be available when you're in the environment. To leave the environment, type source deactivate ( on OSX/Linux ) On Windows, use deactivate","title":"Environments"},{"location":"GettingStarted/env/#saving-and-loading-environments","text":"A really useful feature is sharing environments so others can install all the packages used in your code, with the correct versions. You can save the packages to a YAML file with conda env export > environment.yaml The first part conda env export writes out all the packages in the environment, including the Python version. Above you can see the name of the environment and all the dependencies (along with versions) are listed. The second part of the export command, > environment.yaml writes the exported text to a YAML file environment.yaml . This file can now be shared and others will be able to create the same environment you used for the project. To create an environment from an environment file use conda env create -f environment.yaml This will create a new environment with the same name listed in environment.yaml .","title":"Saving and loading environments"},{"location":"GettingStarted/env/#listing-environments","text":"If you forget what your environments are named (happens to me sometimes), use conda env list to list out all the environments you've created. You should see a list of environments, there will be an asterisk next to the environment you're currently in. The default environment, the environment used when you aren't in one, is called root .","title":"Listing environments"},{"location":"GettingStarted/env/#removing-environments","text":"If there are environments you don't use anymore, conda env remove -n env_name will remove the specified environment (here, named env_name ).","title":"Removing environments"},{"location":"GettingStarted/env/#using-environments","text":"One thing that\u2019s helped me tremendously is having separate environments for Python 2 and Python 3. I used conda create -n py2 python = 2 and conda create -n py3 python = 3 to create two separate environments, py2 and py3 . Now I have a general use environment for each Python version. In each of those environments, I've installed most of the standard data science packages (numpy, scipy, pandas, etc.) I\u2019ve also found it useful to create environments for each project I\u2019m working on. It works great for non-data related projects too like web apps with Flask. For example, I have an environment for my personal blog using Pelican .","title":"Using environments"},{"location":"GettingStarted/env/#sharing-environments","text":"When sharing your code on GitHub, it's good practice to make an environment file and include it in the repository. This will make it easier for people to install all the dependencies for your code. I also usually include a pip requirements.txt file using pip freeze ( learn more here ) for people not using conda.","title":"Sharing environments"},{"location":"GettingStarted/env/#more-to-learn","text":"To learn more about conda and how it fits in the Python ecosystem, check out this article by Jake Vanderplas: Conda myths and misconceptions. And here's the conda documentation you can reference later.","title":"More to learn"},{"location":"GettingStarted/git/","text":"How to git Reference : How to Git Create a new repository on GitHub. To avoid errors, do not initialize the new repository with README, license, or gitignore files. You can add these files after your project has been pushed to GitHub. Open Terminal. Change the current working directory to your local project. Initialize the local directory as a Git repository. git init Add the files in your new local repository. This stages them for the first commit. git add . Adds the files in the local repository and stages them for commit. To unstage a file, use 'git reset HEAD YOUR-FILE'. Commit the files that you've staged in your local repository. git commit -m \"First commit\" Commits the tracked changes and prepares them to be pushed to a remote repository. To remove this commit and modify the file, use 'git reset --soft HEAD~1' and commit and add the file again. Copy remote repository URL fieldAt the top of your GitHub repository's Quick Setup page, click to copy the remote repository URL. In Terminal, add the URL for the remote repository where your local repository will be pushed. git remote add origin remote repository URL Sets the new remote git remote -v Verifies the new remote URL Push the changes in your local repository to GitHub. git push origin master Pushes the changes in your local repository up to the remote repository you specified as the origin","title":"How to Git"},{"location":"GettingStarted/git/#how-to-git","text":"Reference : How to Git Create a new repository on GitHub. To avoid errors, do not initialize the new repository with README, license, or gitignore files. You can add these files after your project has been pushed to GitHub. Open Terminal. Change the current working directory to your local project. Initialize the local directory as a Git repository. git init Add the files in your new local repository. This stages them for the first commit. git add . Adds the files in the local repository and stages them for commit. To unstage a file, use 'git reset HEAD YOUR-FILE'. Commit the files that you've staged in your local repository. git commit -m \"First commit\" Commits the tracked changes and prepares them to be pushed to a remote repository. To remove this commit and modify the file, use 'git reset --soft HEAD~1' and commit and add the file again. Copy remote repository URL fieldAt the top of your GitHub repository's Quick Setup page, click to copy the remote repository URL. In Terminal, add the URL for the remote repository where your local repository will be pushed. git remote add origin remote repository URL Sets the new remote git remote -v Verifies the new remote URL Push the changes in your local repository to GitHub. git push origin master Pushes the changes in your local repository up to the remote repository you specified as the origin","title":"How to git"},{"location":"GettingStarted/jupyter/","text":"Installing Jupyter Notebook By far the easiest way to install Jupyter is with Anaconda. Jupyter notebooks automatically come with the distribution. You'll be able to use notebooks from the default environment. To install Jupyter notebooks in a conda environment, use conda install jupyter notebook Jupyter notebooks are also available through pip with pip install jupyter notebook Markdown Cheatsheet : https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet Convert a notebook to an HTML file, in your terminal use jupyter nbconvert --to html notebook.ipynb Convert: https://nbconvert.readthedocs.io/en/latest/usage.html To create the slideshow from the notebook file, you'll need to use nbconvert: jupyter nbconvert notebook.ipynb --to slides This just converts the notebook to the necessary files for the slideshow, but you need to serve it with an HTTP server to actually see the presentation. To convert it and immediately see it, use jupyter nbconvert notebook.ipynb --to slides --post serve This will open up the slideshow in your browser so you can present it. panda presentation: presentation","title":"Installing Jupyter Notebook"},{"location":"GettingStarted/jupyter/#installing-jupyter-notebook","text":"By far the easiest way to install Jupyter is with Anaconda. Jupyter notebooks automatically come with the distribution. You'll be able to use notebooks from the default environment. To install Jupyter notebooks in a conda environment, use conda install jupyter notebook Jupyter notebooks are also available through pip with pip install jupyter notebook Markdown Cheatsheet : https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet Convert a notebook to an HTML file, in your terminal use jupyter nbconvert --to html notebook.ipynb Convert: https://nbconvert.readthedocs.io/en/latest/usage.html To create the slideshow from the notebook file, you'll need to use nbconvert: jupyter nbconvert notebook.ipynb --to slides This just converts the notebook to the necessary files for the slideshow, but you need to serve it with an HTTP server to actually see the presentation. To convert it and immediately see it, use jupyter nbconvert notebook.ipynb --to slides --post serve This will open up the slideshow in your browser so you can present it. panda presentation: presentation","title":"Installing Jupyter Notebook"},{"location":"GettingStarted/lib/","text":"Python Libraries Following are the best Python Libraries: TensorFlow Scikit-Learn Numpy Keras PyTorch LightGBM Eli5 SciPy Theano Pandas","title":"Guide to Python Libraries"},{"location":"GettingStarted/lib/#python-libraries","text":"Following are the best Python Libraries: TensorFlow Scikit-Learn Numpy Keras PyTorch LightGBM Eli5 SciPy Theano Pandas","title":"Python Libraries"},{"location":"GettingStarted/note/","text":"Why Codes?","title":"Note"},{"location":"GettingStarted/note/#why-codes","text":"","title":"Why Codes?"},{"location":"Index/","text":"Index Why Codes? Getting started Guide to starting Python Guide to Python Libraries Fundamentals of Programming DataStructure Loops and Conditions Project Fern Project Diffusion Functions and Class Project N-charges Project Random Walk Numpy Array MeshGrid Algebra Statistics Pandas Dataframe Indexing Data Exploration GroupBy Lambda Transform","title":"Index"},{"location":"Index/#index","text":"Why Codes? Getting started Guide to starting Python Guide to Python Libraries Fundamentals of Programming DataStructure Loops and Conditions Project Fern Project Diffusion Functions and Class Project N-charges Project Random Walk Numpy Array MeshGrid Algebra Statistics Pandas Dataframe Indexing Data Exploration GroupBy Lambda Transform","title":"Index"},{"location":"LinearModels/Linear-models/","text":"Linear model Introduction Examples 1. Linear Regression Example ( source ) This example uses the only the first feature of the diabetes dataset, in order to illustrate a two-dimensional plot of this regression technique. The straight line can be seen in the plot, showing how linear regression attempts to draw a straight line that will best minimize the residual sum of squares between the observed responses in the dataset, and the responses predicted by the linear approximation. The coefficients, the residual sum of squares and the variance score are also calculated. import matplotlib.pyplot as plt import numpy as np from sklearn import datasets, linear_model # Load the diabetes dataset diabetes = datasets . load_diabetes() # Use only one feature diabetes_X = diabetes . data[:, np . newaxis, 2 ] # Split the data into training/testing sets diabetes_X_train = diabetes_X[: - 20 ] diabetes_X_test = diabetes_X[ - 20 :] # Split the targets into training/testing sets diabetes_y_train = diabetes . target[: - 20 ] diabetes_y_test = diabetes . target[ - 20 :] # Create linear regression object regr = linear_model . LinearRegression() # Train the model using the training sets regr . fit(diabetes_X_train, diabetes_y_train) # The coefficients print ( 'Coefficients: \\n ' , regr . coef_) # The mean squared error print ( \"Mean squared error: %.2f \" % np . mean((regr . predict(diabetes_X_test) - diabetes_y_test) ** 2 )) # Explained variance score: 1 is perfect prediction print ( 'Variance score: %.2f ' % regr . score(diabetes_X_test, diabetes_y_test)) # Plot outputs plt . scatter(diabetes_X_test, diabetes_y_test, color = 'black' ) plt . plot(diabetes_X_test, regr . predict(diabetes_X_test), color = 'blue' , linewidth = 3 ) plt . xticks(()) plt . yticks(()) plt . show() Coefficients: [ 938.23786125] Mean squared error: 2548.07 Variance score: 0.47 2. Sparsity Example: Fitting only features 1 and 2 ( source ) Features 1 and 2 of the diabetes-dataset are fitted and plotted below. It illustrates that although feature 2 has a strong coefficient on the full model, it does not give us much regarding y when compared to just feature 1 import matplotlib.pyplot as plt import numpy as np from mpl_toolkits.mplot3d import Axes3D from sklearn import datasets, linear_model diabetes = datasets . load_diabetes() indices = ( 0 , 1 ) X_train = diabetes . data[: - 20 , indices] X_test = diabetes . data[ - 20 :, indices] y_train = diabetes . target[: - 20 ] y_test = diabetes . target[ - 20 :] ols = linear_model . LinearRegression() ols . fit(X_train, y_train) ##################################### # Plot the figure def plot_figs (fig_num, elev, azim, X_train, clf): fig = plt . figure(fig_num, figsize = ( 4 , 3 )) plt . clf() ax = Axes3D(fig, elev = elev, azim = azim) ax . scatter(X_train[:, 0 ], X_train[:, 1 ], y_train, c = 'k' , marker = '+' ) ax . plot_surface(np . array([[ -. 1 , -. 1 ], [ . 15 , . 15 ]]), np . array([[ -. 1 , . 15 ], [ -. 1 , . 15 ]]), clf . predict(np . array([[ -. 1 , -. 1 , . 15 , . 15 ], [ -. 1 , . 15 , -. 1 , . 15 ]]) . T ) . reshape(( 2 , 2 )), alpha =. 5 ) ax . set_xlabel( 'X_1' ) ax . set_ylabel( 'X_2' ) ax . set_zlabel( 'Y' ) ax . w_xaxis . set_ticklabels([]) ax . w_yaxis . set_ticklabels([]) ax . w_zaxis . set_ticklabels([]) #Generate the three different figures from different views elev = 43.5 azim = - 110 plot_figs( 1 , elev, azim, X_train, ols) elev = -. 5 azim = 0 plot_figs( 2 , elev, azim, X_train, ols) elev = -. 5 azim = 90 plot_figs( 3 , elev, azim, X_train, ols) plt . show() 3. Ordinary Least Squares and Ridge Regression Variance ( source ) Due to the few points in each dimension and the straight line that linear regression uses to follow these points as well as it can, noise on the observations will cause great variance as shown in the first plot. Every line's slope can vary quite a bit for each prediction due to the noise induced in the observations. Ridge regression is basically minimizing a penalised version of the least-squared function. The penalising shrinks the value of the regression coefficients. Despite the few data points in each dimension, the slope of the prediction is much more stable and the variance in the line itself is greatly reduced, in comparison to that of the standard linear regression import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model X_train = np . c_[ . 5 , 1 ] . T y_train = [ . 5 , 1 ] X_test = np . c_[ 0 , 2 ] . T np . random . seed( 0 ) classifiers = dict (ols = linear_model . LinearRegression(), ridge = linear_model . Ridge(alpha =. 1 )) fignum = 1 for name, clf in classifiers . items(): fig = plt . figure(fignum, figsize = ( 4 , 3 )) plt . clf() plt . title(name) ax = plt . axes([ . 12 , . 12 , . 8 , . 8 ]) for _ in range ( 6 ): this_X = . 1 * np . random . normal(size = ( 2 , 1 )) + X_train clf . fit(this_X, y_train) ax . plot(X_test, clf . predict(X_test), color = '.5' ) ax . scatter(this_X, y_train, s = 3 , c = '.5' , marker = 'o' , zorder = 10 ) clf . fit(X_train, y_train) ax . plot(X_test, clf . predict(X_test), linewidth = 2 , color = 'blue' ) ax . scatter(X_train, y_train, s = 30 , c = 'r' , marker = '+' , zorder = 10 ) ax . set_xticks(()) ax . set_yticks(()) ax . set_ylim(( 0 , 1.6 )) ax . set_xlabel( 'X' ) ax . set_ylabel( 'y' ) ax . set_xlim( 0 , 2 ) fignum += 1 plt . show() 4. Plot Ridge coefficients as a function of the L2 regularization ( source ) Ridge Regression is the estimator used in this example. Each color in the left plot represents one different dimension of the coefficient vector, and this is displayed as a function of the regularization parameter. The right plot shows how exact the solution is. This example illustrates how a well defined solution is found by Ridge regression and how regularization affects the coefficients and their values. The plot on the right shows how the difference of the coefficients from the estimator changes as a function of regularization. In this example the dependent variable Y is set as a function of the input features: $y = X*w + c. $ The coefficient vector w is randomly sampled from a normal distribution, whereas the bias term c is set to a constant. As alpha tends toward zero the coefficients found by Ridge regression stabilize towards the randomly sampled vector w. For big alpha (strong regularisation) the coefficients are smaller (eventually converging at 0) leading to a simpler and biased solution. These dependencies can be observed on the left plot. The right plot shows the mean squared error between the coefficients found by the model and the chosen vector w. Less regularised models retrieve the exact coefficients (error is equal to 0), stronger regularised models increase the error. Please note that in this example the data is non-noisy, hence it is possible to extract the exact coefficients. import matplotlib.pyplot as plt import numpy as np from sklearn.datasets import make_regression from sklearn.linear_model import Ridge from sklearn.metrics import mean_squared_error clf = Ridge() X, y, w = make_regression(n_samples = 10 , n_features = 10 , coef = True , random_state = 1 , bias = 3.5 ) coefs = [] errors = [] alphas = np . logspace( - 6 , 6 , 200 ) # Train the model with different regularisation strengths for a in alphas: clf . set_params(alpha = a) clf . fit(X, y) coefs . append(clf . coef_) errors . append(mean_squared_error(clf . coef_, w)) # Display results plt . figure(figsize = ( 20 , 6 )) plt . subplot( 121 ) ax = plt . gca() ax . plot(alphas, coefs) ax . set_xscale( 'log' ) plt . xlabel( 'alpha' ) plt . ylabel( 'weights' ) plt . title( 'Ridge coefficients as a function of the regularization' ) plt . axis( 'tight' ) plt . subplot( 122 ) ax = plt . gca() ax . plot(alphas, errors) ax . set_xscale( 'log' ) plt . xlabel( 'alpha' ) plt . ylabel( 'error' ) plt . title( 'Coefficient error as a function of the regularization' ) plt . axis( 'tight' ) plt . show() 5. Plot Ridge coefficients as a function of the regularization ( source ) Shows the effect of collinearity in the coefficients of an estimator. Regression is the estimator used in this example. Each color represents a different feature of the coefficient vector, and this is displayed as a function of the regularization parameter. This example also shows the usefulness of applying Ridge regression to highly ill-conditioned matrices. For such matrices, a slight change in the target variable can cause huge variances in the calculated weights. In such cases, it is useful to set a certain regularization (alpha) to reduce this variation (noise). When alpha is very large, the regularization effect dominates the squared loss function and the coefficients tend to zero. At the end of the path, as alpha tends toward zero and the solution tends towards the ordinary least squares, coefficients exhibit big oscillations. In practise it is necessary to tune alpha in such a way that a balance is maintained between both. import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model # X is the 10x10 Hilbert matrix X = 1. / (np . arange( 1 , 11 ) + np . arange( 0 , 10 )[:, np . newaxis]) y = np . ones( 10 ) ######################################### # Compute paths n_alphas = 200 alphas = np . logspace( - 10 , - 2 , n_alphas) clf = linear_model . Ridge(fit_intercept = False ) coefs = [] for a in alphas: clf . set_params(alpha = a) clf . fit(X, y) coefs . append(clf . coef_) ######################################## # Display results ax = plt . gca() ax . plot(alphas, coefs) ax . set_xscale( 'log' ) ax . set_xlim(ax . get_xlim()[:: - 1 ]) # reverse axis plt . xlabel( 'alpha' ) plt . ylabel( 'weights' ) plt . title( 'Ridge coefficients as a function of the regularization' ) plt . axis( 'tight' ) plt . show() 6. Bayesian Ridge Regression ( source ) Computes a Bayesian Ridge Regression on a synthetic dataset. bayesian_ridge_regression for more information on the regressor. Compared to the OLS (ordinary least squares) estimator, the coefficient weights are slightly shifted toward zeros, which stabilises them. As the prior on the weights is a Gaussian prior, the histogram of the estimated weights is Gaussian. The estimation of the model is done by iteratively maximizing the marginal log-likelihood of the observations. We also plot predictions and uncertainties for Bayesian Ridge Regression for one dimensional regression using polynomial feature expansion. Note the uncertainty starts going up on the right side of the plot. This is because these test samples are outside of the range of the training samples. import numpy as np import matplotlib.pyplot as plt from scipy import stats % matplotlib inline from sklearn.linear_model import BayesianRidge, LinearRegression ################################################# # Generating simulated data with Gaussian weights np . random . seed( 0 ) n_samples, n_features = 100 , 100 X = np . random . randn(n_samples, n_features) # Create Gaussian data # Create weights with a precision lambda_ of 4. lambda_ = 4. w = np . zeros(n_features) # Only keep 10 weights of interest relevant_features = np . random . randint( 0 , n_features, 10 ) for i in relevant_features: w[i] = stats . norm . rvs(loc = 0 , scale = 1. / np . sqrt(lambda_)) # Create noise with a precision alpha of 50. alpha_ = 50. noise = stats . norm . rvs(loc = 0 , scale = 1. / np . sqrt(alpha_), size = n_samples) # Create the target y = np . dot(X, w) + noise ############################################ # Fit the Bayesian Ridge Regression and an OLS for comparison clf = BayesianRidge(compute_score = True ) clf . fit(X, y) ols = LinearRegression() ols . fit(X, y) ################################################ # Plot true weights, estimated weights, histogram of the weights, and # predictions with standard deviations lw = 2 plt . figure(figsize = ( 6 , 5 )) plt . title( \"Weights of the model\" ) plt . plot(clf . coef_, color = 'lightgreen' , linewidth = lw, label = \"Bayesian Ridge estimate\" ) plt . plot(w, color = 'gold' , linewidth = lw, label = \"Ground truth\" ) plt . plot(ols . coef_, color = 'navy' , linestyle = '--' , label = \"OLS estimate\" ) plt . xlabel( \"Features\" ) plt . ylabel( \"Values of the weights\" ) plt . legend(loc = \"best\" , prop = dict (size = 12 )) plt . figure(figsize = ( 6 , 5 )) plt . title( \"Histogram of the weights\" ) plt . hist(clf . coef_, bins = n_features, color = 'gold' , log = True ) plt . scatter(clf . coef_[relevant_features], 5 * np . ones( len (relevant_features)), color = 'navy' , label = \"Relevant features\" ) plt . ylabel( \"Features\" ) plt . xlabel( \"Values of the weights\" ) plt . legend(loc = \"upper left\" ) plt . figure(figsize = ( 6 , 5 )) plt . title( \"Marginal log-likelihood\" ) plt . plot(clf . scores_, color = 'navy' , linewidth = lw) plt . ylabel( \"Score\" ) plt . xlabel( \"Iterations\" ) # Plotting some predictions for polynomial regression def f (x, noise_amount): y = np . sqrt(x) * np . sin(x) noise = np . random . normal( 0 , 1 , len (x)) return y + noise_amount * noise degree = 10 X = np . linspace( 0 , 10 , 100 ) y = f(X, noise_amount = 0.1 ) clf_poly = BayesianRidge() clf_poly . fit(np . vander(X, degree), y) X_plot = np . linspace( 0 , 11 , 25 ) y_plot = f(X_plot, noise_amount = 0 ) y_mean, y_std = clf_poly . predict(np . vander(X_plot, degree), return_std = True ) plt . figure(figsize = ( 6 , 5 )) plt . errorbar(X_plot, y_mean, y_std, color = 'navy' , label = \"Polynomial Bayesian Ridge Regression\" , linewidth = lw) plt . plot(X_plot, y_plot, color = 'gold' , linewidth = lw, label = \"Ground Truth\" ) plt . ylabel( \"Output y\" ) plt . xlabel( \"Feature X\" ) plt . legend(loc = \"lower left\" ) plt . show() 7. Logistic Regression 3-class Classifier ( source ) Show below is a logistic-regression classifiers decision boundaries on the iris <https://en.wikipedia.org/wiki/Iris_flower_data_set> _ dataset. The datapoints are colored according to their labels. import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model, datasets # import some data to play with iris = datasets . load_iris() X = iris . data[:, : 2 ] # we only take the first two features. Y = iris . target h = . 02 # step size in the mesh logreg = linear_model . LogisticRegression(C = 1e5 ) # we create an instance of Neighbours Classifier and fit the data. logreg . fit(X, Y) # Plot the decision boundary. For that, we will assign a color to each # point in the mesh [x_min, x_max]x[y_min, y_max]. x_min, x_max = X[:, 0 ] . min() - . 5 , X[:, 0 ] . max() + . 5 y_min, y_max = X[:, 1 ] . min() - . 5 , X[:, 1 ] . max() + . 5 xx, yy = np . meshgrid(np . arange(x_min, x_max, h), np . arange(y_min, y_max, h)) Z = logreg . predict(np . c_[xx . ravel(), yy . ravel()]) # Put the result into a color plot Z = Z . reshape(xx . shape) plt . figure( 1 , figsize = ( 4 , 3 )) plt . pcolormesh(xx, yy, Z, cmap = plt . cm . Paired) # Plot also the training points plt . scatter(X[:, 0 ], X[:, 1 ], c = Y, edgecolors = 'k' , cmap = plt . cm . Paired) plt . xlabel( 'Sepal length' ) plt . ylabel( 'Sepal width' ) plt . xlim(xx . min(), xx . max()) plt . ylim(yy . min(), yy . max()) plt . xticks(()) plt . yticks(()) plt . show() 8. L1 Penalty and Sparsity in Logistic Regression ( source ) Comparison of the sparsity (percentage of zero coefficients) of solutions when L1 and L2 penalty are used for different values of C. We can see that large values of C give more freedom to the model. Conversely, smaller values of C constrain the model more. In the L1 penalty case, this leads to sparser solutions. We classify 8x8 images of digits into two classes: 0-4 against 5-9. The visualization shows coefficients of the models for varying C. import numpy as np import matplotlib.pyplot as plt from sklearn.linear_model import LogisticRegression from sklearn import datasets from sklearn.preprocessing import StandardScaler digits = datasets . load_digits() X, y = digits . data, digits . target X = StandardScaler() . fit_transform(X) # classify small against large digits y = (y > 4 ) . astype(np . int) # Set regularization parameter for i, C in enumerate (( 100 , 1 , 0.01 )): # turn down tolerance for short training time clf_l1_LR = LogisticRegression(C = C, penalty = 'l1' , tol = 0.01 ) clf_l2_LR = LogisticRegression(C = C, penalty = 'l2' , tol = 0.01 ) clf_l1_LR . fit(X, y) clf_l2_LR . fit(X, y) coef_l1_LR = clf_l1_LR . coef_ . ravel() coef_l2_LR = clf_l2_LR . coef_ . ravel() # coef_l1_LR contains zeros due to the # L1 sparsity inducing norm sparsity_l1_LR = np . mean(coef_l1_LR == 0 ) * 100 sparsity_l2_LR = np . mean(coef_l2_LR == 0 ) * 100 print ( \"C= %.2f \" % C) print ( \"Sparsity with L1 penalty: %.2f%% \" % sparsity_l1_LR) print ( \"score with L1 penalty: %.4f \" % clf_l1_LR . score(X, y)) print ( \"Sparsity with L2 penalty: %.2f%% \" % sparsity_l2_LR) print ( \"score with L2 penalty: %.4f \" % clf_l2_LR . score(X, y)) l1_plot = plt . subplot( 3 , 2 , 2 * i + 1 ) l2_plot = plt . subplot( 3 , 2 , 2 * (i + 1 )) if i == 0 : l1_plot . set_title( \"L1 penalty\" ) l2_plot . set_title( \"L2 penalty\" ) l1_plot . imshow(np . abs(coef_l1_LR . reshape( 8 , 8 )), interpolation = 'nearest' , cmap = 'binary' , vmax = 1 , vmin = 0 ) l2_plot . imshow(np . abs(coef_l2_LR . reshape( 8 , 8 )), interpolation = 'nearest' , cmap = 'binary' , vmax = 1 , vmin = 0 ) plt . text( - 8 , 3 , \"C = %.2f \" % C) l1_plot . set_xticks(()) l1_plot . set_yticks(()) l2_plot . set_xticks(()) l2_plot . set_yticks(()) plt . show() C=100.00 Sparsity with L1 penalty: 4.69% score with L1 penalty: 0.9093 Sparsity with L2 penalty: 4.69% score with L2 penalty: 0.9098 C=1.00 Sparsity with L1 penalty: 9.38% score with L1 penalty: 0.9098 Sparsity with L2 penalty: 4.69% score with L2 penalty: 0.9093 C=0.01 Sparsity with L1 penalty: 85.94% score with L1 penalty: 0.8625 Sparsity with L2 penalty: 4.69% score with L2 penalty: 0.8915 9. Path with L1- Logistic Regression ( source ) Computes path on IRIS dataset. from datetime import datetime import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model from sklearn import datasets from sklearn.svm import l1_min_c iris = datasets . load_iris() X = iris . data y = iris . target X = X[y != 2 ] y = y[y != 2 ] X -= np . mean(X, 0 ) #################################### # Demo path functions cs = l1_min_c(X, y, loss = 'log' ) * np . logspace( 0 , 3 ) print ( \"Computing regularization path ...\" ) start = datetime . now() clf = linear_model . LogisticRegression(C = 1.0 , penalty = 'l1' , tol = 1e-6 ) coefs_ = [] for c in cs: clf . set_params(C = c) clf . fit(X, y) coefs_ . append(clf . coef_ . ravel() . copy()) print ( \"This took \" , datetime . now() - start) coefs_ = np . array(coefs_) plt . plot(np . log10(cs), coefs_) ymin, ymax = plt . ylim() plt . xlabel( 'log(C)' ) plt . ylabel( 'Coefficients' ) plt . title( 'Logistic Regression Path' ) plt . axis( 'tight' ) plt . show() Computing regularization path ... This took 0:00:00.036331 10. Logistic function ( source ) Shown in the plot is how the logistic regression would, in this synthetic dataset, classify values as either 0 or 1, i.e. class one or two, using the logistic curve. import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model # this is our test set, it's just a straight line with some # Gaussian noise xmin, xmax = - 5 , 5 n_samples = 100 np . random . seed( 0 ) X = np . random . normal(size = n_samples) y = (X > 0 ) . astype(np . float) X[X > 0 ] *= 4 X += . 3 * np . random . normal(size = n_samples) X = X[:, np . newaxis] # run the classifier clf = linear_model . LogisticRegression(C = 1e5 ) clf . fit(X, y) # and plot the result plt . figure( 1 , figsize = ( 4 , 3 )) plt . clf() plt . scatter(X . ravel(), y, color = 'black' , zorder = 20 ) X_test = np . linspace( - 5 , 10 , 300 ) def model (x): return 1 / ( 1 + np . exp( - x)) loss = model(X_test * clf . coef_ + clf . intercept_) . ravel() plt . plot(X_test, loss, color = 'red' , linewidth = 3 ) ols = linear_model . LinearRegression() ols . fit(X, y) plt . plot(X_test, ols . coef_ * X_test + ols . intercept_, linewidth = 1 ) plt . axhline( . 5 , color = '.5' ) plt . ylabel( 'y' ) plt . xlabel( 'X' ) plt . xticks( range ( - 5 , 10 )) plt . yticks([ 0 , 0.5 , 1 ]) plt . ylim( -. 25 , 1.25 ) plt . xlim( - 4 , 10 ) plt . legend(( 'Logistic Regression Model' , 'Linear Regression Model' ), loc = \"lower right\" , fontsize = 'small' ) plt . show() 11. Plot multinomial and One-vs-Rest Logistic Regression ( source ) Plot decision surface of multinomial and One-vs-Rest Logistic Regression. The hyperplanes corresponding to the three One-vs-Rest (OVR) classifiers are represented by the dashed lines. import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_blobs from sklearn.linear_model import LogisticRegression # make 3-class dataset for classification centers = [[ - 5 , 0 ], [ 0 , 1.5 ], [ 5 , - 1 ]] X, y = make_blobs(n_samples = 1000 , centers = centers, random_state = 40 ) transformation = [[ 0.4 , 0.2 ], [ - 0.4 , 1.2 ]] X = np . dot(X, transformation) for multi_class in ( 'multinomial' , 'ovr' ): clf = LogisticRegression(solver = 'sag' , max_iter = 100 , random_state = 42 , multi_class = multi_class) . fit(X, y) # print the training scores print ( \"training score : %.3f ( %s )\" % (clf . score(X, y), multi_class)) # create a mesh to plot in h = . 02 # step size in the mesh x_min, x_max = X[:, 0 ] . min() - 1 , X[:, 0 ] . max() + 1 y_min, y_max = X[:, 1 ] . min() - 1 , X[:, 1 ] . max() + 1 xx, yy = np . meshgrid(np . arange(x_min, x_max, h), np . arange(y_min, y_max, h)) # Plot the decision boundary. For that, we will assign a color to each # point in the mesh [x_min, x_max]x[y_min, y_max]. Z = clf . predict(np . c_[xx . ravel(), yy . ravel()]) # Put the result into a color plot Z = Z . reshape(xx . shape) plt . figure() plt . contourf(xx, yy, Z, cmap = plt . cm . Paired) plt . title( \"Decision surface of LogisticRegression ( %s )\" % multi_class) plt . axis( 'tight' ) # Plot also the training points colors = \"bry\" for i, color in zip (clf . classes_, colors): idx = np . where(y == i) plt . scatter(X[idx, 0 ], X[idx, 1 ], c = color, cmap = plt . cm . Paired) # Plot the three one-against-all classifiers xmin, xmax = plt . xlim() ymin, ymax = plt . ylim() coef = clf . coef_ intercept = clf . intercept_ def plot_hyperplane (c, color): def line (x0): return ( - (x0 * coef[c, 0 ]) - intercept[c]) / coef[c, 1 ] plt . plot([xmin, xmax], [line(xmin), line(xmax)], ls = \"--\" , color = color) for i, color in zip (clf . classes_, colors): plot_hyperplane(i, color) plt . show() training score : 0.995 (multinomial) training score : 0.976 (ovr) 12. Lasso path using LARS ( source ) Computes Lasso Path along the regularization parameter using the LARS algorithm on the diabetes dataset. Each color represents a different feature of the coefficient vector, and this is displayed as a function of the regularization parameter. import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model from sklearn import datasets diabetes = datasets . load_diabetes() X = diabetes . data y = diabetes . target print ( \"Computing regularization path using the LARS ...\" ) alphas, _, coefs = linear_model . lars_path(X, y, method = 'lasso' , verbose = True ) xx = np . sum(np . abs(coefs . T), axis = 1 ) xx /= xx[ - 1 ] plt . plot(xx, coefs . T) ymin, ymax = plt . ylim() plt . vlines(xx, ymin, ymax, linestyle = 'dashed' ) plt . xlabel( '|coef| / max|coef|' ) plt . ylabel( 'Coefficients' ) plt . title( 'LASSO Path' ) plt . axis( 'tight' ) plt . show() Computing regularization path using the LARS ... . 13. Lasso and Elastic Net ( source ) Lasso and elastic net (L1 and L2 penalisation) implemented using a coordinate descent. The coefficients can be forced to be positive. from itertools import cycle import numpy as np import matplotlib.pyplot as plt from sklearn.linear_model import lasso_path, enet_path from sklearn import datasets diabetes = datasets . load_diabetes() X = diabetes . data y = diabetes . target X /= X . std(axis = 0 ) # Standardize data (easier to set the l1_ratio parameter) # Compute paths eps = 5e-3 # the smaller it is the longer is the path print ( \"Computing regularization path using the lasso...\" ) alphas_lasso, coefs_lasso, _ = lasso_path(X, y, eps, fit_intercept = False ) print ( \"Computing regularization path using the positive lasso...\" ) alphas_positive_lasso, coefs_positive_lasso, _ = lasso_path( X, y, eps, positive = True , fit_intercept = False ) print ( \"Computing regularization path using the elastic net...\" ) alphas_enet, coefs_enet, _ = enet_path( X, y, eps = eps, l1_ratio = 0.8 , fit_intercept = False ) print ( \"Computing regularization path using the positive elastic net...\" ) alphas_positive_enet, coefs_positive_enet, _ = enet_path( X, y, eps = eps, l1_ratio = 0.8 , positive = True , fit_intercept = False ) # Display results plt . figure( 1 ) ax = plt . gca() colors = cycle([ 'b' , 'r' , 'g' , 'c' , 'k' ]) neg_log_alphas_lasso = - np . log10(alphas_lasso) neg_log_alphas_enet = - np . log10(alphas_enet) for coef_l, coef_e, c in zip (coefs_lasso, coefs_enet, colors): l1 = plt . plot(neg_log_alphas_lasso, coef_l, c = c) l2 = plt . plot(neg_log_alphas_enet, coef_e, linestyle = '--' , c = c) plt . xlabel( '-Log(alpha)' ) plt . ylabel( 'coefficients' ) plt . title( 'Lasso and Elastic-Net Paths' ) plt . legend((l1[ - 1 ], l2[ - 1 ]), ( 'Lasso' , 'Elastic-Net' ), loc = 'lower left' ) plt . axis( 'tight' ) plt . figure( 2 ) ax = plt . gca() neg_log_alphas_positive_lasso = - np . log10(alphas_positive_lasso) for coef_l, coef_pl, c in zip (coefs_lasso, coefs_positive_lasso, colors): l1 = plt . plot(neg_log_alphas_lasso, coef_l, c = c) l2 = plt . plot(neg_log_alphas_positive_lasso, coef_pl, linestyle = '--' , c = c) plt . xlabel( '-Log(alpha)' ) plt . ylabel( 'coefficients' ) plt . title( 'Lasso and positive Lasso' ) plt . legend((l1[ - 1 ], l2[ - 1 ]), ( 'Lasso' , 'positive Lasso' ), loc = 'lower left' ) plt . axis( 'tight' ) plt . figure( 3 ) ax = plt . gca() neg_log_alphas_positive_enet = - np . log10(alphas_positive_enet) for (coef_e, coef_pe, c) in zip (coefs_enet, coefs_positive_enet, colors): l1 = plt . plot(neg_log_alphas_enet, coef_e, c = c) l2 = plt . plot(neg_log_alphas_positive_enet, coef_pe, linestyle = '--' , c = c) plt . xlabel( '-Log(alpha)' ) plt . ylabel( 'coefficients' ) plt . title( 'Elastic-Net and positive Elastic-Net' ) plt . legend((l1[ - 1 ], l2[ - 1 ]), ( 'Elastic-Net' , 'positive Elastic-Net' ), loc = 'lower left' ) plt . axis( 'tight' ) plt . show() Computing regularization path using the lasso... Computing regularization path using the positive lasso... Computing regularization path using the elastic net... Computing regularization path using the positive elastic net... 14. Lasso model selection: Cross-Validation / AIC / BIC ( source ) Use the Akaike information criterion (AIC), the Bayes Information criterion (BIC) and cross-validation to select an optimal value of the regularization parameter alpha of the :ref: lasso estimator. Results obtained with LassoLarsIC are based on AIC/BIC criteria. Information-criterion based model selection is very fast, but it relies on a proper estimation of degrees of freedom, are derived for large samples (asymptotic results) and assume the model is correct, i.e. that the data are actually generated by this model. They also tend to break when the problem is badly conditioned (more features than samples). For cross-validation, we use 20-fold with 2 algorithms to compute the Lasso path: coordinate descent, as implemented by the LassoCV class, and Lars (least angle regression) as implemented by the LassoLarsCV class. Both algorithms give roughly the same results. They differ with regards to their execution speed and sources of numerical errors. Lars computes a path solution only for each kink in the path. As a result, it is very efficient when there are only of few kinks, which is the case if there are few features or samples. Also, it is able to compute the full path without setting any meta parameter. On the opposite, coordinate descent compute the path points on a pre-specified grid (here we use the default). Thus it is more efficient if the number of grid points is smaller than the number of kinks in the path. Such a strategy can be interesting if the number of features is really large and there are enough samples to select a large amount. In terms of numerical errors, for heavily correlated variables, Lars will accumulate more errors, while the coordinate descent algorithm will only sample the path on a grid. Note how the optimal value of alpha varies for each fold. This illustrates why nested-cross validation is necessary when trying to evaluate the performance of a method for which a parameter is chosen by cross-validation: this choice of parameter may not be optimal for unseen data. import time import numpy as np import matplotlib.pyplot as plt from sklearn.linear_model import LassoCV, LassoLarsCV, LassoLarsIC from sklearn import datasets diabetes = datasets . load_diabetes() X = diabetes . data y = diabetes . target rng = np . random . RandomState( 42 ) X = np . c_[X, rng . randn(X . shape[ 0 ], 14 )] # add some bad features # normalize data as done by Lars to allow for comparison X /= np . sqrt(np . sum(X ** 2 , axis = 0 )) ######################################## # LassoLarsIC: least angle regression with BIC/AIC criterion model_bic = LassoLarsIC(criterion = 'bic' ) t1 = time . time() model_bic . fit(X, y) t_bic = time . time() - t1 alpha_bic_ = model_bic . alpha_ model_aic = LassoLarsIC(criterion = 'aic' ) model_aic . fit(X, y) alpha_aic_ = model_aic . alpha_ def plot_ic_criterion (model, name, color): alpha_ = model . alpha_ alphas_ = model . alphas_ criterion_ = model . criterion_ plt . plot( - np . log10(alphas_), criterion_, '--' , color = color, linewidth = 3 , label = ' %s criterion' % name) plt . axvline( - np . log10(alpha_), color = color, linewidth = 3 , label = 'alpha: %s estimate' % name) plt . xlabel( '-log(alpha)' ) plt . ylabel( 'criterion' ) plt . figure() plot_ic_criterion(model_aic, 'AIC' , 'b' ) plot_ic_criterion(model_bic, 'BIC' , 'r' ) plt . legend() plt . title( 'Information-criterion for model selection (training time %.3f s)' % t_bic) ############################################## # LassoCV: coordinate descent # Compute paths print ( \"Computing regularization path using the coordinate descent lasso...\" ) t1 = time . time() model = LassoCV(cv = 20 ) . fit(X, y) t_lasso_cv = time . time() - t1 # Display results m_log_alphas = - np . log10(model . alphas_) plt . figure() ymin, ymax = 2300 , 3800 plt . plot(m_log_alphas, model . mse_path_, ':' ) plt . plot(m_log_alphas, model . mse_path_ . mean(axis =- 1 ), 'k' , label = 'Average across the folds' , linewidth = 2 ) plt . axvline( - np . log10(model . alpha_), linestyle = '--' , color = 'k' , label = 'alpha: CV estimate' ) plt . legend() plt . xlabel( '-log(alpha)' ) plt . ylabel( 'Mean square error' ) plt . title( 'Mean square error on each fold: coordinate descent ' '(train time: %.2f s)' % t_lasso_cv) plt . axis( 'tight' ) plt . ylim(ymin, ymax) ###################################### # LassoLarsCV: least angle regression # Compute paths print ( \"Computing regularization path using the Lars lasso...\" ) t1 = time . time() model = LassoLarsCV(cv = 20 ) . fit(X, y) t_lasso_lars_cv = time . time() - t1 # Display results m_log_alphas = - np . log10(model . cv_alphas_) plt . figure() plt . plot(m_log_alphas, model . mse_path_, ':' ) plt . plot(m_log_alphas, model . mse_path_ . mean(axis =- 1 ), 'k' , label = 'Average across the folds' , linewidth = 2 ) plt . axvline( - np . log10(model . alpha_), linestyle = '--' , color = 'k' , label = 'alpha CV' ) plt . legend() plt . xlabel( '-log(alpha)' ) plt . ylabel( 'Mean square error' ) plt . title( 'Mean square error on each fold: Lars (train time: %.2f s)' % t_lasso_lars_cv) plt . axis( 'tight' ) plt . ylim(ymin, ymax) plt . show() /home/bitnami/anaconda3/envs/caseolap/lib/python3.6/site-packages/ipykernel_launcher.py:37: RuntimeWarning: divide by zero encountered in log10 Computing regularization path using the coordinate descent lasso... Computing regularization path using the Lars lasso... /home/bitnami/anaconda3/envs/caseolap/lib/python3.6/site-packages/ipykernel_launcher.py:90: RuntimeWarning: divide by zero encountered in log10 15. Lasso and Elastic Net for Sparse Signals ( source ) Estimates Lasso and Elastic-Net regression models on a manually generated sparse signal corrupted with an additive noise. Estimated coefficients are compared with the ground-truth. import numpy as np import matplotlib.pyplot as plt from sklearn.metrics import r2_score ######################################### # generate some sparse data to play with np . random . seed( 42 ) n_samples, n_features = 50 , 200 X = np . random . randn(n_samples, n_features) coef = 3 * np . random . randn(n_features) inds = np . arange(n_features) np . random . shuffle(inds) coef[inds[ 10 :]] = 0 # sparsify coef y = np . dot(X, coef) # add noise y += 0.01 * np . random . normal((n_samples,)) # Split data in train set and test set n_samples = X . shape[ 0 ] X_train, y_train = X[:n_samples / 2 ], y[:n_samples / 2 ] X_test, y_test = X[n_samples / 2 :], y[n_samples / 2 :] ############################################### # Lasso from sklearn.linear_model import Lasso alpha = 0.1 lasso = Lasso(alpha = alpha) y_pred_lasso = lasso . fit(X_train, y_train) . predict(X_test) r2_score_lasso = r2_score(y_test, y_pred_lasso) print (lasso) print ( \"r^2 on test data : %f \" % r2_score_lasso) ############################################ # ElasticNet from sklearn.linear_model import ElasticNet enet = ElasticNet(alpha = alpha, l1_ratio = 0.7 ) y_pred_enet = enet . fit(X_train, y_train) . predict(X_test) r2_score_enet = r2_score(y_test, y_pred_enet) print (enet) print ( \"r^2 on test data : %f \" % r2_score_enet) plt . plot(enet . coef_, color = 'lightgreen' , linewidth = 2 , label = 'Elastic net coefficients' ) plt . plot(lasso . coef_, color = 'gold' , linewidth = 2 , label = 'Lasso coefficients' ) plt . plot(coef, '--' , color = 'navy' , label = 'original coefficients' ) plt . legend(loc = 'best' ) plt . title( \"Lasso R^2: %f , Elastic Net R^2: %f \" % (r2_score_lasso, r2_score_enet)) plt . show() /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:23: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:24: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000, normalize=False, positive=False, precompute=False, random_state=None, selection='cyclic', tol=0.0001, warm_start=False) r^2 on test data : 0.384710 ElasticNet(alpha=0.1, copy_X=True, fit_intercept=True, l1_ratio=0.7, max_iter=1000, normalize=False, positive=False, precompute=False, random_state=None, selection='cyclic', tol=0.0001, warm_start=False) r^2 on test data : 0.240176 16. Joint feature selection with multi-task Lasso ( source ) The multi-task lasso allows to fit multiple regression problems jointly enforcing the selected features to be the same across tasks. This example simulates sequential measurements, each task is a time instant, and the relevant features vary in amplitude over time while being the same. The multi-task lasso imposes that features that are selected at one time point are select for all time point. This makes feature selection by the Lasso more stable. import matplotlib.pyplot as plt import numpy as np from sklearn.linear_model import MultiTaskLasso, Lasso rng = np . random . RandomState( 42 ) # Generate some 2D coefficients with sine waves with random frequency and phase n_samples, n_features, n_tasks = 100 , 30 , 40 n_relevant_features = 5 coef = np . zeros((n_tasks, n_features)) times = np . linspace( 0 , 2 * np . pi, n_tasks) for k in range (n_relevant_features): coef[:, k] = np . sin(( 1. + rng . randn( 1 )) * times + 3 * rng . randn( 1 )) X = rng . randn(n_samples, n_features) Y = np . dot(X, coef . T) + rng . randn(n_samples, n_tasks) coef_lasso_ = np . array([Lasso(alpha = 0.5 ) . fit(X, y) . coef_ for y in Y . T]) coef_multi_task_lasso_ = MultiTaskLasso(alpha = 1. ) . fit(X, Y) . coef_ ############################################# # Plot support and time series fig = plt . figure(figsize = ( 8 , 5 )) plt . subplot( 1 , 2 , 1 ) plt . spy(coef_lasso_) plt . xlabel( 'Feature' ) plt . ylabel( 'Time (or Task)' ) plt . text( 10 , 5 , 'Lasso' ) plt . subplot( 1 , 2 , 2 ) plt . spy(coef_multi_task_lasso_) plt . xlabel( 'Feature' ) plt . ylabel( 'Time (or Task)' ) plt . text( 10 , 5 , 'MultiTaskLasso' ) fig . suptitle( 'Coefficient non-zero location' ) feature_to_plot = 0 plt . figure() lw = 2 plt . plot(coef[:, feature_to_plot], color = 'seagreen' , linewidth = lw, label = 'Ground truth' ) plt . plot(coef_lasso_[:, feature_to_plot], color = 'cornflowerblue' , linewidth = lw, label = 'Lasso' ) plt . plot(coef_multi_task_lasso_[:, feature_to_plot], color = 'gold' , linewidth = lw, label = 'MultiTaskLasso' ) plt . legend(loc = 'upper center' ) plt . axis( 'tight' ) plt . ylim([ - 1.1 , 1.1 ]) plt . show() 17. Lasso on dense and sparse data ( source ) We show that linear_model.Lasso provides the same results for dense and sparse data and that in the case of sparse data the speed is improved. from time import time from scipy import sparse from scipy import linalg from sklearn.datasets.samples_generator import make_regression from sklearn.linear_model import Lasso ######################################### # The two Lasso implementations on Dense data print ( \"--- Dense matrices\" ) X, y = make_regression(n_samples = 200 , n_features = 5000 , random_state = 0 ) X_sp = sparse . coo_matrix(X) alpha = 1 sparse_lasso = Lasso(alpha = alpha, fit_intercept = False , max_iter = 1000 ) dense_lasso = Lasso(alpha = alpha, fit_intercept = False , max_iter = 1000 ) t0 = time() sparse_lasso . fit(X_sp, y) print ( \"Sparse Lasso done in %f s\" % (time() - t0)) t0 = time() dense_lasso . fit(X, y) print ( \"Dense Lasso done in %f s\" % (time() - t0)) print ( \"Distance between coefficients : %s \" % linalg . norm(sparse_lasso . coef_ - dense_lasso . coef_)) ############################################ # The two Lasso implementations on Sparse data print ( \"--- Sparse matrices\" ) Xs = X . copy() Xs[Xs < 2.5 ] = 0.0 Xs = sparse . coo_matrix(Xs) Xs = Xs . tocsc() print ( \"Matrix density : %s %% \" % (Xs . nnz / float (X . size) * 100 )) alpha = 0.1 sparse_lasso = Lasso(alpha = alpha, fit_intercept = False , max_iter = 10000 ) dense_lasso = Lasso(alpha = alpha, fit_intercept = False , max_iter = 10000 ) t0 = time() sparse_lasso . fit(Xs, y) print ( \"Sparse Lasso done in %f s\" % (time() - t0)) t0 = time() dense_lasso . fit(Xs . toarray(), y) print ( \"Dense Lasso done in %f s\" % (time() - t0)) print ( \"Distance between coefficients : %s \" % linalg . norm(sparse_lasso . coef_ - dense_lasso . coef_)) --- Dense matrices Sparse Lasso done in 0.385830s Dense Lasso done in 0.069551s Distance between coefficients : 8.407255028117243e-14 --- Sparse matrices Matrix density : 0.6263000000000001 % Sparse Lasso done in 0.237209s Dense Lasso done in 1.616462s Distance between coefficients : 1.0424172088134681e-11 18. Plot multi-class SGD on the iris dataset ( source ) Plot decision surface of multi-class SGD on iris dataset. The hyperplanes corresponding to the three one-versus-all (OVA) classifiers are represented by the dashed lines. import numpy as np import matplotlib.pyplot as plt from sklearn import datasets from sklearn.linear_model import SGDClassifier # import some data to play with iris = datasets . load_iris() X = iris . data[:, : 2 ] # we only take the first two features. We could # avoid this ugly slicing by using a two-dim dataset y = iris . target colors = \"bry\" # shuffle idx = np . arange(X . shape[ 0 ]) np . random . seed( 13 ) np . random . shuffle(idx) X = X[idx] y = y[idx] # standardize mean = X . mean(axis = 0 ) std = X . std(axis = 0 ) X = (X - mean) / std h = . 02 # step size in the mesh clf = SGDClassifier(alpha = 0.001 , n_iter = 100 ) . fit(X, y) # create a mesh to plot in x_min, x_max = X[:, 0 ] . min() - 1 , X[:, 0 ] . max() + 1 y_min, y_max = X[:, 1 ] . min() - 1 , X[:, 1 ] . max() + 1 xx, yy = np . meshgrid(np . arange(x_min, x_max, h), np . arange(y_min, y_max, h)) # Plot the decision boundary. For that, we will assign a color to each # point in the mesh [x_min, x_max]x[y_min, y_max]. Z = clf . predict(np . c_[xx . ravel(), yy . ravel()]) # Put the result into a color plot Z = Z . reshape(xx . shape) cs = plt . contourf(xx, yy, Z, cmap = plt . cm . Paired) plt . axis( 'tight' ) # Plot also the training points for i, color in zip (clf . classes_, colors): idx = np . where(y == i) plt . scatter(X[idx, 0 ], X[idx, 1 ], c = color, label = iris . target_names[i], cmap = plt . cm . Paired) plt . title( \"Decision surface of multi-class SGD\" ) plt . axis( 'tight' ) # Plot the three one-against-all classifiers xmin, xmax = plt . xlim() ymin, ymax = plt . ylim() coef = clf . coef_ intercept = clf . intercept_ def plot_hyperplane (c, color): def line (x0): return ( - (x0 * coef[c, 0 ]) - intercept[c]) / coef[c, 1 ] plt . plot([xmin, xmax], [line(xmin), line(xmax)], ls = \"--\" , color = color) for i, color in zip (clf . classes_, colors): plot_hyperplane(i, color) plt . legend() plt . show() 19. SGD: Penalties ( source ) Plot the contours of the three penalties. import numpy as np import matplotlib.pyplot as plt def l1 (xs): return np . array([np . sqrt(( 1 - np . sqrt(x ** 2.0 )) ** 2.0 ) for x in xs]) def l2 (xs): return np . array([np . sqrt( 1.0 - x ** 2.0 ) for x in xs]) def el (xs, z): return np . array([( 2 - 2 * x - 2 * z + 4 * x * z - ( 4 * z ** 2 - 8 * x * z ** 2 + 8 * x ** 2 * z ** 2 - 16 * x ** 2 * z ** 3 + 8 * x * z ** 3 + 4 * x ** 2 * z ** 4 ) ** ( 1. / 2 ) - 2 * x * z ** 2 ) / ( 2 - 4 * z) for x in xs]) def cross (ext): plt . plot([ - ext, ext], [ 0 , 0 ], \"k-\" ) plt . plot([ 0 , 0 ], [ - ext, ext], \"k-\" ) xs = np . linspace( 0 , 1 , 100 ) alpha = 0.501 # 0.5 division throuh zero cross( 1.2 ) l1_color = \"navy\" l2_color = \"c\" elastic_net_color = \"darkorange\" lw = 2 plt . plot(xs, l1(xs), color = l1_color, label = \"L1\" , lw = lw) plt . plot(xs, - 1.0 * l1(xs), color = l1_color, lw = lw) plt . plot( - 1 * xs, l1(xs), color = l1_color, lw = lw) plt . plot( - 1 * xs, - 1.0 * l1(xs), color = l1_color, lw = lw) plt . plot(xs, l2(xs), color = l2_color, label = \"L2\" , lw = lw) plt . plot(xs, - 1.0 * l2(xs), color = l2_color, lw = lw) plt . plot( - 1 * xs, l2(xs), color = l2_color, lw = lw) plt . plot( - 1 * xs, - 1.0 * l2(xs), color = l2_color, lw = lw) plt . plot(xs, el(xs, alpha), color = elastic_net_color, label = \"Elastic Net\" , lw = lw) plt . plot(xs, - 1.0 * el(xs, alpha), color = elastic_net_color, lw = lw) plt . plot( - 1 * xs, el(xs, alpha), color = elastic_net_color, lw = lw) plt . plot( - 1 * xs, - 1.0 * el(xs, alpha), color = elastic_net_color, lw = lw) plt . xlabel( r\"$w_0$\" ) plt . ylabel( r\"$w_1$\" ) plt . legend() plt . axis( \"equal\" ) plt . show() 20. Comparing various online solvers ( source ) An example showing how different online solvers perform on the hand-written digits dataset. import numpy as np import matplotlib.pyplot as plt from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.linear_model import SGDClassifier, Perceptron from sklearn.linear_model import PassiveAggressiveClassifier from sklearn.linear_model import LogisticRegression heldout = [ 0.95 , 0.90 , 0.75 , 0.50 , 0.01 ] rounds = 20 digits = datasets . load_digits() X, y = digits . data, digits . target classifiers = [ ( \"SGD\" , SGDClassifier()), ( \"ASGD\" , SGDClassifier(average = True )), ( \"Perceptron\" , Perceptron()), ( \"Passive-Aggressive I\" , PassiveAggressiveClassifier(loss = 'hinge' , C = 1.0 )), ( \"Passive-Aggressive II\" , PassiveAggressiveClassifier(loss = 'squared_hinge' , C = 1.0 )), ( \"SAG\" , LogisticRegression(solver = 'sag' , tol = 1e-1 , C = 1.e4 / X . shape[ 0 ])) ] xx = 1. - np . array(heldout) for name, clf in classifiers: print ( \"training %s \" % name) rng = np . random . RandomState( 42 ) yy = [] for i in heldout: yy_ = [] for r in range (rounds): X_train, X_test, y_train, y_test = \\ train_test_split(X, y, test_size = i, random_state = rng) clf . fit(X_train, y_train) y_pred = clf . predict(X_test) yy_ . append( 1 - np . mean(y_pred == y_test)) yy . append(np . mean(yy_)) plt . plot(xx, yy, label = name) plt . legend(loc = \"upper right\" ) plt . xlabel( \"Proportion train\" ) plt . ylabel( \"Test Error Rate\" ) plt . show() training SGD training ASGD training Perceptron training Passive-Aggressive I training Passive-Aggressive II training SAG 21. SGD: convex loss functions ( source ) A plot that compares the various convex loss functions supported by :class: sklearn.linear_model.SGDClassifier . import numpy as np import matplotlib.pyplot as plt def modified_huber_loss (y_true, y_pred): z = y_pred * y_true loss = - 4 * z loss[z >= - 1 ] = ( 1 - z[z >= - 1 ]) ** 2 loss[z >= 1. ] = 0 return loss xmin, xmax = - 4 , 4 xx = np . linspace(xmin, xmax, 100 ) lw = 2 plt . plot([xmin, 0 , 0 , xmax], [ 1 , 1 , 0 , 0 ], color = 'gold' , lw = lw, label = \"Zero-one loss\" ) plt . plot(xx, np . where(xx < 1 , 1 - xx, 0 ), color = 'teal' , lw = lw, label = \"Hinge loss\" ) plt . plot(xx, - np . minimum(xx, 0 ), color = 'yellowgreen' , lw = lw, label = \"Perceptron loss\" ) plt . plot(xx, np . log2( 1 + np . exp( - xx)), color = 'cornflowerblue' , lw = lw, label = \"Log loss\" ) plt . plot(xx, np . where(xx < 1 , 1 - xx, 0 ) ** 2 , color = 'orange' , lw = lw, label = \"Squared hinge loss\" ) plt . plot(xx, modified_huber_loss(xx, 1 ), color = 'darkorchid' , lw = lw, linestyle = '--' , label = \"Modified Huber loss\" ) plt . ylim(( 0 , 8 )) plt . legend(loc = \"upper right\" ) plt . xlabel( r\"Decision function $f(x)$\" ) plt . ylabel( \"$L(y, f(x))$\" ) plt . show() 22. SGD: Maximum margin separating hyperplane ( source ) Plot the maximum margin separating hyperplane within a two-class separable dataset using a linear Support Vector Machines classifier trained using SGD. import numpy as np import matplotlib.pyplot as plt from sklearn.linear_model import SGDClassifier from sklearn.datasets.samples_generator import make_blobs # we create 50 separable points X, Y = make_blobs(n_samples = 50 , centers = 2 , random_state = 0 , cluster_std = 0.60 ) # fit the model clf = SGDClassifier(loss = \"hinge\" , alpha = 0.01 , n_iter = 200 , fit_intercept = True ) clf . fit(X, Y) # plot the line, the points, and the nearest vectors to the plane xx = np . linspace( - 1 , 5 , 10 ) yy = np . linspace( - 1 , 5 , 10 ) X1, X2 = np . meshgrid(xx, yy) Z = np . empty(X1 . shape) for (i, j), val in np . ndenumerate(X1): x1 = val x2 = X2[i, j] p = clf . decision_function([[x1, x2]]) Z[i, j] = p[ 0 ] levels = [ - 1.0 , 0.0 , 1.0 ] linestyles = [ 'dashed' , 'solid' , 'dashed' ] colors = 'k' plt . contour(X1, X2, Z, levels, colors = colors, linestyles = linestyles) plt . scatter(X[:, 0 ], X[:, 1 ], c = Y, cmap = plt . cm . Paired) plt . axis( 'tight' ) plt . show() 23. SGD: Weighted samples ( source ) Plot decision function of a weighted dataset, where the size of points is proportional to its weight. import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model # we create 20 points np . random . seed( 0 ) X = np . r_[np . random . randn( 10 , 2 ) + [ 1 , 1 ], np . random . randn( 10 , 2 )] y = [ 1 ] * 10 + [ - 1 ] * 10 sample_weight = 100 * np . abs(np . random . randn( 20 )) # and assign a bigger weight to the last 10 samples sample_weight[: 10 ] *= 10 # plot the weighted data points xx, yy = np . meshgrid(np . linspace( - 4 , 5 , 500 ), np . linspace( - 4 , 5 , 500 )) plt . figure() plt . scatter(X[:, 0 ], X[:, 1 ], c = y, s = sample_weight, alpha = 0.9 , cmap = plt . cm . bone) ## fit the unweighted model clf = linear_model . SGDClassifier(alpha = 0.01 , n_iter = 100 ) clf . fit(X, y) Z = clf . decision_function(np . c_[xx . ravel(), yy . ravel()]) Z = Z . reshape(xx . shape) no_weights = plt . contour(xx, yy, Z, levels = [ 0 ], linestyles = [ 'solid' ]) ## fit the weighted model clf = linear_model . SGDClassifier(alpha = 0.01 , n_iter = 100 ) clf . fit(X, y, sample_weight = sample_weight) Z = clf . decision_function(np . c_[xx . ravel(), yy . ravel()]) Z = Z . reshape(xx . shape) samples_weights = plt . contour(xx, yy, Z, levels = [ 0 ], linestyles = [ 'dashed' ]) plt . legend([no_weights . collections[ 0 ], samples_weights . collections[ 0 ]], [ \"no weights\" , \"with weights\" ], loc = \"lower left\" ) plt . xticks(()) plt . yticks(()) plt . show() 24. Polynomial interpolation ( source ) This example demonstrates how to approximate a function with a polynomial of degree n_degree by using ridge regression. Concretely, from n_samples 1d points, it suffices to build the Vandermonde matrix, which is n_samples x n_degree+1 and has the following form: [[1, x_1, x_1 2, x_1 3, ...], [1, x_2, x_2 2, x_2 3, ...], ...] Intuitively, this matrix can be interpreted as a matrix of pseudo features (the points raised to some power). The matrix is akin to (but different from) the matrix induced by a polynomial kernel. This example shows that you can do non-linear regression with a linear model, using a pipeline to add non-linear features. Kernel methods extend this idea and can induce very high (even infinite) dimensional feature spaces. import numpy as np import matplotlib.pyplot as plt from sklearn.linear_model import Ridge from sklearn.preprocessing import PolynomialFeatures from sklearn.pipeline import make_pipeline def f (x): \"\"\" function to approximate by polynomial interpolation\"\"\" return x * np . sin(x) # generate points used to plot x_plot = np . linspace( 0 , 10 , 100 ) # generate points and keep a subset of them x = np . linspace( 0 , 10 , 100 ) rng = np . random . RandomState( 0 ) rng . shuffle(x) x = np . sort(x[: 20 ]) y = f(x) # create matrix versions of these arrays X = x[:, np . newaxis] X_plot = x_plot[:, np . newaxis] colors = [ 'teal' , 'yellowgreen' , 'gold' ] lw = 2 plt . plot(x_plot, f(x_plot), color = 'cornflowerblue' , linewidth = lw, label = \"ground truth\" ) plt . scatter(x, y, color = 'navy' , s = 30 , marker = 'o' , label = \"training points\" ) for count, degree in enumerate ([ 3 , 4 , 5 ]): model = make_pipeline(PolynomialFeatures(degree), Ridge()) model . fit(X, y) y_plot = model . predict(X_plot) plt . plot(x_plot, y_plot, color = colors[count], linewidth = lw, label = \"degree %d \" % degree) plt . legend(loc = 'lower left' ) plt . show() 25. Orthogonal Matching Pursuit ( source ) Using orthogonal matching pursuit for recovering a sparse signal from a noisy measurement encoded with a dictionary import matplotlib.pyplot as plt import numpy as np from sklearn.linear_model import OrthogonalMatchingPursuit from sklearn.linear_model import OrthogonalMatchingPursuitCV from sklearn.datasets import make_sparse_coded_signal n_components, n_features = 512 , 100 n_nonzero_coefs = 17 # generate the data ################### # y = Xw # |x|_0 = n_nonzero_coefs y, X, w = make_sparse_coded_signal(n_samples = 1 , n_components = n_components, n_features = n_features, n_nonzero_coefs = n_nonzero_coefs, random_state = 0 ) idx, = w . nonzero() # distort the clean signal ########################## y_noisy = y + 0.05 * np . random . randn( len (y)) # plot the sparse signal ######################## plt . figure(figsize = ( 7 , 7 )) plt . subplot( 4 , 1 , 1 ) plt . xlim( 0 , 512 ) plt . title( \"Sparse signal\" ) plt . stem(idx, w[idx]) # plot the noise-free reconstruction #################################### omp = OrthogonalMatchingPursuit(n_nonzero_coefs = n_nonzero_coefs) omp . fit(X, y) coef = omp . coef_ idx_r, = coef . nonzero() plt . subplot( 4 , 1 , 2 ) plt . xlim( 0 , 512 ) plt . title( \"Recovered signal from noise-free measurements\" ) plt . stem(idx_r, coef[idx_r]) # plot the noisy reconstruction ############################### omp . fit(X, y_noisy) coef = omp . coef_ idx_r, = coef . nonzero() plt . subplot( 4 , 1 , 3 ) plt . xlim( 0 , 512 ) plt . title( \"Recovered signal from noisy measurements\" ) plt . stem(idx_r, coef[idx_r]) # plot the noisy reconstruction with number of non-zeros set by CV ################################################################## omp_cv = OrthogonalMatchingPursuitCV() omp_cv . fit(X, y_noisy) coef = omp_cv . coef_ idx_r, = coef . nonzero() plt . subplot( 4 , 1 , 4 ) plt . xlim( 0 , 512 ) plt . title( \"Recovered signal from noisy measurements with CV\" ) plt . stem(idx_r, coef[idx_r]) plt . subplots_adjust( 0.06 , 0.04 , 0.94 , 0.90 , 0.20 , 0.38 ) plt . suptitle( 'Sparse signal recovery with Orthogonal Matching Pursuit' , fontsize = 16 ) plt . show() 26. Automatic Relevance Determination Regression (ARD) ( source ) Fit regression model with Bayesian Ridge Regression. See :ref: bayesian_ridge_regression for more information on the regressor. Compared to the OLS (ordinary least squares) estimator, the coefficient weights are slightly shifted toward zeros, which stabilises them. The histogram of the estimated weights is very peaked, as a sparsity-inducing prior is implied on the weights. The estimation of the model is done by iteratively maximizing the marginal log-likelihood of the observations. We also plot predictions and uncertainties for ARD for one dimensional regression using polynomial feature expansion. Note the uncertainty starts going up on the right side of the plot. This is because these test samples are outside of the range of the training samples. import numpy as np import matplotlib.pyplot as plt from scipy import stats from sklearn.linear_model import ARDRegression, LinearRegression ############################################################################### # Generating simulated data with Gaussian weights # Parameters of the example np . random . seed( 0 ) n_samples, n_features = 100 , 100 # Create Gaussian data X = np . random . randn(n_samples, n_features) # Create weights with a precision lambda_ of 4. lambda_ = 4. w = np . zeros(n_features) # Only keep 10 weights of interest relevant_features = np . random . randint( 0 , n_features, 10 ) for i in relevant_features: w[i] = stats . norm . rvs(loc = 0 , scale = 1. / np . sqrt(lambda_)) # Create noise with a precision alpha of 50. alpha_ = 50. noise = stats . norm . rvs(loc = 0 , scale = 1. / np . sqrt(alpha_), size = n_samples) # Create the target y = np . dot(X, w) + noise ##################################### # Fit the ARD Regression clf = ARDRegression(compute_score = True ) clf . fit(X, y) ols = LinearRegression() ols . fit(X, y) ##################################### # Plot the true weights, the estimated weights, the histogram of the # weights, and predictions with standard deviations plt . figure(figsize = ( 6 , 5 )) plt . title( \"Weights of the model\" ) plt . plot(clf . coef_, color = 'darkblue' , linestyle = '-' , linewidth = 2 , label = \"ARD estimate\" ) plt . plot(ols . coef_, color = 'yellowgreen' , linestyle = ':' , linewidth = 2 , label = \"OLS estimate\" ) plt . plot(w, color = 'orange' , linestyle = '-' , linewidth = 2 , label = \"Ground truth\" ) plt . xlabel( \"Features\" ) plt . ylabel( \"Values of the weights\" ) plt . legend(loc = 1 ) plt . figure(figsize = ( 6 , 5 )) plt . title( \"Histogram of the weights\" ) plt . hist(clf . coef_, bins = n_features, color = 'navy' , log = True ) plt . scatter(clf . coef_[relevant_features], 5 * np . ones( len (relevant_features)), color = 'gold' , marker = 'o' , label = \"Relevant features\" ) plt . ylabel( \"Features\" ) plt . xlabel( \"Values of the weights\" ) plt . legend(loc = 1 ) plt . figure(figsize = ( 6 , 5 )) plt . title( \"Marginal log-likelihood\" ) plt . plot(clf . scores_, color = 'navy' , linewidth = 2 ) plt . ylabel( \"Score\" ) plt . xlabel( \"Iterations\" ) # Plotting some predictions for polynomial regression def f (x, noise_amount): y = np . sqrt(x) * np . sin(x) noise = np . random . normal( 0 , 1 , len (x)) return y + noise_amount * noise degree = 10 X = np . linspace( 0 , 10 , 100 ) y = f(X, noise_amount = 1 ) clf_poly = ARDRegression(threshold_lambda = 1e5 ) clf_poly . fit(np . vander(X, degree), y) X_plot = np . linspace( 0 , 11 , 25 ) y_plot = f(X_plot, noise_amount = 0 ) y_mean, y_std = clf_poly . predict(np . vander(X_plot, degree), return_std = True ) plt . figure(figsize = ( 6 , 5 )) plt . errorbar(X_plot, y_mean, y_std, color = 'navy' , label = \"Polynomial ARD\" , linewidth = 2 ) plt . plot(X_plot, y_plot, color = 'gold' , linewidth = 2 , label = \"Ground Truth\" ) plt . ylabel( \"Output y\" ) plt . xlabel( \"Feature X\" ) plt . legend(loc = \"lower left\" ) plt . show() 27. HuberRegressor vs Ridge on dataset with strong outliers ( source ) Fit Ridge and HuberRegressor on a dataset with outliers. The example shows that the predictions in ridge are strongly influenced by the outliers present in the dataset. The Huber regressor is less influenced by the outliers since the model uses the linear loss for these. As the parameter epsilon is increased for the Huber regressor, the decision function approaches that of the ridge. import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_regression from sklearn.linear_model import HuberRegressor, Ridge # Generate toy data. rng = np . random . RandomState( 0 ) X, y = make_regression(n_samples = 20 , n_features = 1 , random_state = 0 , noise = 4.0 , bias = 100.0 ) # Add four strong outliers to the dataset. X_outliers = rng . normal( 0 , 0.5 , size = ( 4 , 1 )) y_outliers = rng . normal( 0 , 2.0 , size = 4 ) X_outliers[: 2 , :] += X . max() + X . mean() / 4. X_outliers[ 2 :, :] += X . min() - X . mean() / 4. y_outliers[: 2 ] += y . min() - y . mean() / 4. y_outliers[ 2 :] += y . max() + y . mean() / 4. X = np . vstack((X, X_outliers)) y = np . concatenate((y, y_outliers)) plt . plot(X, y, 'b.' ) # Fit the huber regressor over a series of epsilon values. colors = [ 'r-' , 'b-' , 'y-' , 'm-' ] x = np . linspace(X . min(), X . max(), 7 ) epsilon_values = [ 1.35 , 1.5 , 1.75 , 1.9 ] for k, epsilon in enumerate (epsilon_values): huber = HuberRegressor(fit_intercept = True , alpha = 0.0 , max_iter = 100 , epsilon = epsilon) huber . fit(X, y) coef_ = huber . coef_ * x + huber . intercept_ plt . plot(x, coef_, colors[k], label = \"huber loss, %s \" % epsilon) # Fit a ridge regressor to compare it to huber regressor. ridge = Ridge(fit_intercept = True , alpha = 0.0 , random_state = 0 , normalize = True ) ridge . fit(X, y) coef_ridge = ridge . coef_ coef_ = ridge . coef_ * x + ridge . intercept_ plt . plot(x, coef_, 'g-' , label = \"ridge regression\" ) plt . title( \"Comparison of HuberRegressor vs Ridge\" ) plt . xlabel( \"X\" ) plt . ylabel( \"y\" ) plt . legend(loc = 0 ) plt . show() 28. Robust linear model estimation using RANSAC ( source ) import numpy as np from matplotlib import pyplot as plt from sklearn import linear_model, datasets n_samples = 1000 n_outliers = 50 X, y, coef = datasets . make_regression(n_samples = n_samples, n_features = 1 , n_informative = 1 , noise = 10 , coef = True , random_state = 0 ) # Add outlier data np . random . seed( 0 ) X[:n_outliers] = 3 + 0.5 * np . random . normal(size = (n_outliers, 1 )) y[:n_outliers] = - 3 + 10 * np . random . normal(size = n_outliers) # Fit line using all data model = linear_model . LinearRegression() model . fit(X, y) # Robustly fit linear model with RANSAC algorithm model_ransac = linear_model . RANSACRegressor(linear_model . LinearRegression()) model_ransac . fit(X, y) inlier_mask = model_ransac . inlier_mask_ outlier_mask = np . logical_not(inlier_mask) # Predict data of estimated models line_X = np . arange( - 5 , 5 ) line_y = model . predict(line_X[:, np . newaxis]) line_y_ransac = model_ransac . predict(line_X[:, np . newaxis]) # Compare estimated coefficients print ( \"Estimated coefficients (true, normal, RANSAC):\" ) print (coef, model . coef_, model_ransac . estimator_ . coef_) lw = 2 plt . scatter(X[inlier_mask], y[inlier_mask], color = 'yellowgreen' , marker = '.' , label = 'Inliers' ) plt . scatter(X[outlier_mask], y[outlier_mask], color = 'gold' , marker = '.' , label = 'Outliers' ) plt . plot(line_X, line_y, color = 'navy' , linestyle = '-' , linewidth = lw, label = 'Linear regressor' ) plt . plot(line_X, line_y_ransac, color = 'cornflowerblue' , linestyle = '-' , linewidth = lw, label = 'RANSAC regressor' ) plt . legend(loc = 'lower right' ) plt . show() Estimated coefficients (true, normal, RANSAC): 82.1903908407869 [ 54.17236387] [ 82.08533159] 29. Robust linear estimator fitting ( source ) Here a sine function is fit with a polynomial of order 3, for values close to zero. Robust fitting is demoed in different situations: No measurement errors, only modelling errors (fitting a sine with a polynomial) Measurement errors in X Measurement errors in y The median absolute deviation to non corrupt new data is used to judge the quality of the prediction. What we can see that: RANSAC is good for strong outliers in the y direction TheilSen is good for small outliers, both in direction X and y, but has a break point above which it performs worse than OLS. The scores of HuberRegressor may not be compared directly to both TheilSen and RANSAC because it does not attempt to completely filter the outliers but lessen their effect. from matplotlib import pyplot as plt import numpy as np from sklearn.linear_model import ( LinearRegression, TheilSenRegressor, RANSACRegressor, HuberRegressor) from sklearn.metrics import mean_squared_error from sklearn.preprocessing import PolynomialFeatures from sklearn.pipeline import make_pipeline np . random . seed( 42 ) X = np . random . normal(size = 400 ) y = np . sin(X) # Make sure that it X is 2D X = X[:, np . newaxis] X_test = np . random . normal(size = 200 ) y_test = np . sin(X_test) X_test = X_test[:, np . newaxis] y_errors = y . copy() y_errors[:: 3 ] = 3 X_errors = X . copy() X_errors[:: 3 ] = 3 y_errors_large = y . copy() y_errors_large[:: 3 ] = 10 X_errors_large = X . copy() X_errors_large[:: 3 ] = 10 estimators = [( 'OLS' , LinearRegression()), ( 'Theil-Sen' , TheilSenRegressor(random_state = 42 )), ( 'RANSAC' , RANSACRegressor(random_state = 42 )), ( 'HuberRegressor' , HuberRegressor())] colors = { 'OLS' : 'turquoise' , 'Theil-Sen' : 'gold' , 'RANSAC' : 'lightgreen' , 'HuberRegressor' : 'black' } linestyle = { 'OLS' : '-' , 'Theil-Sen' : '-.' , 'RANSAC' : '--' , 'HuberRegressor' : '--' } lw = 3 x_plot = np . linspace(X . min(), X . max()) for title, this_X, this_y in [ ( 'Modeling Errors Only' , X, y), ( 'Corrupt X, Small Deviants' , X_errors, y), ( 'Corrupt y, Small Deviants' , X, y_errors), ( 'Corrupt X, Large Deviants' , X_errors_large, y), ( 'Corrupt y, Large Deviants' , X, y_errors_large)]: plt . figure(figsize = ( 5 , 4 )) plt . plot(this_X[:, 0 ], this_y, 'b+' ) for name, estimator in estimators: model = make_pipeline(PolynomialFeatures( 3 ), estimator) model . fit(this_X, this_y) mse = mean_squared_error(model . predict(X_test), y_test) y_plot = model . predict(x_plot[:, np . newaxis]) plt . plot(x_plot, y_plot, color = colors[name], linestyle = linestyle[name], linewidth = lw, label = ' %s : error = %.3f ' % (name, mse)) legend_title = 'Error of Mean \\n Absolute Deviation \\n to Non-corrupt Data' legend = plt . legend(loc = 'upper right' , frameon = False , title = legend_title, prop = dict (size = 'x-small' )) plt . xlim( - 4 , 10.2 ) plt . ylim( - 2 , 10.2 ) plt . title(title) plt . show() 30. Sparse recovery: feature selection for sparse linear models ( source ) Given a small number of observations, we want to recover which features of X are relevant to explain y. For this :ref: sparse linear models <l1_feature_selection> can outperform standard statistical tests if the true model is sparse, i.e. if a small fraction of the features are relevant. As detailed in :ref: the compressive sensing notes <compressive_sensing> , the ability of L1-based approach to identify the relevant variables depends on the sparsity of the ground truth, the number of samples, the number of features, the conditioning of the design matrix on the signal subspace, the amount of noise, and the absolute value of the smallest non-zero coefficient [Wainwright2006] (http://statistics.berkeley.edu/sites/default/files/tech-reports/709.pdf). Here we keep all parameters constant and vary the conditioning of the design matrix. For a well-conditioned design matrix (small mutual incoherence) we are exactly in compressive sensing conditions (i.i.d Gaussian sensing matrix), and L1-recovery with the Lasso performs very well. For an ill-conditioned matrix (high mutual incoherence), regressors are very correlated, and the Lasso randomly selects one. However, randomized-Lasso can recover the ground truth well. In each situation, we first vary the alpha parameter setting the sparsity of the estimated model and look at the stability scores of the randomized Lasso. This analysis, knowing the ground truth, shows an optimal regime in which relevant features stand out from the irrelevant ones. If alpha is chosen too small, non-relevant variables enter the model. On the opposite, if alpha is selected too large, the Lasso is equivalent to stepwise regression, and thus brings no advantage over a univariate F-test. In a second time, we set alpha and compare the performance of different feature selection methods, using the area under curve (AUC) of the precision-recall. import warnings import matplotlib.pyplot as plt import numpy as np from scipy import linalg from sklearn.linear_model import (RandomizedLasso, lasso_stability_path, LassoLarsCV) from sklearn.feature_selection import f_regression from sklearn.preprocessing import StandardScaler from sklearn.metrics import auc, precision_recall_curve from sklearn.ensemble import ExtraTreesRegressor from sklearn.utils.extmath import pinvh from sklearn.exceptions import ConvergenceWarning def mutual_incoherence (X_relevant, X_irelevant): \"\"\"Mutual incoherence, as defined by formula (26a) of [Wainwright2006]. \"\"\" projector = np . dot(np . dot(X_irelevant . T, X_relevant), pinvh(np . dot(X_relevant . T, X_relevant))) return np . max(np . abs(projector) . sum(axis = 1 )) for conditioning in ( 1 , 1e-4 ): ################################## # Simulate regression data with a correlated design n_features = 501 n_relevant_features = 3 noise_level = . 2 coef_min = . 2 # The Donoho-Tanner phase transition is around n_samples=25: below we # will completely fail to recover in the well-conditioned case n_samples = 25 block_size = n_relevant_features rng = np . random . RandomState( 42 ) # The coefficients of our model coef = np . zeros(n_features) coef[:n_relevant_features] = coef_min + rng . rand(n_relevant_features) # The correlation of our design: variables correlated by blocs of 3 corr = np . zeros((n_features, n_features)) for i in range ( 0 , n_features, block_size): corr[i:i + block_size, i:i + block_size] = 1 - conditioning corr . flat[::n_features + 1 ] = 1 corr = linalg . cholesky(corr) # Our design X = rng . normal(size = (n_samples, n_features)) X = np . dot(X, corr) # Keep [Wainwright2006] (26c) constant X[:n_relevant_features] /= np . abs( linalg . svdvals(X[:n_relevant_features])) . max() X = StandardScaler() . fit_transform(X . copy()) # The output variable y = np . dot(X, coef) y /= np . std(y) # We scale the added noise as a function of the average correlation # between the design and the output variable y += noise_level * rng . normal(size = n_samples) mi = mutual_incoherence(X[:, :n_relevant_features], X[:, n_relevant_features:]) ######################################### # Plot stability selection path, using a high eps for early stopping # of the path, to save computation time alpha_grid, scores_path = lasso_stability_path(X, y, random_state = 42 , eps = 0.05 ) plt . figure() # We plot the path as a function of alpha/alpha_max to the power 1/3: the # power 1/3 scales the path less brutally than the log, and enables to # see the progression along the path hg = plt . plot(alpha_grid[ 1 :] ** . 333 , scores_path[coef != 0 ] . T[ 1 :], 'r' ) hb = plt . plot(alpha_grid[ 1 :] ** . 333 , scores_path[coef == 0 ] . T[ 1 :], 'k' ) ymin, ymax = plt . ylim() plt . xlabel( r'$(\\alpha / \\alpha_{max})^{1/3}$' ) plt . ylabel( 'Stability score: proportion of times selected' ) plt . title( 'Stability Scores Path - Mutual incoherence: %.1f ' % mi) plt . axis( 'tight' ) plt . legend((hg[ 0 ], hb[ 0 ]), ( 'relevant features' , 'irrelevant features' ), loc = 'best' ) ####################################### # Plot the estimated stability scores for a given alpha # Use 6-fold cross-validation rather than the default 3-fold: it leads to # a better choice of alpha: # Stop the user warnings outputs- they are not necessary for the example # as it is specifically set up to be challenging. with warnings . catch_warnings(): warnings . simplefilter( 'ignore' , UserWarning ) warnings . simplefilter( 'ignore' , ConvergenceWarning) lars_cv = LassoLarsCV(cv = 6 ) . fit(X, y) # Run the RandomizedLasso: we use a paths going down to .1*alpha_max # to avoid exploring the regime in which very noisy variables enter # the model alphas = np . linspace(lars_cv . alphas_[ 0 ], . 1 * lars_cv . alphas_[ 0 ], 6 ) clf = RandomizedLasso(alpha = alphas, random_state = 42 ) . fit(X, y) trees = ExtraTreesRegressor( 100 ) . fit(X, y) # Compare with F-score F, _ = f_regression(X, y) plt . figure() for name, score in [( 'F-test' , F), ( 'Stability selection' , clf . scores_), ( 'Lasso coefs' , np . abs(lars_cv . coef_)), ( 'Trees' , trees . feature_importances_), ]: precision, recall, thresholds = precision_recall_curve(coef != 0 , score) plt . semilogy(np . maximum(score / np . max(score), 1e-4 ), label = \" %s . AUC: %.3f \" % (name, auc(recall, precision))) plt . plot(np . where(coef != 0 )[ 0 ], [ 2e-4 ] * n_relevant_features, 'mo' , label = \"Ground truth\" ) plt . xlabel( \"Features\" ) plt . ylabel( \"Score\" ) # Plot only the 100 first coefficients plt . xlim( 0 , 100 ) plt . legend(loc = 'best' ) plt . title( 'Feature selection scores - Mutual incoherence: %.1f ' % mi) plt . show() 31. Theil-Sen Regression ( source ) Computes a Theil-Sen Regression on a synthetic dataset. See :ref: theil_sen_regression for more information on the regressor. Compared to the OLS (ordinary least squares) estimator, the Theil-Sen estimator is robust against outliers. It has a breakdown point of about 29.3% in case of a simple linear regression which means that it can tolerate arbitrary corrupted data (outliers) of up to 29.3% in the two-dimensional case. The estimation of the model is done by calculating the slopes and intercepts of a subpopulation of all possible combinations of p subsample points. If an intercept is fitted, p must be greater than or equal to n_features + 1. The final slope and intercept is then defined as the spatial median of these slopes and intercepts. In certain cases Theil-Sen performs better than :ref: RANSAC <ransac_regression> which is also a robust method. This is illustrated in the second example below where outliers with respect to the x-axis perturb RANSAC. Tuning the residual_threshold parameter of RANSAC remedies this but in general a priori knowledge about the data and the nature of the outliers is needed. Due to the computational complexity of Theil-Sen it is recommended to use it only for small problems in terms of number of samples and features. For larger problems the max_subpopulation parameter restricts the magnitude of all possible combinations of p subsample points to a randomly chosen subset and therefore also limits the runtime. Therefore, Theil-Sen is applicable to larger problems with the drawback of losing some of its mathematical properties since it then works on a random subset. import time import numpy as np import matplotlib.pyplot as plt from sklearn.linear_model import LinearRegression, TheilSenRegressor from sklearn.linear_model import RANSACRegressor print ( __doc__ ) estimators = [( 'OLS' , LinearRegression()), ( 'Theil-Sen' , TheilSenRegressor(random_state = 42 )), ( 'RANSAC' , RANSACRegressor(random_state = 42 )), ] colors = { 'OLS' : 'turquoise' , 'Theil-Sen' : 'gold' , 'RANSAC' : 'lightgreen' } lw = 2 ########################################### # Outliers only in the y direction np . random . seed( 0 ) n_samples = 200 # Linear model y = 3*x + N(2, 0.1**2) x = np . random . randn(n_samples) w = 3. c = 2. noise = 0.1 * np . random . randn(n_samples) y = w * x + c + noise # 10% outliers y[ - 20 :] += - 20 * x[ - 20 :] X = x[:, np . newaxis] plt . scatter(x, y, color = 'indigo' , marker = 'x' , s = 40 ) line_x = np . array([ - 3 , 3 ]) for name, estimator in estimators: t0 = time . time() estimator . fit(X, y) elapsed_time = time . time() - t0 y_pred = estimator . predict(line_x . reshape( 2 , 1 )) plt . plot(line_x, y_pred, color = colors[name], linewidth = lw, label = ' %s (fit time: %.2f s)' % (name, elapsed_time)) plt . axis( 'tight' ) plt . legend(loc = 'upper left' ) plt . title( \"Corrupt y\" ) ########################################## # Outliers in the X direction np . random . seed( 0 ) # Linear model y = 3*x + N(2, 0.1**2) x = np . random . randn(n_samples) noise = 0.1 * np . random . randn(n_samples) y = 3 * x + 2 + noise # 10% outliers x[ - 20 :] = 9.9 y[ - 20 :] += 22 X = x[:, np . newaxis] plt . figure() plt . scatter(x, y, color = 'indigo' , marker = 'x' , s = 40 ) line_x = np . array([ - 3 , 10 ]) for name, estimator in estimators: t0 = time . time() estimator . fit(X, y) elapsed_time = time . time() - t0 y_pred = estimator . predict(line_x . reshape( 2 , 1 )) plt . plot(line_x, y_pred, color = colors[name], linewidth = lw, label = ' %s (fit time: %.2f s)' % (name, elapsed_time)) plt . axis( 'tight' ) plt . legend(loc = 'upper left' ) plt . title( \"Corrupt x\" ) plt . show() Automatically created module for IPython interactive environment","title":"Regression"},{"location":"LinearModels/Linear-models/#linear-model","text":"","title":"Linear model"},{"location":"LinearModels/Linear-models/#introduction","text":"","title":"Introduction"},{"location":"LinearModels/Linear-models/#examples","text":"","title":"Examples"},{"location":"LinearModels/Linear-models/#1-linear-regression-example-source","text":"This example uses the only the first feature of the diabetes dataset, in order to illustrate a two-dimensional plot of this regression technique. The straight line can be seen in the plot, showing how linear regression attempts to draw a straight line that will best minimize the residual sum of squares between the observed responses in the dataset, and the responses predicted by the linear approximation. The coefficients, the residual sum of squares and the variance score are also calculated. import matplotlib.pyplot as plt import numpy as np from sklearn import datasets, linear_model # Load the diabetes dataset diabetes = datasets . load_diabetes() # Use only one feature diabetes_X = diabetes . data[:, np . newaxis, 2 ] # Split the data into training/testing sets diabetes_X_train = diabetes_X[: - 20 ] diabetes_X_test = diabetes_X[ - 20 :] # Split the targets into training/testing sets diabetes_y_train = diabetes . target[: - 20 ] diabetes_y_test = diabetes . target[ - 20 :] # Create linear regression object regr = linear_model . LinearRegression() # Train the model using the training sets regr . fit(diabetes_X_train, diabetes_y_train) # The coefficients print ( 'Coefficients: \\n ' , regr . coef_) # The mean squared error print ( \"Mean squared error: %.2f \" % np . mean((regr . predict(diabetes_X_test) - diabetes_y_test) ** 2 )) # Explained variance score: 1 is perfect prediction print ( 'Variance score: %.2f ' % regr . score(diabetes_X_test, diabetes_y_test)) # Plot outputs plt . scatter(diabetes_X_test, diabetes_y_test, color = 'black' ) plt . plot(diabetes_X_test, regr . predict(diabetes_X_test), color = 'blue' , linewidth = 3 ) plt . xticks(()) plt . yticks(()) plt . show() Coefficients: [ 938.23786125] Mean squared error: 2548.07 Variance score: 0.47","title":"1. Linear Regression Example (source)"},{"location":"LinearModels/Linear-models/#2-sparsity-example-fitting-only-features-1-and-2-source","text":"Features 1 and 2 of the diabetes-dataset are fitted and plotted below. It illustrates that although feature 2 has a strong coefficient on the full model, it does not give us much regarding y when compared to just feature 1 import matplotlib.pyplot as plt import numpy as np from mpl_toolkits.mplot3d import Axes3D from sklearn import datasets, linear_model diabetes = datasets . load_diabetes() indices = ( 0 , 1 ) X_train = diabetes . data[: - 20 , indices] X_test = diabetes . data[ - 20 :, indices] y_train = diabetes . target[: - 20 ] y_test = diabetes . target[ - 20 :] ols = linear_model . LinearRegression() ols . fit(X_train, y_train) ##################################### # Plot the figure def plot_figs (fig_num, elev, azim, X_train, clf): fig = plt . figure(fig_num, figsize = ( 4 , 3 )) plt . clf() ax = Axes3D(fig, elev = elev, azim = azim) ax . scatter(X_train[:, 0 ], X_train[:, 1 ], y_train, c = 'k' , marker = '+' ) ax . plot_surface(np . array([[ -. 1 , -. 1 ], [ . 15 , . 15 ]]), np . array([[ -. 1 , . 15 ], [ -. 1 , . 15 ]]), clf . predict(np . array([[ -. 1 , -. 1 , . 15 , . 15 ], [ -. 1 , . 15 , -. 1 , . 15 ]]) . T ) . reshape(( 2 , 2 )), alpha =. 5 ) ax . set_xlabel( 'X_1' ) ax . set_ylabel( 'X_2' ) ax . set_zlabel( 'Y' ) ax . w_xaxis . set_ticklabels([]) ax . w_yaxis . set_ticklabels([]) ax . w_zaxis . set_ticklabels([]) #Generate the three different figures from different views elev = 43.5 azim = - 110 plot_figs( 1 , elev, azim, X_train, ols) elev = -. 5 azim = 0 plot_figs( 2 , elev, azim, X_train, ols) elev = -. 5 azim = 90 plot_figs( 3 , elev, azim, X_train, ols) plt . show()","title":"2. Sparsity Example: Fitting only features 1  and 2 (source)"},{"location":"LinearModels/Linear-models/#3-ordinary-least-squares-and-ridge-regression-variance-source","text":"Due to the few points in each dimension and the straight line that linear regression uses to follow these points as well as it can, noise on the observations will cause great variance as shown in the first plot. Every line's slope can vary quite a bit for each prediction due to the noise induced in the observations. Ridge regression is basically minimizing a penalised version of the least-squared function. The penalising shrinks the value of the regression coefficients. Despite the few data points in each dimension, the slope of the prediction is much more stable and the variance in the line itself is greatly reduced, in comparison to that of the standard linear regression import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model X_train = np . c_[ . 5 , 1 ] . T y_train = [ . 5 , 1 ] X_test = np . c_[ 0 , 2 ] . T np . random . seed( 0 ) classifiers = dict (ols = linear_model . LinearRegression(), ridge = linear_model . Ridge(alpha =. 1 )) fignum = 1 for name, clf in classifiers . items(): fig = plt . figure(fignum, figsize = ( 4 , 3 )) plt . clf() plt . title(name) ax = plt . axes([ . 12 , . 12 , . 8 , . 8 ]) for _ in range ( 6 ): this_X = . 1 * np . random . normal(size = ( 2 , 1 )) + X_train clf . fit(this_X, y_train) ax . plot(X_test, clf . predict(X_test), color = '.5' ) ax . scatter(this_X, y_train, s = 3 , c = '.5' , marker = 'o' , zorder = 10 ) clf . fit(X_train, y_train) ax . plot(X_test, clf . predict(X_test), linewidth = 2 , color = 'blue' ) ax . scatter(X_train, y_train, s = 30 , c = 'r' , marker = '+' , zorder = 10 ) ax . set_xticks(()) ax . set_yticks(()) ax . set_ylim(( 0 , 1.6 )) ax . set_xlabel( 'X' ) ax . set_ylabel( 'y' ) ax . set_xlim( 0 , 2 ) fignum += 1 plt . show()","title":"3. Ordinary Least Squares and Ridge Regression Variance (source)"},{"location":"LinearModels/Linear-models/#4-plot-ridge-coefficients-as-a-function-of-the-l2-regularization-source","text":"Ridge Regression is the estimator used in this example. Each color in the left plot represents one different dimension of the coefficient vector, and this is displayed as a function of the regularization parameter. The right plot shows how exact the solution is. This example illustrates how a well defined solution is found by Ridge regression and how regularization affects the coefficients and their values. The plot on the right shows how the difference of the coefficients from the estimator changes as a function of regularization. In this example the dependent variable Y is set as a function of the input features: $y = X*w + c. $ The coefficient vector w is randomly sampled from a normal distribution, whereas the bias term c is set to a constant. As alpha tends toward zero the coefficients found by Ridge regression stabilize towards the randomly sampled vector w. For big alpha (strong regularisation) the coefficients are smaller (eventually converging at 0) leading to a simpler and biased solution. These dependencies can be observed on the left plot. The right plot shows the mean squared error between the coefficients found by the model and the chosen vector w. Less regularised models retrieve the exact coefficients (error is equal to 0), stronger regularised models increase the error. Please note that in this example the data is non-noisy, hence it is possible to extract the exact coefficients. import matplotlib.pyplot as plt import numpy as np from sklearn.datasets import make_regression from sklearn.linear_model import Ridge from sklearn.metrics import mean_squared_error clf = Ridge() X, y, w = make_regression(n_samples = 10 , n_features = 10 , coef = True , random_state = 1 , bias = 3.5 ) coefs = [] errors = [] alphas = np . logspace( - 6 , 6 , 200 ) # Train the model with different regularisation strengths for a in alphas: clf . set_params(alpha = a) clf . fit(X, y) coefs . append(clf . coef_) errors . append(mean_squared_error(clf . coef_, w)) # Display results plt . figure(figsize = ( 20 , 6 )) plt . subplot( 121 ) ax = plt . gca() ax . plot(alphas, coefs) ax . set_xscale( 'log' ) plt . xlabel( 'alpha' ) plt . ylabel( 'weights' ) plt . title( 'Ridge coefficients as a function of the regularization' ) plt . axis( 'tight' ) plt . subplot( 122 ) ax = plt . gca() ax . plot(alphas, errors) ax . set_xscale( 'log' ) plt . xlabel( 'alpha' ) plt . ylabel( 'error' ) plt . title( 'Coefficient error as a function of the regularization' ) plt . axis( 'tight' ) plt . show()","title":"4. Plot Ridge coefficients as a function of the L2 regularization (source)"},{"location":"LinearModels/Linear-models/#5-plot-ridge-coefficients-as-a-function-of-the-regularization-source","text":"Shows the effect of collinearity in the coefficients of an estimator. Regression is the estimator used in this example. Each color represents a different feature of the coefficient vector, and this is displayed as a function of the regularization parameter. This example also shows the usefulness of applying Ridge regression to highly ill-conditioned matrices. For such matrices, a slight change in the target variable can cause huge variances in the calculated weights. In such cases, it is useful to set a certain regularization (alpha) to reduce this variation (noise). When alpha is very large, the regularization effect dominates the squared loss function and the coefficients tend to zero. At the end of the path, as alpha tends toward zero and the solution tends towards the ordinary least squares, coefficients exhibit big oscillations. In practise it is necessary to tune alpha in such a way that a balance is maintained between both. import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model # X is the 10x10 Hilbert matrix X = 1. / (np . arange( 1 , 11 ) + np . arange( 0 , 10 )[:, np . newaxis]) y = np . ones( 10 ) ######################################### # Compute paths n_alphas = 200 alphas = np . logspace( - 10 , - 2 , n_alphas) clf = linear_model . Ridge(fit_intercept = False ) coefs = [] for a in alphas: clf . set_params(alpha = a) clf . fit(X, y) coefs . append(clf . coef_) ######################################## # Display results ax = plt . gca() ax . plot(alphas, coefs) ax . set_xscale( 'log' ) ax . set_xlim(ax . get_xlim()[:: - 1 ]) # reverse axis plt . xlabel( 'alpha' ) plt . ylabel( 'weights' ) plt . title( 'Ridge coefficients as a function of the regularization' ) plt . axis( 'tight' ) plt . show()","title":"5. Plot Ridge coefficients as a function of the regularization (source)"},{"location":"LinearModels/Linear-models/#6-bayesian-ridge-regression-source","text":"Computes a Bayesian Ridge Regression on a synthetic dataset. bayesian_ridge_regression for more information on the regressor. Compared to the OLS (ordinary least squares) estimator, the coefficient weights are slightly shifted toward zeros, which stabilises them. As the prior on the weights is a Gaussian prior, the histogram of the estimated weights is Gaussian. The estimation of the model is done by iteratively maximizing the marginal log-likelihood of the observations. We also plot predictions and uncertainties for Bayesian Ridge Regression for one dimensional regression using polynomial feature expansion. Note the uncertainty starts going up on the right side of the plot. This is because these test samples are outside of the range of the training samples. import numpy as np import matplotlib.pyplot as plt from scipy import stats % matplotlib inline from sklearn.linear_model import BayesianRidge, LinearRegression ################################################# # Generating simulated data with Gaussian weights np . random . seed( 0 ) n_samples, n_features = 100 , 100 X = np . random . randn(n_samples, n_features) # Create Gaussian data # Create weights with a precision lambda_ of 4. lambda_ = 4. w = np . zeros(n_features) # Only keep 10 weights of interest relevant_features = np . random . randint( 0 , n_features, 10 ) for i in relevant_features: w[i] = stats . norm . rvs(loc = 0 , scale = 1. / np . sqrt(lambda_)) # Create noise with a precision alpha of 50. alpha_ = 50. noise = stats . norm . rvs(loc = 0 , scale = 1. / np . sqrt(alpha_), size = n_samples) # Create the target y = np . dot(X, w) + noise ############################################ # Fit the Bayesian Ridge Regression and an OLS for comparison clf = BayesianRidge(compute_score = True ) clf . fit(X, y) ols = LinearRegression() ols . fit(X, y) ################################################ # Plot true weights, estimated weights, histogram of the weights, and # predictions with standard deviations lw = 2 plt . figure(figsize = ( 6 , 5 )) plt . title( \"Weights of the model\" ) plt . plot(clf . coef_, color = 'lightgreen' , linewidth = lw, label = \"Bayesian Ridge estimate\" ) plt . plot(w, color = 'gold' , linewidth = lw, label = \"Ground truth\" ) plt . plot(ols . coef_, color = 'navy' , linestyle = '--' , label = \"OLS estimate\" ) plt . xlabel( \"Features\" ) plt . ylabel( \"Values of the weights\" ) plt . legend(loc = \"best\" , prop = dict (size = 12 )) plt . figure(figsize = ( 6 , 5 )) plt . title( \"Histogram of the weights\" ) plt . hist(clf . coef_, bins = n_features, color = 'gold' , log = True ) plt . scatter(clf . coef_[relevant_features], 5 * np . ones( len (relevant_features)), color = 'navy' , label = \"Relevant features\" ) plt . ylabel( \"Features\" ) plt . xlabel( \"Values of the weights\" ) plt . legend(loc = \"upper left\" ) plt . figure(figsize = ( 6 , 5 )) plt . title( \"Marginal log-likelihood\" ) plt . plot(clf . scores_, color = 'navy' , linewidth = lw) plt . ylabel( \"Score\" ) plt . xlabel( \"Iterations\" ) # Plotting some predictions for polynomial regression def f (x, noise_amount): y = np . sqrt(x) * np . sin(x) noise = np . random . normal( 0 , 1 , len (x)) return y + noise_amount * noise degree = 10 X = np . linspace( 0 , 10 , 100 ) y = f(X, noise_amount = 0.1 ) clf_poly = BayesianRidge() clf_poly . fit(np . vander(X, degree), y) X_plot = np . linspace( 0 , 11 , 25 ) y_plot = f(X_plot, noise_amount = 0 ) y_mean, y_std = clf_poly . predict(np . vander(X_plot, degree), return_std = True ) plt . figure(figsize = ( 6 , 5 )) plt . errorbar(X_plot, y_mean, y_std, color = 'navy' , label = \"Polynomial Bayesian Ridge Regression\" , linewidth = lw) plt . plot(X_plot, y_plot, color = 'gold' , linewidth = lw, label = \"Ground Truth\" ) plt . ylabel( \"Output y\" ) plt . xlabel( \"Feature X\" ) plt . legend(loc = \"lower left\" ) plt . show()","title":"6. Bayesian Ridge Regression (source)"},{"location":"LinearModels/Linear-models/#7-logistic-regression-3-class-classifier-source","text":"Show below is a logistic-regression classifiers decision boundaries on the iris <https://en.wikipedia.org/wiki/Iris_flower_data_set> _ dataset. The datapoints are colored according to their labels. import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model, datasets # import some data to play with iris = datasets . load_iris() X = iris . data[:, : 2 ] # we only take the first two features. Y = iris . target h = . 02 # step size in the mesh logreg = linear_model . LogisticRegression(C = 1e5 ) # we create an instance of Neighbours Classifier and fit the data. logreg . fit(X, Y) # Plot the decision boundary. For that, we will assign a color to each # point in the mesh [x_min, x_max]x[y_min, y_max]. x_min, x_max = X[:, 0 ] . min() - . 5 , X[:, 0 ] . max() + . 5 y_min, y_max = X[:, 1 ] . min() - . 5 , X[:, 1 ] . max() + . 5 xx, yy = np . meshgrid(np . arange(x_min, x_max, h), np . arange(y_min, y_max, h)) Z = logreg . predict(np . c_[xx . ravel(), yy . ravel()]) # Put the result into a color plot Z = Z . reshape(xx . shape) plt . figure( 1 , figsize = ( 4 , 3 )) plt . pcolormesh(xx, yy, Z, cmap = plt . cm . Paired) # Plot also the training points plt . scatter(X[:, 0 ], X[:, 1 ], c = Y, edgecolors = 'k' , cmap = plt . cm . Paired) plt . xlabel( 'Sepal length' ) plt . ylabel( 'Sepal width' ) plt . xlim(xx . min(), xx . max()) plt . ylim(yy . min(), yy . max()) plt . xticks(()) plt . yticks(()) plt . show()","title":"7. Logistic Regression 3-class Classifier (source)"},{"location":"LinearModels/Linear-models/#8-l1-penalty-and-sparsity-in-logistic-regression-source","text":"Comparison of the sparsity (percentage of zero coefficients) of solutions when L1 and L2 penalty are used for different values of C. We can see that large values of C give more freedom to the model. Conversely, smaller values of C constrain the model more. In the L1 penalty case, this leads to sparser solutions. We classify 8x8 images of digits into two classes: 0-4 against 5-9. The visualization shows coefficients of the models for varying C. import numpy as np import matplotlib.pyplot as plt from sklearn.linear_model import LogisticRegression from sklearn import datasets from sklearn.preprocessing import StandardScaler digits = datasets . load_digits() X, y = digits . data, digits . target X = StandardScaler() . fit_transform(X) # classify small against large digits y = (y > 4 ) . astype(np . int) # Set regularization parameter for i, C in enumerate (( 100 , 1 , 0.01 )): # turn down tolerance for short training time clf_l1_LR = LogisticRegression(C = C, penalty = 'l1' , tol = 0.01 ) clf_l2_LR = LogisticRegression(C = C, penalty = 'l2' , tol = 0.01 ) clf_l1_LR . fit(X, y) clf_l2_LR . fit(X, y) coef_l1_LR = clf_l1_LR . coef_ . ravel() coef_l2_LR = clf_l2_LR . coef_ . ravel() # coef_l1_LR contains zeros due to the # L1 sparsity inducing norm sparsity_l1_LR = np . mean(coef_l1_LR == 0 ) * 100 sparsity_l2_LR = np . mean(coef_l2_LR == 0 ) * 100 print ( \"C= %.2f \" % C) print ( \"Sparsity with L1 penalty: %.2f%% \" % sparsity_l1_LR) print ( \"score with L1 penalty: %.4f \" % clf_l1_LR . score(X, y)) print ( \"Sparsity with L2 penalty: %.2f%% \" % sparsity_l2_LR) print ( \"score with L2 penalty: %.4f \" % clf_l2_LR . score(X, y)) l1_plot = plt . subplot( 3 , 2 , 2 * i + 1 ) l2_plot = plt . subplot( 3 , 2 , 2 * (i + 1 )) if i == 0 : l1_plot . set_title( \"L1 penalty\" ) l2_plot . set_title( \"L2 penalty\" ) l1_plot . imshow(np . abs(coef_l1_LR . reshape( 8 , 8 )), interpolation = 'nearest' , cmap = 'binary' , vmax = 1 , vmin = 0 ) l2_plot . imshow(np . abs(coef_l2_LR . reshape( 8 , 8 )), interpolation = 'nearest' , cmap = 'binary' , vmax = 1 , vmin = 0 ) plt . text( - 8 , 3 , \"C = %.2f \" % C) l1_plot . set_xticks(()) l1_plot . set_yticks(()) l2_plot . set_xticks(()) l2_plot . set_yticks(()) plt . show() C=100.00 Sparsity with L1 penalty: 4.69% score with L1 penalty: 0.9093 Sparsity with L2 penalty: 4.69% score with L2 penalty: 0.9098 C=1.00 Sparsity with L1 penalty: 9.38% score with L1 penalty: 0.9098 Sparsity with L2 penalty: 4.69% score with L2 penalty: 0.9093 C=0.01 Sparsity with L1 penalty: 85.94% score with L1 penalty: 0.8625 Sparsity with L2 penalty: 4.69% score with L2 penalty: 0.8915","title":"8. L1 Penalty and Sparsity in Logistic Regression (source)"},{"location":"LinearModels/Linear-models/#9-path-with-l1-logistic-regression-source","text":"Computes path on IRIS dataset. from datetime import datetime import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model from sklearn import datasets from sklearn.svm import l1_min_c iris = datasets . load_iris() X = iris . data y = iris . target X = X[y != 2 ] y = y[y != 2 ] X -= np . mean(X, 0 ) #################################### # Demo path functions cs = l1_min_c(X, y, loss = 'log' ) * np . logspace( 0 , 3 ) print ( \"Computing regularization path ...\" ) start = datetime . now() clf = linear_model . LogisticRegression(C = 1.0 , penalty = 'l1' , tol = 1e-6 ) coefs_ = [] for c in cs: clf . set_params(C = c) clf . fit(X, y) coefs_ . append(clf . coef_ . ravel() . copy()) print ( \"This took \" , datetime . now() - start) coefs_ = np . array(coefs_) plt . plot(np . log10(cs), coefs_) ymin, ymax = plt . ylim() plt . xlabel( 'log(C)' ) plt . ylabel( 'Coefficients' ) plt . title( 'Logistic Regression Path' ) plt . axis( 'tight' ) plt . show() Computing regularization path ... This took 0:00:00.036331","title":"9. Path with L1- Logistic Regression (source)"},{"location":"LinearModels/Linear-models/#10-logistic-function-source","text":"Shown in the plot is how the logistic regression would, in this synthetic dataset, classify values as either 0 or 1, i.e. class one or two, using the logistic curve. import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model # this is our test set, it's just a straight line with some # Gaussian noise xmin, xmax = - 5 , 5 n_samples = 100 np . random . seed( 0 ) X = np . random . normal(size = n_samples) y = (X > 0 ) . astype(np . float) X[X > 0 ] *= 4 X += . 3 * np . random . normal(size = n_samples) X = X[:, np . newaxis] # run the classifier clf = linear_model . LogisticRegression(C = 1e5 ) clf . fit(X, y) # and plot the result plt . figure( 1 , figsize = ( 4 , 3 )) plt . clf() plt . scatter(X . ravel(), y, color = 'black' , zorder = 20 ) X_test = np . linspace( - 5 , 10 , 300 ) def model (x): return 1 / ( 1 + np . exp( - x)) loss = model(X_test * clf . coef_ + clf . intercept_) . ravel() plt . plot(X_test, loss, color = 'red' , linewidth = 3 ) ols = linear_model . LinearRegression() ols . fit(X, y) plt . plot(X_test, ols . coef_ * X_test + ols . intercept_, linewidth = 1 ) plt . axhline( . 5 , color = '.5' ) plt . ylabel( 'y' ) plt . xlabel( 'X' ) plt . xticks( range ( - 5 , 10 )) plt . yticks([ 0 , 0.5 , 1 ]) plt . ylim( -. 25 , 1.25 ) plt . xlim( - 4 , 10 ) plt . legend(( 'Logistic Regression Model' , 'Linear Regression Model' ), loc = \"lower right\" , fontsize = 'small' ) plt . show()","title":"10. Logistic function (source)"},{"location":"LinearModels/Linear-models/#11-plot-multinomial-and-one-vs-rest-logistic-regression-source","text":"Plot decision surface of multinomial and One-vs-Rest Logistic Regression. The hyperplanes corresponding to the three One-vs-Rest (OVR) classifiers are represented by the dashed lines. import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_blobs from sklearn.linear_model import LogisticRegression # make 3-class dataset for classification centers = [[ - 5 , 0 ], [ 0 , 1.5 ], [ 5 , - 1 ]] X, y = make_blobs(n_samples = 1000 , centers = centers, random_state = 40 ) transformation = [[ 0.4 , 0.2 ], [ - 0.4 , 1.2 ]] X = np . dot(X, transformation) for multi_class in ( 'multinomial' , 'ovr' ): clf = LogisticRegression(solver = 'sag' , max_iter = 100 , random_state = 42 , multi_class = multi_class) . fit(X, y) # print the training scores print ( \"training score : %.3f ( %s )\" % (clf . score(X, y), multi_class)) # create a mesh to plot in h = . 02 # step size in the mesh x_min, x_max = X[:, 0 ] . min() - 1 , X[:, 0 ] . max() + 1 y_min, y_max = X[:, 1 ] . min() - 1 , X[:, 1 ] . max() + 1 xx, yy = np . meshgrid(np . arange(x_min, x_max, h), np . arange(y_min, y_max, h)) # Plot the decision boundary. For that, we will assign a color to each # point in the mesh [x_min, x_max]x[y_min, y_max]. Z = clf . predict(np . c_[xx . ravel(), yy . ravel()]) # Put the result into a color plot Z = Z . reshape(xx . shape) plt . figure() plt . contourf(xx, yy, Z, cmap = plt . cm . Paired) plt . title( \"Decision surface of LogisticRegression ( %s )\" % multi_class) plt . axis( 'tight' ) # Plot also the training points colors = \"bry\" for i, color in zip (clf . classes_, colors): idx = np . where(y == i) plt . scatter(X[idx, 0 ], X[idx, 1 ], c = color, cmap = plt . cm . Paired) # Plot the three one-against-all classifiers xmin, xmax = plt . xlim() ymin, ymax = plt . ylim() coef = clf . coef_ intercept = clf . intercept_ def plot_hyperplane (c, color): def line (x0): return ( - (x0 * coef[c, 0 ]) - intercept[c]) / coef[c, 1 ] plt . plot([xmin, xmax], [line(xmin), line(xmax)], ls = \"--\" , color = color) for i, color in zip (clf . classes_, colors): plot_hyperplane(i, color) plt . show() training score : 0.995 (multinomial) training score : 0.976 (ovr)","title":"11. Plot multinomial and One-vs-Rest Logistic Regression (source)"},{"location":"LinearModels/Linear-models/#12-lasso-path-using-lars-source","text":"Computes Lasso Path along the regularization parameter using the LARS algorithm on the diabetes dataset. Each color represents a different feature of the coefficient vector, and this is displayed as a function of the regularization parameter. import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model from sklearn import datasets diabetes = datasets . load_diabetes() X = diabetes . data y = diabetes . target print ( \"Computing regularization path using the LARS ...\" ) alphas, _, coefs = linear_model . lars_path(X, y, method = 'lasso' , verbose = True ) xx = np . sum(np . abs(coefs . T), axis = 1 ) xx /= xx[ - 1 ] plt . plot(xx, coefs . T) ymin, ymax = plt . ylim() plt . vlines(xx, ymin, ymax, linestyle = 'dashed' ) plt . xlabel( '|coef| / max|coef|' ) plt . ylabel( 'Coefficients' ) plt . title( 'LASSO Path' ) plt . axis( 'tight' ) plt . show() Computing regularization path using the LARS ... .","title":"12. Lasso path using LARS (source)"},{"location":"LinearModels/Linear-models/#13-lasso-and-elastic-net-source","text":"Lasso and elastic net (L1 and L2 penalisation) implemented using a coordinate descent. The coefficients can be forced to be positive. from itertools import cycle import numpy as np import matplotlib.pyplot as plt from sklearn.linear_model import lasso_path, enet_path from sklearn import datasets diabetes = datasets . load_diabetes() X = diabetes . data y = diabetes . target X /= X . std(axis = 0 ) # Standardize data (easier to set the l1_ratio parameter) # Compute paths eps = 5e-3 # the smaller it is the longer is the path print ( \"Computing regularization path using the lasso...\" ) alphas_lasso, coefs_lasso, _ = lasso_path(X, y, eps, fit_intercept = False ) print ( \"Computing regularization path using the positive lasso...\" ) alphas_positive_lasso, coefs_positive_lasso, _ = lasso_path( X, y, eps, positive = True , fit_intercept = False ) print ( \"Computing regularization path using the elastic net...\" ) alphas_enet, coefs_enet, _ = enet_path( X, y, eps = eps, l1_ratio = 0.8 , fit_intercept = False ) print ( \"Computing regularization path using the positive elastic net...\" ) alphas_positive_enet, coefs_positive_enet, _ = enet_path( X, y, eps = eps, l1_ratio = 0.8 , positive = True , fit_intercept = False ) # Display results plt . figure( 1 ) ax = plt . gca() colors = cycle([ 'b' , 'r' , 'g' , 'c' , 'k' ]) neg_log_alphas_lasso = - np . log10(alphas_lasso) neg_log_alphas_enet = - np . log10(alphas_enet) for coef_l, coef_e, c in zip (coefs_lasso, coefs_enet, colors): l1 = plt . plot(neg_log_alphas_lasso, coef_l, c = c) l2 = plt . plot(neg_log_alphas_enet, coef_e, linestyle = '--' , c = c) plt . xlabel( '-Log(alpha)' ) plt . ylabel( 'coefficients' ) plt . title( 'Lasso and Elastic-Net Paths' ) plt . legend((l1[ - 1 ], l2[ - 1 ]), ( 'Lasso' , 'Elastic-Net' ), loc = 'lower left' ) plt . axis( 'tight' ) plt . figure( 2 ) ax = plt . gca() neg_log_alphas_positive_lasso = - np . log10(alphas_positive_lasso) for coef_l, coef_pl, c in zip (coefs_lasso, coefs_positive_lasso, colors): l1 = plt . plot(neg_log_alphas_lasso, coef_l, c = c) l2 = plt . plot(neg_log_alphas_positive_lasso, coef_pl, linestyle = '--' , c = c) plt . xlabel( '-Log(alpha)' ) plt . ylabel( 'coefficients' ) plt . title( 'Lasso and positive Lasso' ) plt . legend((l1[ - 1 ], l2[ - 1 ]), ( 'Lasso' , 'positive Lasso' ), loc = 'lower left' ) plt . axis( 'tight' ) plt . figure( 3 ) ax = plt . gca() neg_log_alphas_positive_enet = - np . log10(alphas_positive_enet) for (coef_e, coef_pe, c) in zip (coefs_enet, coefs_positive_enet, colors): l1 = plt . plot(neg_log_alphas_enet, coef_e, c = c) l2 = plt . plot(neg_log_alphas_positive_enet, coef_pe, linestyle = '--' , c = c) plt . xlabel( '-Log(alpha)' ) plt . ylabel( 'coefficients' ) plt . title( 'Elastic-Net and positive Elastic-Net' ) plt . legend((l1[ - 1 ], l2[ - 1 ]), ( 'Elastic-Net' , 'positive Elastic-Net' ), loc = 'lower left' ) plt . axis( 'tight' ) plt . show() Computing regularization path using the lasso... Computing regularization path using the positive lasso... Computing regularization path using the elastic net... Computing regularization path using the positive elastic net...","title":"13. Lasso and Elastic Net (source)"},{"location":"LinearModels/Linear-models/#14-lasso-model-selection-cross-validation-aic-bic-source","text":"Use the Akaike information criterion (AIC), the Bayes Information criterion (BIC) and cross-validation to select an optimal value of the regularization parameter alpha of the :ref: lasso estimator. Results obtained with LassoLarsIC are based on AIC/BIC criteria. Information-criterion based model selection is very fast, but it relies on a proper estimation of degrees of freedom, are derived for large samples (asymptotic results) and assume the model is correct, i.e. that the data are actually generated by this model. They also tend to break when the problem is badly conditioned (more features than samples). For cross-validation, we use 20-fold with 2 algorithms to compute the Lasso path: coordinate descent, as implemented by the LassoCV class, and Lars (least angle regression) as implemented by the LassoLarsCV class. Both algorithms give roughly the same results. They differ with regards to their execution speed and sources of numerical errors. Lars computes a path solution only for each kink in the path. As a result, it is very efficient when there are only of few kinks, which is the case if there are few features or samples. Also, it is able to compute the full path without setting any meta parameter. On the opposite, coordinate descent compute the path points on a pre-specified grid (here we use the default). Thus it is more efficient if the number of grid points is smaller than the number of kinks in the path. Such a strategy can be interesting if the number of features is really large and there are enough samples to select a large amount. In terms of numerical errors, for heavily correlated variables, Lars will accumulate more errors, while the coordinate descent algorithm will only sample the path on a grid. Note how the optimal value of alpha varies for each fold. This illustrates why nested-cross validation is necessary when trying to evaluate the performance of a method for which a parameter is chosen by cross-validation: this choice of parameter may not be optimal for unseen data. import time import numpy as np import matplotlib.pyplot as plt from sklearn.linear_model import LassoCV, LassoLarsCV, LassoLarsIC from sklearn import datasets diabetes = datasets . load_diabetes() X = diabetes . data y = diabetes . target rng = np . random . RandomState( 42 ) X = np . c_[X, rng . randn(X . shape[ 0 ], 14 )] # add some bad features # normalize data as done by Lars to allow for comparison X /= np . sqrt(np . sum(X ** 2 , axis = 0 )) ######################################## # LassoLarsIC: least angle regression with BIC/AIC criterion model_bic = LassoLarsIC(criterion = 'bic' ) t1 = time . time() model_bic . fit(X, y) t_bic = time . time() - t1 alpha_bic_ = model_bic . alpha_ model_aic = LassoLarsIC(criterion = 'aic' ) model_aic . fit(X, y) alpha_aic_ = model_aic . alpha_ def plot_ic_criterion (model, name, color): alpha_ = model . alpha_ alphas_ = model . alphas_ criterion_ = model . criterion_ plt . plot( - np . log10(alphas_), criterion_, '--' , color = color, linewidth = 3 , label = ' %s criterion' % name) plt . axvline( - np . log10(alpha_), color = color, linewidth = 3 , label = 'alpha: %s estimate' % name) plt . xlabel( '-log(alpha)' ) plt . ylabel( 'criterion' ) plt . figure() plot_ic_criterion(model_aic, 'AIC' , 'b' ) plot_ic_criterion(model_bic, 'BIC' , 'r' ) plt . legend() plt . title( 'Information-criterion for model selection (training time %.3f s)' % t_bic) ############################################## # LassoCV: coordinate descent # Compute paths print ( \"Computing regularization path using the coordinate descent lasso...\" ) t1 = time . time() model = LassoCV(cv = 20 ) . fit(X, y) t_lasso_cv = time . time() - t1 # Display results m_log_alphas = - np . log10(model . alphas_) plt . figure() ymin, ymax = 2300 , 3800 plt . plot(m_log_alphas, model . mse_path_, ':' ) plt . plot(m_log_alphas, model . mse_path_ . mean(axis =- 1 ), 'k' , label = 'Average across the folds' , linewidth = 2 ) plt . axvline( - np . log10(model . alpha_), linestyle = '--' , color = 'k' , label = 'alpha: CV estimate' ) plt . legend() plt . xlabel( '-log(alpha)' ) plt . ylabel( 'Mean square error' ) plt . title( 'Mean square error on each fold: coordinate descent ' '(train time: %.2f s)' % t_lasso_cv) plt . axis( 'tight' ) plt . ylim(ymin, ymax) ###################################### # LassoLarsCV: least angle regression # Compute paths print ( \"Computing regularization path using the Lars lasso...\" ) t1 = time . time() model = LassoLarsCV(cv = 20 ) . fit(X, y) t_lasso_lars_cv = time . time() - t1 # Display results m_log_alphas = - np . log10(model . cv_alphas_) plt . figure() plt . plot(m_log_alphas, model . mse_path_, ':' ) plt . plot(m_log_alphas, model . mse_path_ . mean(axis =- 1 ), 'k' , label = 'Average across the folds' , linewidth = 2 ) plt . axvline( - np . log10(model . alpha_), linestyle = '--' , color = 'k' , label = 'alpha CV' ) plt . legend() plt . xlabel( '-log(alpha)' ) plt . ylabel( 'Mean square error' ) plt . title( 'Mean square error on each fold: Lars (train time: %.2f s)' % t_lasso_lars_cv) plt . axis( 'tight' ) plt . ylim(ymin, ymax) plt . show() /home/bitnami/anaconda3/envs/caseolap/lib/python3.6/site-packages/ipykernel_launcher.py:37: RuntimeWarning: divide by zero encountered in log10 Computing regularization path using the coordinate descent lasso... Computing regularization path using the Lars lasso... /home/bitnami/anaconda3/envs/caseolap/lib/python3.6/site-packages/ipykernel_launcher.py:90: RuntimeWarning: divide by zero encountered in log10","title":"14. Lasso model selection: Cross-Validation / AIC / BIC (source)"},{"location":"LinearModels/Linear-models/#15-lasso-and-elastic-net-for-sparse-signals-source","text":"Estimates Lasso and Elastic-Net regression models on a manually generated sparse signal corrupted with an additive noise. Estimated coefficients are compared with the ground-truth. import numpy as np import matplotlib.pyplot as plt from sklearn.metrics import r2_score ######################################### # generate some sparse data to play with np . random . seed( 42 ) n_samples, n_features = 50 , 200 X = np . random . randn(n_samples, n_features) coef = 3 * np . random . randn(n_features) inds = np . arange(n_features) np . random . shuffle(inds) coef[inds[ 10 :]] = 0 # sparsify coef y = np . dot(X, coef) # add noise y += 0.01 * np . random . normal((n_samples,)) # Split data in train set and test set n_samples = X . shape[ 0 ] X_train, y_train = X[:n_samples / 2 ], y[:n_samples / 2 ] X_test, y_test = X[n_samples / 2 :], y[n_samples / 2 :] ############################################### # Lasso from sklearn.linear_model import Lasso alpha = 0.1 lasso = Lasso(alpha = alpha) y_pred_lasso = lasso . fit(X_train, y_train) . predict(X_test) r2_score_lasso = r2_score(y_test, y_pred_lasso) print (lasso) print ( \"r^2 on test data : %f \" % r2_score_lasso) ############################################ # ElasticNet from sklearn.linear_model import ElasticNet enet = ElasticNet(alpha = alpha, l1_ratio = 0.7 ) y_pred_enet = enet . fit(X_train, y_train) . predict(X_test) r2_score_enet = r2_score(y_test, y_pred_enet) print (enet) print ( \"r^2 on test data : %f \" % r2_score_enet) plt . plot(enet . coef_, color = 'lightgreen' , linewidth = 2 , label = 'Elastic net coefficients' ) plt . plot(lasso . coef_, color = 'gold' , linewidth = 2 , label = 'Lasso coefficients' ) plt . plot(coef, '--' , color = 'navy' , label = 'original coefficients' ) plt . legend(loc = 'best' ) plt . title( \"Lasso R^2: %f , Elastic Net R^2: %f \" % (r2_score_lasso, r2_score_enet)) plt . show() /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:23: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future /Users/dibakarsigdel/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:24: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000, normalize=False, positive=False, precompute=False, random_state=None, selection='cyclic', tol=0.0001, warm_start=False) r^2 on test data : 0.384710 ElasticNet(alpha=0.1, copy_X=True, fit_intercept=True, l1_ratio=0.7, max_iter=1000, normalize=False, positive=False, precompute=False, random_state=None, selection='cyclic', tol=0.0001, warm_start=False) r^2 on test data : 0.240176","title":"15. Lasso and Elastic Net for Sparse Signals (source)"},{"location":"LinearModels/Linear-models/#16-joint-feature-selection-with-multi-task-lasso-source","text":"The multi-task lasso allows to fit multiple regression problems jointly enforcing the selected features to be the same across tasks. This example simulates sequential measurements, each task is a time instant, and the relevant features vary in amplitude over time while being the same. The multi-task lasso imposes that features that are selected at one time point are select for all time point. This makes feature selection by the Lasso more stable. import matplotlib.pyplot as plt import numpy as np from sklearn.linear_model import MultiTaskLasso, Lasso rng = np . random . RandomState( 42 ) # Generate some 2D coefficients with sine waves with random frequency and phase n_samples, n_features, n_tasks = 100 , 30 , 40 n_relevant_features = 5 coef = np . zeros((n_tasks, n_features)) times = np . linspace( 0 , 2 * np . pi, n_tasks) for k in range (n_relevant_features): coef[:, k] = np . sin(( 1. + rng . randn( 1 )) * times + 3 * rng . randn( 1 )) X = rng . randn(n_samples, n_features) Y = np . dot(X, coef . T) + rng . randn(n_samples, n_tasks) coef_lasso_ = np . array([Lasso(alpha = 0.5 ) . fit(X, y) . coef_ for y in Y . T]) coef_multi_task_lasso_ = MultiTaskLasso(alpha = 1. ) . fit(X, Y) . coef_ ############################################# # Plot support and time series fig = plt . figure(figsize = ( 8 , 5 )) plt . subplot( 1 , 2 , 1 ) plt . spy(coef_lasso_) plt . xlabel( 'Feature' ) plt . ylabel( 'Time (or Task)' ) plt . text( 10 , 5 , 'Lasso' ) plt . subplot( 1 , 2 , 2 ) plt . spy(coef_multi_task_lasso_) plt . xlabel( 'Feature' ) plt . ylabel( 'Time (or Task)' ) plt . text( 10 , 5 , 'MultiTaskLasso' ) fig . suptitle( 'Coefficient non-zero location' ) feature_to_plot = 0 plt . figure() lw = 2 plt . plot(coef[:, feature_to_plot], color = 'seagreen' , linewidth = lw, label = 'Ground truth' ) plt . plot(coef_lasso_[:, feature_to_plot], color = 'cornflowerblue' , linewidth = lw, label = 'Lasso' ) plt . plot(coef_multi_task_lasso_[:, feature_to_plot], color = 'gold' , linewidth = lw, label = 'MultiTaskLasso' ) plt . legend(loc = 'upper center' ) plt . axis( 'tight' ) plt . ylim([ - 1.1 , 1.1 ]) plt . show()","title":"16. Joint feature selection with multi-task Lasso (source)"},{"location":"LinearModels/Linear-models/#17-lasso-on-dense-and-sparse-data-source","text":"We show that linear_model.Lasso provides the same results for dense and sparse data and that in the case of sparse data the speed is improved. from time import time from scipy import sparse from scipy import linalg from sklearn.datasets.samples_generator import make_regression from sklearn.linear_model import Lasso ######################################### # The two Lasso implementations on Dense data print ( \"--- Dense matrices\" ) X, y = make_regression(n_samples = 200 , n_features = 5000 , random_state = 0 ) X_sp = sparse . coo_matrix(X) alpha = 1 sparse_lasso = Lasso(alpha = alpha, fit_intercept = False , max_iter = 1000 ) dense_lasso = Lasso(alpha = alpha, fit_intercept = False , max_iter = 1000 ) t0 = time() sparse_lasso . fit(X_sp, y) print ( \"Sparse Lasso done in %f s\" % (time() - t0)) t0 = time() dense_lasso . fit(X, y) print ( \"Dense Lasso done in %f s\" % (time() - t0)) print ( \"Distance between coefficients : %s \" % linalg . norm(sparse_lasso . coef_ - dense_lasso . coef_)) ############################################ # The two Lasso implementations on Sparse data print ( \"--- Sparse matrices\" ) Xs = X . copy() Xs[Xs < 2.5 ] = 0.0 Xs = sparse . coo_matrix(Xs) Xs = Xs . tocsc() print ( \"Matrix density : %s %% \" % (Xs . nnz / float (X . size) * 100 )) alpha = 0.1 sparse_lasso = Lasso(alpha = alpha, fit_intercept = False , max_iter = 10000 ) dense_lasso = Lasso(alpha = alpha, fit_intercept = False , max_iter = 10000 ) t0 = time() sparse_lasso . fit(Xs, y) print ( \"Sparse Lasso done in %f s\" % (time() - t0)) t0 = time() dense_lasso . fit(Xs . toarray(), y) print ( \"Dense Lasso done in %f s\" % (time() - t0)) print ( \"Distance between coefficients : %s \" % linalg . norm(sparse_lasso . coef_ - dense_lasso . coef_)) --- Dense matrices Sparse Lasso done in 0.385830s Dense Lasso done in 0.069551s Distance between coefficients : 8.407255028117243e-14 --- Sparse matrices Matrix density : 0.6263000000000001 % Sparse Lasso done in 0.237209s Dense Lasso done in 1.616462s Distance between coefficients : 1.0424172088134681e-11","title":"17. Lasso on dense and sparse data (source)"},{"location":"LinearModels/Linear-models/#18-plot-multi-class-sgd-on-the-iris-dataset-source","text":"Plot decision surface of multi-class SGD on iris dataset. The hyperplanes corresponding to the three one-versus-all (OVA) classifiers are represented by the dashed lines. import numpy as np import matplotlib.pyplot as plt from sklearn import datasets from sklearn.linear_model import SGDClassifier # import some data to play with iris = datasets . load_iris() X = iris . data[:, : 2 ] # we only take the first two features. We could # avoid this ugly slicing by using a two-dim dataset y = iris . target colors = \"bry\" # shuffle idx = np . arange(X . shape[ 0 ]) np . random . seed( 13 ) np . random . shuffle(idx) X = X[idx] y = y[idx] # standardize mean = X . mean(axis = 0 ) std = X . std(axis = 0 ) X = (X - mean) / std h = . 02 # step size in the mesh clf = SGDClassifier(alpha = 0.001 , n_iter = 100 ) . fit(X, y) # create a mesh to plot in x_min, x_max = X[:, 0 ] . min() - 1 , X[:, 0 ] . max() + 1 y_min, y_max = X[:, 1 ] . min() - 1 , X[:, 1 ] . max() + 1 xx, yy = np . meshgrid(np . arange(x_min, x_max, h), np . arange(y_min, y_max, h)) # Plot the decision boundary. For that, we will assign a color to each # point in the mesh [x_min, x_max]x[y_min, y_max]. Z = clf . predict(np . c_[xx . ravel(), yy . ravel()]) # Put the result into a color plot Z = Z . reshape(xx . shape) cs = plt . contourf(xx, yy, Z, cmap = plt . cm . Paired) plt . axis( 'tight' ) # Plot also the training points for i, color in zip (clf . classes_, colors): idx = np . where(y == i) plt . scatter(X[idx, 0 ], X[idx, 1 ], c = color, label = iris . target_names[i], cmap = plt . cm . Paired) plt . title( \"Decision surface of multi-class SGD\" ) plt . axis( 'tight' ) # Plot the three one-against-all classifiers xmin, xmax = plt . xlim() ymin, ymax = plt . ylim() coef = clf . coef_ intercept = clf . intercept_ def plot_hyperplane (c, color): def line (x0): return ( - (x0 * coef[c, 0 ]) - intercept[c]) / coef[c, 1 ] plt . plot([xmin, xmax], [line(xmin), line(xmax)], ls = \"--\" , color = color) for i, color in zip (clf . classes_, colors): plot_hyperplane(i, color) plt . legend() plt . show()","title":"18. Plot multi-class SGD on the iris dataset (source)"},{"location":"LinearModels/Linear-models/#19-sgd-penalties-source","text":"Plot the contours of the three penalties. import numpy as np import matplotlib.pyplot as plt def l1 (xs): return np . array([np . sqrt(( 1 - np . sqrt(x ** 2.0 )) ** 2.0 ) for x in xs]) def l2 (xs): return np . array([np . sqrt( 1.0 - x ** 2.0 ) for x in xs]) def el (xs, z): return np . array([( 2 - 2 * x - 2 * z + 4 * x * z - ( 4 * z ** 2 - 8 * x * z ** 2 + 8 * x ** 2 * z ** 2 - 16 * x ** 2 * z ** 3 + 8 * x * z ** 3 + 4 * x ** 2 * z ** 4 ) ** ( 1. / 2 ) - 2 * x * z ** 2 ) / ( 2 - 4 * z) for x in xs]) def cross (ext): plt . plot([ - ext, ext], [ 0 , 0 ], \"k-\" ) plt . plot([ 0 , 0 ], [ - ext, ext], \"k-\" ) xs = np . linspace( 0 , 1 , 100 ) alpha = 0.501 # 0.5 division throuh zero cross( 1.2 ) l1_color = \"navy\" l2_color = \"c\" elastic_net_color = \"darkorange\" lw = 2 plt . plot(xs, l1(xs), color = l1_color, label = \"L1\" , lw = lw) plt . plot(xs, - 1.0 * l1(xs), color = l1_color, lw = lw) plt . plot( - 1 * xs, l1(xs), color = l1_color, lw = lw) plt . plot( - 1 * xs, - 1.0 * l1(xs), color = l1_color, lw = lw) plt . plot(xs, l2(xs), color = l2_color, label = \"L2\" , lw = lw) plt . plot(xs, - 1.0 * l2(xs), color = l2_color, lw = lw) plt . plot( - 1 * xs, l2(xs), color = l2_color, lw = lw) plt . plot( - 1 * xs, - 1.0 * l2(xs), color = l2_color, lw = lw) plt . plot(xs, el(xs, alpha), color = elastic_net_color, label = \"Elastic Net\" , lw = lw) plt . plot(xs, - 1.0 * el(xs, alpha), color = elastic_net_color, lw = lw) plt . plot( - 1 * xs, el(xs, alpha), color = elastic_net_color, lw = lw) plt . plot( - 1 * xs, - 1.0 * el(xs, alpha), color = elastic_net_color, lw = lw) plt . xlabel( r\"$w_0$\" ) plt . ylabel( r\"$w_1$\" ) plt . legend() plt . axis( \"equal\" ) plt . show()","title":"19. SGD: Penalties (source)"},{"location":"LinearModels/Linear-models/#20-comparing-various-online-solvers-source","text":"An example showing how different online solvers perform on the hand-written digits dataset. import numpy as np import matplotlib.pyplot as plt from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.linear_model import SGDClassifier, Perceptron from sklearn.linear_model import PassiveAggressiveClassifier from sklearn.linear_model import LogisticRegression heldout = [ 0.95 , 0.90 , 0.75 , 0.50 , 0.01 ] rounds = 20 digits = datasets . load_digits() X, y = digits . data, digits . target classifiers = [ ( \"SGD\" , SGDClassifier()), ( \"ASGD\" , SGDClassifier(average = True )), ( \"Perceptron\" , Perceptron()), ( \"Passive-Aggressive I\" , PassiveAggressiveClassifier(loss = 'hinge' , C = 1.0 )), ( \"Passive-Aggressive II\" , PassiveAggressiveClassifier(loss = 'squared_hinge' , C = 1.0 )), ( \"SAG\" , LogisticRegression(solver = 'sag' , tol = 1e-1 , C = 1.e4 / X . shape[ 0 ])) ] xx = 1. - np . array(heldout) for name, clf in classifiers: print ( \"training %s \" % name) rng = np . random . RandomState( 42 ) yy = [] for i in heldout: yy_ = [] for r in range (rounds): X_train, X_test, y_train, y_test = \\ train_test_split(X, y, test_size = i, random_state = rng) clf . fit(X_train, y_train) y_pred = clf . predict(X_test) yy_ . append( 1 - np . mean(y_pred == y_test)) yy . append(np . mean(yy_)) plt . plot(xx, yy, label = name) plt . legend(loc = \"upper right\" ) plt . xlabel( \"Proportion train\" ) plt . ylabel( \"Test Error Rate\" ) plt . show() training SGD training ASGD training Perceptron training Passive-Aggressive I training Passive-Aggressive II training SAG","title":"20. Comparing various online solvers  (source)"},{"location":"LinearModels/Linear-models/#21-sgd-convex-loss-functions-source","text":"A plot that compares the various convex loss functions supported by :class: sklearn.linear_model.SGDClassifier . import numpy as np import matplotlib.pyplot as plt def modified_huber_loss (y_true, y_pred): z = y_pred * y_true loss = - 4 * z loss[z >= - 1 ] = ( 1 - z[z >= - 1 ]) ** 2 loss[z >= 1. ] = 0 return loss xmin, xmax = - 4 , 4 xx = np . linspace(xmin, xmax, 100 ) lw = 2 plt . plot([xmin, 0 , 0 , xmax], [ 1 , 1 , 0 , 0 ], color = 'gold' , lw = lw, label = \"Zero-one loss\" ) plt . plot(xx, np . where(xx < 1 , 1 - xx, 0 ), color = 'teal' , lw = lw, label = \"Hinge loss\" ) plt . plot(xx, - np . minimum(xx, 0 ), color = 'yellowgreen' , lw = lw, label = \"Perceptron loss\" ) plt . plot(xx, np . log2( 1 + np . exp( - xx)), color = 'cornflowerblue' , lw = lw, label = \"Log loss\" ) plt . plot(xx, np . where(xx < 1 , 1 - xx, 0 ) ** 2 , color = 'orange' , lw = lw, label = \"Squared hinge loss\" ) plt . plot(xx, modified_huber_loss(xx, 1 ), color = 'darkorchid' , lw = lw, linestyle = '--' , label = \"Modified Huber loss\" ) plt . ylim(( 0 , 8 )) plt . legend(loc = \"upper right\" ) plt . xlabel( r\"Decision function $f(x)$\" ) plt . ylabel( \"$L(y, f(x))$\" ) plt . show()","title":"21. SGD: convex loss functions (source)"},{"location":"LinearModels/Linear-models/#22-sgd-maximum-margin-separating-hyperplane-source","text":"Plot the maximum margin separating hyperplane within a two-class separable dataset using a linear Support Vector Machines classifier trained using SGD. import numpy as np import matplotlib.pyplot as plt from sklearn.linear_model import SGDClassifier from sklearn.datasets.samples_generator import make_blobs # we create 50 separable points X, Y = make_blobs(n_samples = 50 , centers = 2 , random_state = 0 , cluster_std = 0.60 ) # fit the model clf = SGDClassifier(loss = \"hinge\" , alpha = 0.01 , n_iter = 200 , fit_intercept = True ) clf . fit(X, Y) # plot the line, the points, and the nearest vectors to the plane xx = np . linspace( - 1 , 5 , 10 ) yy = np . linspace( - 1 , 5 , 10 ) X1, X2 = np . meshgrid(xx, yy) Z = np . empty(X1 . shape) for (i, j), val in np . ndenumerate(X1): x1 = val x2 = X2[i, j] p = clf . decision_function([[x1, x2]]) Z[i, j] = p[ 0 ] levels = [ - 1.0 , 0.0 , 1.0 ] linestyles = [ 'dashed' , 'solid' , 'dashed' ] colors = 'k' plt . contour(X1, X2, Z, levels, colors = colors, linestyles = linestyles) plt . scatter(X[:, 0 ], X[:, 1 ], c = Y, cmap = plt . cm . Paired) plt . axis( 'tight' ) plt . show()","title":"22. SGD: Maximum margin separating hyperplane (source)"},{"location":"LinearModels/Linear-models/#23-sgd-weighted-samples-source","text":"Plot decision function of a weighted dataset, where the size of points is proportional to its weight. import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model # we create 20 points np . random . seed( 0 ) X = np . r_[np . random . randn( 10 , 2 ) + [ 1 , 1 ], np . random . randn( 10 , 2 )] y = [ 1 ] * 10 + [ - 1 ] * 10 sample_weight = 100 * np . abs(np . random . randn( 20 )) # and assign a bigger weight to the last 10 samples sample_weight[: 10 ] *= 10 # plot the weighted data points xx, yy = np . meshgrid(np . linspace( - 4 , 5 , 500 ), np . linspace( - 4 , 5 , 500 )) plt . figure() plt . scatter(X[:, 0 ], X[:, 1 ], c = y, s = sample_weight, alpha = 0.9 , cmap = plt . cm . bone) ## fit the unweighted model clf = linear_model . SGDClassifier(alpha = 0.01 , n_iter = 100 ) clf . fit(X, y) Z = clf . decision_function(np . c_[xx . ravel(), yy . ravel()]) Z = Z . reshape(xx . shape) no_weights = plt . contour(xx, yy, Z, levels = [ 0 ], linestyles = [ 'solid' ]) ## fit the weighted model clf = linear_model . SGDClassifier(alpha = 0.01 , n_iter = 100 ) clf . fit(X, y, sample_weight = sample_weight) Z = clf . decision_function(np . c_[xx . ravel(), yy . ravel()]) Z = Z . reshape(xx . shape) samples_weights = plt . contour(xx, yy, Z, levels = [ 0 ], linestyles = [ 'dashed' ]) plt . legend([no_weights . collections[ 0 ], samples_weights . collections[ 0 ]], [ \"no weights\" , \"with weights\" ], loc = \"lower left\" ) plt . xticks(()) plt . yticks(()) plt . show()","title":"23. SGD: Weighted samples  (source)"},{"location":"LinearModels/Linear-models/#24-polynomial-interpolation-source","text":"This example demonstrates how to approximate a function with a polynomial of degree n_degree by using ridge regression. Concretely, from n_samples 1d points, it suffices to build the Vandermonde matrix, which is n_samples x n_degree+1 and has the following form: [[1, x_1, x_1 2, x_1 3, ...], [1, x_2, x_2 2, x_2 3, ...], ...] Intuitively, this matrix can be interpreted as a matrix of pseudo features (the points raised to some power). The matrix is akin to (but different from) the matrix induced by a polynomial kernel. This example shows that you can do non-linear regression with a linear model, using a pipeline to add non-linear features. Kernel methods extend this idea and can induce very high (even infinite) dimensional feature spaces. import numpy as np import matplotlib.pyplot as plt from sklearn.linear_model import Ridge from sklearn.preprocessing import PolynomialFeatures from sklearn.pipeline import make_pipeline def f (x): \"\"\" function to approximate by polynomial interpolation\"\"\" return x * np . sin(x) # generate points used to plot x_plot = np . linspace( 0 , 10 , 100 ) # generate points and keep a subset of them x = np . linspace( 0 , 10 , 100 ) rng = np . random . RandomState( 0 ) rng . shuffle(x) x = np . sort(x[: 20 ]) y = f(x) # create matrix versions of these arrays X = x[:, np . newaxis] X_plot = x_plot[:, np . newaxis] colors = [ 'teal' , 'yellowgreen' , 'gold' ] lw = 2 plt . plot(x_plot, f(x_plot), color = 'cornflowerblue' , linewidth = lw, label = \"ground truth\" ) plt . scatter(x, y, color = 'navy' , s = 30 , marker = 'o' , label = \"training points\" ) for count, degree in enumerate ([ 3 , 4 , 5 ]): model = make_pipeline(PolynomialFeatures(degree), Ridge()) model . fit(X, y) y_plot = model . predict(X_plot) plt . plot(x_plot, y_plot, color = colors[count], linewidth = lw, label = \"degree %d \" % degree) plt . legend(loc = 'lower left' ) plt . show()","title":"24. Polynomial interpolation (source)"},{"location":"LinearModels/Linear-models/#25-orthogonal-matching-pursuit-source","text":"Using orthogonal matching pursuit for recovering a sparse signal from a noisy measurement encoded with a dictionary import matplotlib.pyplot as plt import numpy as np from sklearn.linear_model import OrthogonalMatchingPursuit from sklearn.linear_model import OrthogonalMatchingPursuitCV from sklearn.datasets import make_sparse_coded_signal n_components, n_features = 512 , 100 n_nonzero_coefs = 17 # generate the data ################### # y = Xw # |x|_0 = n_nonzero_coefs y, X, w = make_sparse_coded_signal(n_samples = 1 , n_components = n_components, n_features = n_features, n_nonzero_coefs = n_nonzero_coefs, random_state = 0 ) idx, = w . nonzero() # distort the clean signal ########################## y_noisy = y + 0.05 * np . random . randn( len (y)) # plot the sparse signal ######################## plt . figure(figsize = ( 7 , 7 )) plt . subplot( 4 , 1 , 1 ) plt . xlim( 0 , 512 ) plt . title( \"Sparse signal\" ) plt . stem(idx, w[idx]) # plot the noise-free reconstruction #################################### omp = OrthogonalMatchingPursuit(n_nonzero_coefs = n_nonzero_coefs) omp . fit(X, y) coef = omp . coef_ idx_r, = coef . nonzero() plt . subplot( 4 , 1 , 2 ) plt . xlim( 0 , 512 ) plt . title( \"Recovered signal from noise-free measurements\" ) plt . stem(idx_r, coef[idx_r]) # plot the noisy reconstruction ############################### omp . fit(X, y_noisy) coef = omp . coef_ idx_r, = coef . nonzero() plt . subplot( 4 , 1 , 3 ) plt . xlim( 0 , 512 ) plt . title( \"Recovered signal from noisy measurements\" ) plt . stem(idx_r, coef[idx_r]) # plot the noisy reconstruction with number of non-zeros set by CV ################################################################## omp_cv = OrthogonalMatchingPursuitCV() omp_cv . fit(X, y_noisy) coef = omp_cv . coef_ idx_r, = coef . nonzero() plt . subplot( 4 , 1 , 4 ) plt . xlim( 0 , 512 ) plt . title( \"Recovered signal from noisy measurements with CV\" ) plt . stem(idx_r, coef[idx_r]) plt . subplots_adjust( 0.06 , 0.04 , 0.94 , 0.90 , 0.20 , 0.38 ) plt . suptitle( 'Sparse signal recovery with Orthogonal Matching Pursuit' , fontsize = 16 ) plt . show()","title":"25. Orthogonal Matching Pursuit (source)"},{"location":"LinearModels/Linear-models/#26-automatic-relevance-determination-regression-ard-source","text":"Fit regression model with Bayesian Ridge Regression. See :ref: bayesian_ridge_regression for more information on the regressor. Compared to the OLS (ordinary least squares) estimator, the coefficient weights are slightly shifted toward zeros, which stabilises them. The histogram of the estimated weights is very peaked, as a sparsity-inducing prior is implied on the weights. The estimation of the model is done by iteratively maximizing the marginal log-likelihood of the observations. We also plot predictions and uncertainties for ARD for one dimensional regression using polynomial feature expansion. Note the uncertainty starts going up on the right side of the plot. This is because these test samples are outside of the range of the training samples. import numpy as np import matplotlib.pyplot as plt from scipy import stats from sklearn.linear_model import ARDRegression, LinearRegression ############################################################################### # Generating simulated data with Gaussian weights # Parameters of the example np . random . seed( 0 ) n_samples, n_features = 100 , 100 # Create Gaussian data X = np . random . randn(n_samples, n_features) # Create weights with a precision lambda_ of 4. lambda_ = 4. w = np . zeros(n_features) # Only keep 10 weights of interest relevant_features = np . random . randint( 0 , n_features, 10 ) for i in relevant_features: w[i] = stats . norm . rvs(loc = 0 , scale = 1. / np . sqrt(lambda_)) # Create noise with a precision alpha of 50. alpha_ = 50. noise = stats . norm . rvs(loc = 0 , scale = 1. / np . sqrt(alpha_), size = n_samples) # Create the target y = np . dot(X, w) + noise ##################################### # Fit the ARD Regression clf = ARDRegression(compute_score = True ) clf . fit(X, y) ols = LinearRegression() ols . fit(X, y) ##################################### # Plot the true weights, the estimated weights, the histogram of the # weights, and predictions with standard deviations plt . figure(figsize = ( 6 , 5 )) plt . title( \"Weights of the model\" ) plt . plot(clf . coef_, color = 'darkblue' , linestyle = '-' , linewidth = 2 , label = \"ARD estimate\" ) plt . plot(ols . coef_, color = 'yellowgreen' , linestyle = ':' , linewidth = 2 , label = \"OLS estimate\" ) plt . plot(w, color = 'orange' , linestyle = '-' , linewidth = 2 , label = \"Ground truth\" ) plt . xlabel( \"Features\" ) plt . ylabel( \"Values of the weights\" ) plt . legend(loc = 1 ) plt . figure(figsize = ( 6 , 5 )) plt . title( \"Histogram of the weights\" ) plt . hist(clf . coef_, bins = n_features, color = 'navy' , log = True ) plt . scatter(clf . coef_[relevant_features], 5 * np . ones( len (relevant_features)), color = 'gold' , marker = 'o' , label = \"Relevant features\" ) plt . ylabel( \"Features\" ) plt . xlabel( \"Values of the weights\" ) plt . legend(loc = 1 ) plt . figure(figsize = ( 6 , 5 )) plt . title( \"Marginal log-likelihood\" ) plt . plot(clf . scores_, color = 'navy' , linewidth = 2 ) plt . ylabel( \"Score\" ) plt . xlabel( \"Iterations\" ) # Plotting some predictions for polynomial regression def f (x, noise_amount): y = np . sqrt(x) * np . sin(x) noise = np . random . normal( 0 , 1 , len (x)) return y + noise_amount * noise degree = 10 X = np . linspace( 0 , 10 , 100 ) y = f(X, noise_amount = 1 ) clf_poly = ARDRegression(threshold_lambda = 1e5 ) clf_poly . fit(np . vander(X, degree), y) X_plot = np . linspace( 0 , 11 , 25 ) y_plot = f(X_plot, noise_amount = 0 ) y_mean, y_std = clf_poly . predict(np . vander(X_plot, degree), return_std = True ) plt . figure(figsize = ( 6 , 5 )) plt . errorbar(X_plot, y_mean, y_std, color = 'navy' , label = \"Polynomial ARD\" , linewidth = 2 ) plt . plot(X_plot, y_plot, color = 'gold' , linewidth = 2 , label = \"Ground Truth\" ) plt . ylabel( \"Output y\" ) plt . xlabel( \"Feature X\" ) plt . legend(loc = \"lower left\" ) plt . show()","title":"26. Automatic Relevance Determination Regression (ARD) (source)"},{"location":"LinearModels/Linear-models/#27-huberregressor-vs-ridge-on-dataset-with-strong-outliers-source","text":"Fit Ridge and HuberRegressor on a dataset with outliers. The example shows that the predictions in ridge are strongly influenced by the outliers present in the dataset. The Huber regressor is less influenced by the outliers since the model uses the linear loss for these. As the parameter epsilon is increased for the Huber regressor, the decision function approaches that of the ridge. import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_regression from sklearn.linear_model import HuberRegressor, Ridge # Generate toy data. rng = np . random . RandomState( 0 ) X, y = make_regression(n_samples = 20 , n_features = 1 , random_state = 0 , noise = 4.0 , bias = 100.0 ) # Add four strong outliers to the dataset. X_outliers = rng . normal( 0 , 0.5 , size = ( 4 , 1 )) y_outliers = rng . normal( 0 , 2.0 , size = 4 ) X_outliers[: 2 , :] += X . max() + X . mean() / 4. X_outliers[ 2 :, :] += X . min() - X . mean() / 4. y_outliers[: 2 ] += y . min() - y . mean() / 4. y_outliers[ 2 :] += y . max() + y . mean() / 4. X = np . vstack((X, X_outliers)) y = np . concatenate((y, y_outliers)) plt . plot(X, y, 'b.' ) # Fit the huber regressor over a series of epsilon values. colors = [ 'r-' , 'b-' , 'y-' , 'm-' ] x = np . linspace(X . min(), X . max(), 7 ) epsilon_values = [ 1.35 , 1.5 , 1.75 , 1.9 ] for k, epsilon in enumerate (epsilon_values): huber = HuberRegressor(fit_intercept = True , alpha = 0.0 , max_iter = 100 , epsilon = epsilon) huber . fit(X, y) coef_ = huber . coef_ * x + huber . intercept_ plt . plot(x, coef_, colors[k], label = \"huber loss, %s \" % epsilon) # Fit a ridge regressor to compare it to huber regressor. ridge = Ridge(fit_intercept = True , alpha = 0.0 , random_state = 0 , normalize = True ) ridge . fit(X, y) coef_ridge = ridge . coef_ coef_ = ridge . coef_ * x + ridge . intercept_ plt . plot(x, coef_, 'g-' , label = \"ridge regression\" ) plt . title( \"Comparison of HuberRegressor vs Ridge\" ) plt . xlabel( \"X\" ) plt . ylabel( \"y\" ) plt . legend(loc = 0 ) plt . show()","title":"27. HuberRegressor vs Ridge on dataset with strong outliers (source)"},{"location":"LinearModels/Linear-models/#28-robust-linear-model-estimation-using-ransac-source","text":"import numpy as np from matplotlib import pyplot as plt from sklearn import linear_model, datasets n_samples = 1000 n_outliers = 50 X, y, coef = datasets . make_regression(n_samples = n_samples, n_features = 1 , n_informative = 1 , noise = 10 , coef = True , random_state = 0 ) # Add outlier data np . random . seed( 0 ) X[:n_outliers] = 3 + 0.5 * np . random . normal(size = (n_outliers, 1 )) y[:n_outliers] = - 3 + 10 * np . random . normal(size = n_outliers) # Fit line using all data model = linear_model . LinearRegression() model . fit(X, y) # Robustly fit linear model with RANSAC algorithm model_ransac = linear_model . RANSACRegressor(linear_model . LinearRegression()) model_ransac . fit(X, y) inlier_mask = model_ransac . inlier_mask_ outlier_mask = np . logical_not(inlier_mask) # Predict data of estimated models line_X = np . arange( - 5 , 5 ) line_y = model . predict(line_X[:, np . newaxis]) line_y_ransac = model_ransac . predict(line_X[:, np . newaxis]) # Compare estimated coefficients print ( \"Estimated coefficients (true, normal, RANSAC):\" ) print (coef, model . coef_, model_ransac . estimator_ . coef_) lw = 2 plt . scatter(X[inlier_mask], y[inlier_mask], color = 'yellowgreen' , marker = '.' , label = 'Inliers' ) plt . scatter(X[outlier_mask], y[outlier_mask], color = 'gold' , marker = '.' , label = 'Outliers' ) plt . plot(line_X, line_y, color = 'navy' , linestyle = '-' , linewidth = lw, label = 'Linear regressor' ) plt . plot(line_X, line_y_ransac, color = 'cornflowerblue' , linestyle = '-' , linewidth = lw, label = 'RANSAC regressor' ) plt . legend(loc = 'lower right' ) plt . show() Estimated coefficients (true, normal, RANSAC): 82.1903908407869 [ 54.17236387] [ 82.08533159]","title":"28. Robust linear model estimation using RANSAC (source)"},{"location":"LinearModels/Linear-models/#29-robust-linear-estimator-fitting-source","text":"Here a sine function is fit with a polynomial of order 3, for values close to zero. Robust fitting is demoed in different situations: No measurement errors, only modelling errors (fitting a sine with a polynomial) Measurement errors in X Measurement errors in y The median absolute deviation to non corrupt new data is used to judge the quality of the prediction. What we can see that: RANSAC is good for strong outliers in the y direction TheilSen is good for small outliers, both in direction X and y, but has a break point above which it performs worse than OLS. The scores of HuberRegressor may not be compared directly to both TheilSen and RANSAC because it does not attempt to completely filter the outliers but lessen their effect. from matplotlib import pyplot as plt import numpy as np from sklearn.linear_model import ( LinearRegression, TheilSenRegressor, RANSACRegressor, HuberRegressor) from sklearn.metrics import mean_squared_error from sklearn.preprocessing import PolynomialFeatures from sklearn.pipeline import make_pipeline np . random . seed( 42 ) X = np . random . normal(size = 400 ) y = np . sin(X) # Make sure that it X is 2D X = X[:, np . newaxis] X_test = np . random . normal(size = 200 ) y_test = np . sin(X_test) X_test = X_test[:, np . newaxis] y_errors = y . copy() y_errors[:: 3 ] = 3 X_errors = X . copy() X_errors[:: 3 ] = 3 y_errors_large = y . copy() y_errors_large[:: 3 ] = 10 X_errors_large = X . copy() X_errors_large[:: 3 ] = 10 estimators = [( 'OLS' , LinearRegression()), ( 'Theil-Sen' , TheilSenRegressor(random_state = 42 )), ( 'RANSAC' , RANSACRegressor(random_state = 42 )), ( 'HuberRegressor' , HuberRegressor())] colors = { 'OLS' : 'turquoise' , 'Theil-Sen' : 'gold' , 'RANSAC' : 'lightgreen' , 'HuberRegressor' : 'black' } linestyle = { 'OLS' : '-' , 'Theil-Sen' : '-.' , 'RANSAC' : '--' , 'HuberRegressor' : '--' } lw = 3 x_plot = np . linspace(X . min(), X . max()) for title, this_X, this_y in [ ( 'Modeling Errors Only' , X, y), ( 'Corrupt X, Small Deviants' , X_errors, y), ( 'Corrupt y, Small Deviants' , X, y_errors), ( 'Corrupt X, Large Deviants' , X_errors_large, y), ( 'Corrupt y, Large Deviants' , X, y_errors_large)]: plt . figure(figsize = ( 5 , 4 )) plt . plot(this_X[:, 0 ], this_y, 'b+' ) for name, estimator in estimators: model = make_pipeline(PolynomialFeatures( 3 ), estimator) model . fit(this_X, this_y) mse = mean_squared_error(model . predict(X_test), y_test) y_plot = model . predict(x_plot[:, np . newaxis]) plt . plot(x_plot, y_plot, color = colors[name], linestyle = linestyle[name], linewidth = lw, label = ' %s : error = %.3f ' % (name, mse)) legend_title = 'Error of Mean \\n Absolute Deviation \\n to Non-corrupt Data' legend = plt . legend(loc = 'upper right' , frameon = False , title = legend_title, prop = dict (size = 'x-small' )) plt . xlim( - 4 , 10.2 ) plt . ylim( - 2 , 10.2 ) plt . title(title) plt . show()","title":"29.  Robust linear estimator fitting (source)"},{"location":"LinearModels/Linear-models/#30-sparse-recovery-feature-selection-for-sparse-linear-models-source","text":"Given a small number of observations, we want to recover which features of X are relevant to explain y. For this :ref: sparse linear models <l1_feature_selection> can outperform standard statistical tests if the true model is sparse, i.e. if a small fraction of the features are relevant. As detailed in :ref: the compressive sensing notes <compressive_sensing> , the ability of L1-based approach to identify the relevant variables depends on the sparsity of the ground truth, the number of samples, the number of features, the conditioning of the design matrix on the signal subspace, the amount of noise, and the absolute value of the smallest non-zero coefficient [Wainwright2006] (http://statistics.berkeley.edu/sites/default/files/tech-reports/709.pdf). Here we keep all parameters constant and vary the conditioning of the design matrix. For a well-conditioned design matrix (small mutual incoherence) we are exactly in compressive sensing conditions (i.i.d Gaussian sensing matrix), and L1-recovery with the Lasso performs very well. For an ill-conditioned matrix (high mutual incoherence), regressors are very correlated, and the Lasso randomly selects one. However, randomized-Lasso can recover the ground truth well. In each situation, we first vary the alpha parameter setting the sparsity of the estimated model and look at the stability scores of the randomized Lasso. This analysis, knowing the ground truth, shows an optimal regime in which relevant features stand out from the irrelevant ones. If alpha is chosen too small, non-relevant variables enter the model. On the opposite, if alpha is selected too large, the Lasso is equivalent to stepwise regression, and thus brings no advantage over a univariate F-test. In a second time, we set alpha and compare the performance of different feature selection methods, using the area under curve (AUC) of the precision-recall. import warnings import matplotlib.pyplot as plt import numpy as np from scipy import linalg from sklearn.linear_model import (RandomizedLasso, lasso_stability_path, LassoLarsCV) from sklearn.feature_selection import f_regression from sklearn.preprocessing import StandardScaler from sklearn.metrics import auc, precision_recall_curve from sklearn.ensemble import ExtraTreesRegressor from sklearn.utils.extmath import pinvh from sklearn.exceptions import ConvergenceWarning def mutual_incoherence (X_relevant, X_irelevant): \"\"\"Mutual incoherence, as defined by formula (26a) of [Wainwright2006]. \"\"\" projector = np . dot(np . dot(X_irelevant . T, X_relevant), pinvh(np . dot(X_relevant . T, X_relevant))) return np . max(np . abs(projector) . sum(axis = 1 )) for conditioning in ( 1 , 1e-4 ): ################################## # Simulate regression data with a correlated design n_features = 501 n_relevant_features = 3 noise_level = . 2 coef_min = . 2 # The Donoho-Tanner phase transition is around n_samples=25: below we # will completely fail to recover in the well-conditioned case n_samples = 25 block_size = n_relevant_features rng = np . random . RandomState( 42 ) # The coefficients of our model coef = np . zeros(n_features) coef[:n_relevant_features] = coef_min + rng . rand(n_relevant_features) # The correlation of our design: variables correlated by blocs of 3 corr = np . zeros((n_features, n_features)) for i in range ( 0 , n_features, block_size): corr[i:i + block_size, i:i + block_size] = 1 - conditioning corr . flat[::n_features + 1 ] = 1 corr = linalg . cholesky(corr) # Our design X = rng . normal(size = (n_samples, n_features)) X = np . dot(X, corr) # Keep [Wainwright2006] (26c) constant X[:n_relevant_features] /= np . abs( linalg . svdvals(X[:n_relevant_features])) . max() X = StandardScaler() . fit_transform(X . copy()) # The output variable y = np . dot(X, coef) y /= np . std(y) # We scale the added noise as a function of the average correlation # between the design and the output variable y += noise_level * rng . normal(size = n_samples) mi = mutual_incoherence(X[:, :n_relevant_features], X[:, n_relevant_features:]) ######################################### # Plot stability selection path, using a high eps for early stopping # of the path, to save computation time alpha_grid, scores_path = lasso_stability_path(X, y, random_state = 42 , eps = 0.05 ) plt . figure() # We plot the path as a function of alpha/alpha_max to the power 1/3: the # power 1/3 scales the path less brutally than the log, and enables to # see the progression along the path hg = plt . plot(alpha_grid[ 1 :] ** . 333 , scores_path[coef != 0 ] . T[ 1 :], 'r' ) hb = plt . plot(alpha_grid[ 1 :] ** . 333 , scores_path[coef == 0 ] . T[ 1 :], 'k' ) ymin, ymax = plt . ylim() plt . xlabel( r'$(\\alpha / \\alpha_{max})^{1/3}$' ) plt . ylabel( 'Stability score: proportion of times selected' ) plt . title( 'Stability Scores Path - Mutual incoherence: %.1f ' % mi) plt . axis( 'tight' ) plt . legend((hg[ 0 ], hb[ 0 ]), ( 'relevant features' , 'irrelevant features' ), loc = 'best' ) ####################################### # Plot the estimated stability scores for a given alpha # Use 6-fold cross-validation rather than the default 3-fold: it leads to # a better choice of alpha: # Stop the user warnings outputs- they are not necessary for the example # as it is specifically set up to be challenging. with warnings . catch_warnings(): warnings . simplefilter( 'ignore' , UserWarning ) warnings . simplefilter( 'ignore' , ConvergenceWarning) lars_cv = LassoLarsCV(cv = 6 ) . fit(X, y) # Run the RandomizedLasso: we use a paths going down to .1*alpha_max # to avoid exploring the regime in which very noisy variables enter # the model alphas = np . linspace(lars_cv . alphas_[ 0 ], . 1 * lars_cv . alphas_[ 0 ], 6 ) clf = RandomizedLasso(alpha = alphas, random_state = 42 ) . fit(X, y) trees = ExtraTreesRegressor( 100 ) . fit(X, y) # Compare with F-score F, _ = f_regression(X, y) plt . figure() for name, score in [( 'F-test' , F), ( 'Stability selection' , clf . scores_), ( 'Lasso coefs' , np . abs(lars_cv . coef_)), ( 'Trees' , trees . feature_importances_), ]: precision, recall, thresholds = precision_recall_curve(coef != 0 , score) plt . semilogy(np . maximum(score / np . max(score), 1e-4 ), label = \" %s . AUC: %.3f \" % (name, auc(recall, precision))) plt . plot(np . where(coef != 0 )[ 0 ], [ 2e-4 ] * n_relevant_features, 'mo' , label = \"Ground truth\" ) plt . xlabel( \"Features\" ) plt . ylabel( \"Score\" ) # Plot only the 100 first coefficients plt . xlim( 0 , 100 ) plt . legend(loc = 'best' ) plt . title( 'Feature selection scores - Mutual incoherence: %.1f ' % mi) plt . show()","title":"30. Sparse recovery: feature selection for sparse linear models (source)"},{"location":"LinearModels/Linear-models/#31-theil-sen-regression-source","text":"Computes a Theil-Sen Regression on a synthetic dataset. See :ref: theil_sen_regression for more information on the regressor. Compared to the OLS (ordinary least squares) estimator, the Theil-Sen estimator is robust against outliers. It has a breakdown point of about 29.3% in case of a simple linear regression which means that it can tolerate arbitrary corrupted data (outliers) of up to 29.3% in the two-dimensional case. The estimation of the model is done by calculating the slopes and intercepts of a subpopulation of all possible combinations of p subsample points. If an intercept is fitted, p must be greater than or equal to n_features + 1. The final slope and intercept is then defined as the spatial median of these slopes and intercepts. In certain cases Theil-Sen performs better than :ref: RANSAC <ransac_regression> which is also a robust method. This is illustrated in the second example below where outliers with respect to the x-axis perturb RANSAC. Tuning the residual_threshold parameter of RANSAC remedies this but in general a priori knowledge about the data and the nature of the outliers is needed. Due to the computational complexity of Theil-Sen it is recommended to use it only for small problems in terms of number of samples and features. For larger problems the max_subpopulation parameter restricts the magnitude of all possible combinations of p subsample points to a randomly chosen subset and therefore also limits the runtime. Therefore, Theil-Sen is applicable to larger problems with the drawback of losing some of its mathematical properties since it then works on a random subset. import time import numpy as np import matplotlib.pyplot as plt from sklearn.linear_model import LinearRegression, TheilSenRegressor from sklearn.linear_model import RANSACRegressor print ( __doc__ ) estimators = [( 'OLS' , LinearRegression()), ( 'Theil-Sen' , TheilSenRegressor(random_state = 42 )), ( 'RANSAC' , RANSACRegressor(random_state = 42 )), ] colors = { 'OLS' : 'turquoise' , 'Theil-Sen' : 'gold' , 'RANSAC' : 'lightgreen' } lw = 2 ########################################### # Outliers only in the y direction np . random . seed( 0 ) n_samples = 200 # Linear model y = 3*x + N(2, 0.1**2) x = np . random . randn(n_samples) w = 3. c = 2. noise = 0.1 * np . random . randn(n_samples) y = w * x + c + noise # 10% outliers y[ - 20 :] += - 20 * x[ - 20 :] X = x[:, np . newaxis] plt . scatter(x, y, color = 'indigo' , marker = 'x' , s = 40 ) line_x = np . array([ - 3 , 3 ]) for name, estimator in estimators: t0 = time . time() estimator . fit(X, y) elapsed_time = time . time() - t0 y_pred = estimator . predict(line_x . reshape( 2 , 1 )) plt . plot(line_x, y_pred, color = colors[name], linewidth = lw, label = ' %s (fit time: %.2f s)' % (name, elapsed_time)) plt . axis( 'tight' ) plt . legend(loc = 'upper left' ) plt . title( \"Corrupt y\" ) ########################################## # Outliers in the X direction np . random . seed( 0 ) # Linear model y = 3*x + N(2, 0.1**2) x = np . random . randn(n_samples) noise = 0.1 * np . random . randn(n_samples) y = 3 * x + 2 + noise # 10% outliers x[ - 20 :] = 9.9 y[ - 20 :] += 22 X = x[:, np . newaxis] plt . figure() plt . scatter(x, y, color = 'indigo' , marker = 'x' , s = 40 ) line_x = np . array([ - 3 , 10 ]) for name, estimator in estimators: t0 = time . time() estimator . fit(X, y) elapsed_time = time . time() - t0 y_pred = estimator . predict(line_x . reshape( 2 , 1 )) plt . plot(line_x, y_pred, color = colors[name], linewidth = lw, label = ' %s (fit time: %.2f s)' % (name, elapsed_time)) plt . axis( 'tight' ) plt . legend(loc = 'upper left' ) plt . title( \"Corrupt x\" ) plt . show() Automatically created module for IPython interactive environment","title":"31.  Theil-Sen Regression (source)"},{"location":"LoopsConditions/lnc/","text":"Loops and conditions L = [ 'apple' , 'banana' , 'kite' , 'cellphone' ] '''Iterate over the items in a list''' for item in L: print (item) apple banana kite cellphone L = [] for k in range ( 10 ): print (k) '''Append values to list L''' L . append( 10 * k) 0 1 2 3 4 5 6 7 8 9 L [0, 10, 20, 30, 40, 50, 60, 70, 80, 90] D = {} for i in range ( 5 ): for j in range ( 5 ): if i == j : print (i, \"is equal to\" ,j) '''update dictionary with (i,j) as key and 2*i as value''' D . update({(i,j) : 10 * i + j}) elif i != j: '''update dictionary with (i,j) as key and 2*i as value''' D . update({(i,j) : 101 * i + j}) print (i, \"is not equal to\" ,j) 0 is equal to 0 0 is not equal to 1 0 is not equal to 2 0 is not equal to 3 0 is not equal to 4 1 is not equal to 0 1 is equal to 1 1 is not equal to 2 1 is not equal to 3 1 is not equal to 4 2 is not equal to 0 2 is not equal to 1 2 is equal to 2 2 is not equal to 3 2 is not equal to 4 3 is not equal to 0 3 is not equal to 1 3 is not equal to 2 3 is equal to 3 3 is not equal to 4 4 is not equal to 0 4 is not equal to 1 4 is not equal to 2 4 is not equal to 3 4 is equal to 4 D {(0, 0): 0, (0, 1): 1, (0, 2): 2, (0, 3): 3, (0, 4): 4, (1, 0): 101, (1, 1): 11, (1, 2): 103, (1, 3): 104, (1, 4): 105, (2, 0): 202, (2, 1): 203, (2, 2): 22, (2, 3): 205, (2, 4): 206, (3, 0): 303, (3, 1): 304, (3, 2): 305, (3, 3): 33, (3, 4): 307, (4, 0): 404, (4, 1): 405, (4, 2): 406, (4, 3): 407, (4, 4): 44} range ( 5 ) range(0, 5) for i,j in zip ( range ( 5 ), range ( 5 )): print (i,j) 0 0 1 1 2 2 3 3 4 4 for item,j in zip ([ 'apple' , 'banana' , 'kite' , 'cellphone' , 'pen' ], range ( 5 )): print (item,j) apple 0 banana 1 kite 2 cellphone 3 pen 4 for i,item in enumerate ([ 'apple' , 'banana' , 'kite' , 'cellphone' ]): print (i, item) 0 apple 1 banana 2 kite 3 cellphone A = [ 0 for k in range ( 10 )] A [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] A = [k for k in range ( 10 )] A [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] L = [[ 1 , 2 , 3 ],[ 3 , 4 , 5 ],[ 5 , 7 , 9 ]] for i in range ( len (L)): for j in range ( len (L[ 0 ])): print (L[i][j]) 1 2 3 3 4 5 5 7 9 LL = [[k + j for k in range ( 10 )] for j in range ( 10 )] LL [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [3, 4, 5, 6, 7, 8, 9, 10, 11, 12], [4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [5, 6, 7, 8, 9, 10, 11, 12, 13, 14], [6, 7, 8, 9, 10, 11, 12, 13, 14, 15], [7, 8, 9, 10, 11, 12, 13, 14, 15, 16], [8, 9, 10, 11, 12, 13, 14, 15, 16, 17], [9, 10, 11, 12, 13, 14, 15, 16, 17, 18]] i = 0 while i < 10 : print ( i, \"th turn\" ) i = i + 1 0 th turn 1 th turn 2 th turn 3 th turn 4 th turn 5 th turn 6 th turn 7 th turn 8 th turn 9 th turn import json '''download''' with open ( 'data/mylist.json' , 'w' ) as f1: json . dump(LL,f1) import json '''upload''' with open ( 'data/mydic.json' , 'r' ) as f2: AA = json . load(f2) for key,value in AA . items(): print ( \"key : \" , key, \"|\" , \"value: \" , value) key : A | value: [0.8623300586958146, 0.9817282451404751, 0.918013419185538, 0.7163654763224003, 0.9605939306786828, 0.10535569850024595, 0.11017993829505879, 0.7967874445465515, 0.40100560974033395, 0.6683804538904957] key : B | value: [0.9108032733225849, 0.5126845596833859, 0.2889475226297349, 0.4361419616905007, 0.9162781988261498, 0.6417420997937421, 0.5703303219382578, 0.8317203028864074, 0.9987773067590386, 0.19901433153401582] key : C | value: [0.6877286885216957, 0.16565933820204293, 0.25063345210121424, 0.31595887595060124, 0.03522116131022823, 0.5286776181365936, 0.8154337189974739, 0.8202821745739262, 0.0672014040433101, 0.12327287509980445] key : D | value: [0.4836330819912691, 0.8546497284804153, 0.14752285825255218, 0.5918584543549938, 0.14518319590340412, 0.025762251428333438, 0.016788596008689316, 0.009725555304236244, 0.8177641188673302, 0.5450138847266498] key : E | value: [0.6456541452062622, 0.7662672636891902, 0.04445215914793821, 0.3159171150800496, 0.9400712936126994, 0.6085210458061509, 0.6029509689621034, 0.34555270993185316, 0.7452915466172698, 0.03229045002223074] DNA = 'ATGCATGCATATCAAGCTAGCTAGCTAGCTAGCTAGAGCTATTTAATGCTA \\ GCTATATAGCGCTAGCTATAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCT \\ AGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCGCGCGCTA \\ TATATAGAGAGAGAGAGAGACACACATATATCTCTCTCTCTCGAGATCGAT \\ CGTACTAGCTAGCTAGCTAGCTAGCTAGCT' DNA . count( 'A' ),DNA . count( 'AT' ) (66, 16) count = 0 for letter in DNA: if letter == 'T' : count = count + 1 count 61 'I am ok' . split() ['I', 'am', 'ok'] Q: find sum from 0 to 1000 s = 0 for i in range ( 1000 + 1 ): s = s + i s 500500 Q: find sum from 0 to 1000 (only even) s = 0 LE = [] for i in range ( 1001 ): if i % 2 == 0 : LE . append(i) s = s + i s, sum (LE) (250500, 250500)","title":"Loops and Conditions"},{"location":"LoopsConditions/lnc/#loops-and-conditions","text":"L = [ 'apple' , 'banana' , 'kite' , 'cellphone' ] '''Iterate over the items in a list''' for item in L: print (item) apple banana kite cellphone L = [] for k in range ( 10 ): print (k) '''Append values to list L''' L . append( 10 * k) 0 1 2 3 4 5 6 7 8 9 L [0, 10, 20, 30, 40, 50, 60, 70, 80, 90] D = {} for i in range ( 5 ): for j in range ( 5 ): if i == j : print (i, \"is equal to\" ,j) '''update dictionary with (i,j) as key and 2*i as value''' D . update({(i,j) : 10 * i + j}) elif i != j: '''update dictionary with (i,j) as key and 2*i as value''' D . update({(i,j) : 101 * i + j}) print (i, \"is not equal to\" ,j) 0 is equal to 0 0 is not equal to 1 0 is not equal to 2 0 is not equal to 3 0 is not equal to 4 1 is not equal to 0 1 is equal to 1 1 is not equal to 2 1 is not equal to 3 1 is not equal to 4 2 is not equal to 0 2 is not equal to 1 2 is equal to 2 2 is not equal to 3 2 is not equal to 4 3 is not equal to 0 3 is not equal to 1 3 is not equal to 2 3 is equal to 3 3 is not equal to 4 4 is not equal to 0 4 is not equal to 1 4 is not equal to 2 4 is not equal to 3 4 is equal to 4 D {(0, 0): 0, (0, 1): 1, (0, 2): 2, (0, 3): 3, (0, 4): 4, (1, 0): 101, (1, 1): 11, (1, 2): 103, (1, 3): 104, (1, 4): 105, (2, 0): 202, (2, 1): 203, (2, 2): 22, (2, 3): 205, (2, 4): 206, (3, 0): 303, (3, 1): 304, (3, 2): 305, (3, 3): 33, (3, 4): 307, (4, 0): 404, (4, 1): 405, (4, 2): 406, (4, 3): 407, (4, 4): 44} range ( 5 ) range(0, 5) for i,j in zip ( range ( 5 ), range ( 5 )): print (i,j) 0 0 1 1 2 2 3 3 4 4 for item,j in zip ([ 'apple' , 'banana' , 'kite' , 'cellphone' , 'pen' ], range ( 5 )): print (item,j) apple 0 banana 1 kite 2 cellphone 3 pen 4 for i,item in enumerate ([ 'apple' , 'banana' , 'kite' , 'cellphone' ]): print (i, item) 0 apple 1 banana 2 kite 3 cellphone A = [ 0 for k in range ( 10 )] A [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] A = [k for k in range ( 10 )] A [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] L = [[ 1 , 2 , 3 ],[ 3 , 4 , 5 ],[ 5 , 7 , 9 ]] for i in range ( len (L)): for j in range ( len (L[ 0 ])): print (L[i][j]) 1 2 3 3 4 5 5 7 9 LL = [[k + j for k in range ( 10 )] for j in range ( 10 )] LL [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [3, 4, 5, 6, 7, 8, 9, 10, 11, 12], [4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [5, 6, 7, 8, 9, 10, 11, 12, 13, 14], [6, 7, 8, 9, 10, 11, 12, 13, 14, 15], [7, 8, 9, 10, 11, 12, 13, 14, 15, 16], [8, 9, 10, 11, 12, 13, 14, 15, 16, 17], [9, 10, 11, 12, 13, 14, 15, 16, 17, 18]] i = 0 while i < 10 : print ( i, \"th turn\" ) i = i + 1 0 th turn 1 th turn 2 th turn 3 th turn 4 th turn 5 th turn 6 th turn 7 th turn 8 th turn 9 th turn import json '''download''' with open ( 'data/mylist.json' , 'w' ) as f1: json . dump(LL,f1) import json '''upload''' with open ( 'data/mydic.json' , 'r' ) as f2: AA = json . load(f2) for key,value in AA . items(): print ( \"key : \" , key, \"|\" , \"value: \" , value) key : A | value: [0.8623300586958146, 0.9817282451404751, 0.918013419185538, 0.7163654763224003, 0.9605939306786828, 0.10535569850024595, 0.11017993829505879, 0.7967874445465515, 0.40100560974033395, 0.6683804538904957] key : B | value: [0.9108032733225849, 0.5126845596833859, 0.2889475226297349, 0.4361419616905007, 0.9162781988261498, 0.6417420997937421, 0.5703303219382578, 0.8317203028864074, 0.9987773067590386, 0.19901433153401582] key : C | value: [0.6877286885216957, 0.16565933820204293, 0.25063345210121424, 0.31595887595060124, 0.03522116131022823, 0.5286776181365936, 0.8154337189974739, 0.8202821745739262, 0.0672014040433101, 0.12327287509980445] key : D | value: [0.4836330819912691, 0.8546497284804153, 0.14752285825255218, 0.5918584543549938, 0.14518319590340412, 0.025762251428333438, 0.016788596008689316, 0.009725555304236244, 0.8177641188673302, 0.5450138847266498] key : E | value: [0.6456541452062622, 0.7662672636891902, 0.04445215914793821, 0.3159171150800496, 0.9400712936126994, 0.6085210458061509, 0.6029509689621034, 0.34555270993185316, 0.7452915466172698, 0.03229045002223074] DNA = 'ATGCATGCATATCAAGCTAGCTAGCTAGCTAGCTAGAGCTATTTAATGCTA \\ GCTATATAGCGCTAGCTATAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCT \\ AGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCGCGCGCTA \\ TATATAGAGAGAGAGAGAGACACACATATATCTCTCTCTCTCGAGATCGAT \\ CGTACTAGCTAGCTAGCTAGCTAGCTAGCT' DNA . count( 'A' ),DNA . count( 'AT' ) (66, 16) count = 0 for letter in DNA: if letter == 'T' : count = count + 1 count 61 'I am ok' . split() ['I', 'am', 'ok']","title":"Loops and conditions"},{"location":"LoopsConditions/lnc/#q-find-sum-from-0-to-1000","text":"s = 0 for i in range ( 1000 + 1 ): s = s + i s 500500","title":"Q: find sum from 0 to 1000"},{"location":"LoopsConditions/lnc/#q-find-sum-from-0-to-1000-only-even","text":"s = 0 LE = [] for i in range ( 1001 ): if i % 2 == 0 : LE . append(i) s = s + i s, sum (LE) (250500, 250500)","title":"Q: find sum from 0 to 1000 (only even)"},{"location":"Numpy/algebra/algebra/","text":"Algebra: transpose , dot , linalg https://docs.scipy.org/doc/numpy-1.13.0/reference/routines.linalg.html import numpy as np import seaborn as sns import matplotlib.pyplot as plt % matplotlib inline sns . set() Dot Product $ u = M.v $ v = np . random . rand( 5 ) v array([ 0.61880652, 0.16277711, 0.77725885, 0.39357105, 0.72518988]) M = np . random . rand( 5 , 5 ) M array([[ 0.44722927, 0.75871176, 0.74971577, 0.83610699, 0.18085266], [ 0.00121306, 0.84271476, 0.65241612, 0.27094445, 0.63364293], [ 0.07906587, 0.85301928, 0.62214839, 0.59205861, 0.70427723], [ 0.88162665, 0.76250728, 0.15724579, 0.63985535, 0.04617605], [ 0.57250482, 0.6551876 , 0.562632 , 0.29052545, 0.29159418]]) u = np . dot(M,v) u array([ 1.44319253, 1.21116885, 1.41510068, 1.07721068, 1.22403352]) Transpose of a matrix plt . figure(figsize = [ 15 , 6 ]) plt . subplot( 1 , 2 , 1 ) sns . heatmap(M, annot = True ) plt . subplot( 1 , 2 , 2 ) sns . heatmap(M . T, annot = True ) &lt;matplotlib.axes._subplots.AxesSubplot at 0x11ce902b0&gt; Solve import numpy.linalg as LA M = np . random . rand( 5 , 5 ) v = np . random . rand( 5 ) LA . solve(M, v) array([-0.70443073, 0.00920272, 1.79273467, 0.28129874, 0.0351515 ]) Inverse MI = LA . inv(M) np . dot(MI,M) array([[ 1.00000000e+00, 1.11022302e-16, -2.22044605e-16, -2.77555756e-17, -5.55111512e-17], [ -4.44089210e-16, 1.00000000e+00, -4.44089210e-16, -3.74700271e-16, -2.22044605e-16], [ 1.11022302e-16, 1.11022302e-16, 1.00000000e+00, 2.08166817e-17, 1.38777878e-16], [ 0.00000000e+00, 2.22044605e-16, 2.22044605e-16, 1.00000000e+00, 5.55111512e-17], [ 4.44089210e-16, 1.11022302e-16, 2.22044605e-16, 4.16333634e-17, 1.00000000e+00]]) Determinant LA . det(M) -0.045069829276640772 Singular Value Decomposition U,s,v = LA . svd(M) U array([[-0.36955796, 0.82623516, -0.10506496, -0.38322577, 0.15120101], [-0.40915055, -0.52825188, 0.07308364, -0.58566133, 0.45299599], [-0.27156968, -0.02207665, -0.5227498 , 0.57959325, 0.56264272], [-0.70259694, -0.01970269, 0.54356007, 0.39190428, -0.23858507], [-0.35865051, -0.19341623, -0.64412362, -0.14360195, -0.63122359]]) s array([ 2.22951719, 1.04725056, 0.39529212, 0.2559056 , 0.19082116]) v array([[-0.5796746 , -0.39152082, -0.43864895, -0.32442397, -0.46154631], [-0.5284475 , 0.42628206, -0.3319487 , 0.63137979, 0.1737713 ], [ 0.36619987, -0.57923042, -0.56088856, 0.3066463 , 0.3489439 ], [-0.23678668, 0.14559935, -0.16403051, -0.58705857, 0.74242112], [-0.44107859, -0.55523689, 0.59656951, 0.2396642 , 0.28952991]])","title":"Algebra"},{"location":"Numpy/algebra/algebra/#algebra-transpose-dot-linalg","text":"https://docs.scipy.org/doc/numpy-1.13.0/reference/routines.linalg.html import numpy as np import seaborn as sns import matplotlib.pyplot as plt % matplotlib inline sns . set()","title":"Algebra: transpose,    dot,    linalg"},{"location":"Numpy/algebra/algebra/#dot-product","text":"$ u = M.v $ v = np . random . rand( 5 ) v array([ 0.61880652, 0.16277711, 0.77725885, 0.39357105, 0.72518988]) M = np . random . rand( 5 , 5 ) M array([[ 0.44722927, 0.75871176, 0.74971577, 0.83610699, 0.18085266], [ 0.00121306, 0.84271476, 0.65241612, 0.27094445, 0.63364293], [ 0.07906587, 0.85301928, 0.62214839, 0.59205861, 0.70427723], [ 0.88162665, 0.76250728, 0.15724579, 0.63985535, 0.04617605], [ 0.57250482, 0.6551876 , 0.562632 , 0.29052545, 0.29159418]]) u = np . dot(M,v) u array([ 1.44319253, 1.21116885, 1.41510068, 1.07721068, 1.22403352])","title":"Dot Product"},{"location":"Numpy/algebra/algebra/#transpose-of-a-matrix","text":"plt . figure(figsize = [ 15 , 6 ]) plt . subplot( 1 , 2 , 1 ) sns . heatmap(M, annot = True ) plt . subplot( 1 , 2 , 2 ) sns . heatmap(M . T, annot = True ) &lt;matplotlib.axes._subplots.AxesSubplot at 0x11ce902b0&gt;","title":"Transpose of a matrix"},{"location":"Numpy/algebra/algebra/#solve","text":"import numpy.linalg as LA M = np . random . rand( 5 , 5 ) v = np . random . rand( 5 ) LA . solve(M, v) array([-0.70443073, 0.00920272, 1.79273467, 0.28129874, 0.0351515 ])","title":"Solve"},{"location":"Numpy/algebra/algebra/#inverse","text":"MI = LA . inv(M) np . dot(MI,M) array([[ 1.00000000e+00, 1.11022302e-16, -2.22044605e-16, -2.77555756e-17, -5.55111512e-17], [ -4.44089210e-16, 1.00000000e+00, -4.44089210e-16, -3.74700271e-16, -2.22044605e-16], [ 1.11022302e-16, 1.11022302e-16, 1.00000000e+00, 2.08166817e-17, 1.38777878e-16], [ 0.00000000e+00, 2.22044605e-16, 2.22044605e-16, 1.00000000e+00, 5.55111512e-17], [ 4.44089210e-16, 1.11022302e-16, 2.22044605e-16, 4.16333634e-17, 1.00000000e+00]])","title":"Inverse"},{"location":"Numpy/algebra/algebra/#determinant","text":"LA . det(M) -0.045069829276640772","title":"Determinant"},{"location":"Numpy/algebra/algebra/#singular-value-decomposition","text":"U,s,v = LA . svd(M) U array([[-0.36955796, 0.82623516, -0.10506496, -0.38322577, 0.15120101], [-0.40915055, -0.52825188, 0.07308364, -0.58566133, 0.45299599], [-0.27156968, -0.02207665, -0.5227498 , 0.57959325, 0.56264272], [-0.70259694, -0.01970269, 0.54356007, 0.39190428, -0.23858507], [-0.35865051, -0.19341623, -0.64412362, -0.14360195, -0.63122359]]) s array([ 2.22951719, 1.04725056, 0.39529212, 0.2559056 , 0.19082116]) v array([[-0.5796746 , -0.39152082, -0.43864895, -0.32442397, -0.46154631], [-0.5284475 , 0.42628206, -0.3319487 , 0.63137979, 0.1737713 ], [ 0.36619987, -0.57923042, -0.56088856, 0.3066463 , 0.3489439 ], [-0.23678668, 0.14559935, -0.16403051, -0.58705857, 0.74242112], [-0.44107859, -0.55523689, 0.59656951, 0.2396642 , 0.28952991]])","title":"Singular Value Decomposition"},{"location":"Numpy/array/array/","text":"Numpy tutorial : Array numpy.array , numpy.array.shape , numpy.reshape , numpy.concatenate How to import Numpy library in python import numpy as np array : creating array from list np . array([ 1 , 2 , 3 , 4 ]) array([1, 2, 3, 4]) np . zeros([ 3 , 4 ]) array([[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]]) np . ones([ 3 , 3 ]) array([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]]) Creating random matrix of size 10,10 X = np . random . rand( 10 , 10 ) X array([[0.9311515 , 0.67884177, 0.76163691, 0.28748043, 0.06952019, 0.57756085, 0.94357278, 0.66863999, 0.24938489, 0.99347099], [0.76764289, 0.18683651, 0.78127182, 0.8623092 , 0.94597186, 0.9789824 , 0.85675548, 0.86551995, 0.99206322, 0.15268812], [0.981708 , 0.47869981, 0.77828502, 0.76757002, 0.33535944, 0.53592128, 0.40202929, 0.52178243, 0.15259798, 0.94683227], [0.65286918, 0.27848771, 0.32560513, 0.45056069, 0.18100504, 0.84261218, 0.422149 , 0.21301687, 0.57554343, 0.07971878], [0.57691729, 0.16052797, 0.02820894, 0.66050066, 0.56261128, 0.32582041, 0.87131161, 0.36877167, 0.21222514, 0.47938826], [0.32899391, 0.31987521, 0.89478609, 0.90458122, 0.00820214, 0.96769399, 0.56621307, 0.70947586, 0.64261577, 0.61137201], [0.79006349, 0.89937467, 0.46190865, 0.90491822, 0.66487736, 0.24496253, 0.16261092, 0.31506249, 0.0064727 , 0.42401798], [0.99436863, 0.6323061 , 0.4935466 , 0.912547 , 0.38500243, 0.4960181 , 0.71025453, 0.79576306, 0.03396949, 0.71088413], [0.26799206, 0.41849134, 0.72926964, 0.57710126, 0.36234733, 0.85170595, 0.91298546, 0.8891687 , 0.4984441 , 0.23131008], [0.86864829, 0.0859265 , 0.44055358, 0.92151801, 0.65870714, 0.0021256 , 0.49167628, 0.47508668, 0.48659742, 0.92269232]]) Visualization of array : Heat map import matplotlib.pyplot as plt import seaborn as sns sns . set() plt . figure(figsize = [ 15 , 10 ]) sns . heatmap(X, annot = True ) &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f3dcc89b9b0&gt; Find subarray : splitting to 4 subarrays plt . figure(figsize = [ 15 , 10 ]) plt . subplot( 2 , 2 , 1 ) sns . heatmap(X[ 0 : 5 , 0 : 5 ], annot = True ) plt . subplot( 2 , 2 , 2 ) sns . heatmap(X[ 5 : 10 , 0 : 5 ], annot = True ) plt . subplot( 2 , 2 , 3 ) sns . heatmap(X[ 0 : 5 , 5 : 10 ], annot = True ) plt . subplot( 2 , 2 , 4 ) sns . heatmap(X[ 5 : 10 , 5 : 10 ], annot = True ) plt . show() Change shape of array: from [10 by 10] to [20 by 5] X . shape = ( 20 , 5 ) X array([[0.3188929 , 0.6959524 , 0.73093736, 0.43628291, 0.49014931], [0.67804229, 0.26945929, 0.6787637 , 0.49544915, 0.28129553], [0.75672154, 0.09647321, 0.46344621, 0.57988862, 0.0843775 ], [0.21978525, 0.99238676, 0.80045437, 0.83230711, 0.1300036 ], [0.18533263, 0.85867317, 0.4496527 , 0.72355778, 0.97670987], [0.03893615, 0.41116845, 0.61798222, 0.67051191, 0.53543659], [0.95259274, 0.14566815, 0.74642352, 0.92240214, 0.02270644], [0.82583077, 0.22513541, 0.68126091, 0.80503395, 0.20994175], [0.5572094 , 0.61159805, 0.23046078, 0.9423228 , 0.34073382], [0.55237701, 0.56379448, 0.99317082, 0.27824713, 0.75344767], [0.90546342, 0.3534449 , 0.83919252, 0.03603223, 0.56586636], [0.54411187, 0.10881733, 0.29566636, 0.49793917, 0.66915791], [0.23681423, 0.23852226, 0.84623021, 0.2428092 , 0.04609895], [0.42833986, 0.05091374, 0.69665706, 0.29589828, 0.52183274], [0.03402203, 0.23676124, 0.87017633, 0.3862823 , 0.48184487], [0.2613791 , 0.52247716, 0.98673513, 0.35632249, 0.62492285], [0.3047382 , 0.6577787 , 0.10553745, 0.37968368, 0.45684338], [0.86127512, 0.72617042, 0.79811008, 0.71744026, 0.01710278], [0.97486086, 0.3659719 , 0.68581511, 0.85199672, 0.15147913], [0.38602888, 0.60889842, 0.43274503, 0.49652464, 0.55742177]]) plt . figure(figsize = [ 15 , 10 ]) sns . heatmap(X, annot = True ) &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f3dccc2e4a8&gt; Reshaping the Array X = np . arange( 35 ) X array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]) XX = np . reshape(X, ( 7 , 5 )) plt . figure(figsize = [ 8 , 6 ]) sns . heatmap(XX, annot = True ) &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f3dcca794e0&gt; Flatten the Array X = np . random . rand( 4 , 5 ) X . shape (4, 5) X array([[0.46425663, 0.26273357, 0.76715497, 0.02803551, 0.06077554], [0.45085724, 0.20021709, 0.83866571, 0.93010498, 0.82287586], [0.64015274, 0.40214994, 0.46140888, 0.10875569, 0.90314464], [0.1996534 , 0.00575243, 0.19888495, 0.80968552, 0.66819322]]) Y = X . flatten() Y . shape (20,) Y array([0.46425663, 0.26273357, 0.76715497, 0.02803551, 0.06077554, 0.45085724, 0.20021709, 0.83866571, 0.93010498, 0.82287586, 0.64015274, 0.40214994, 0.46140888, 0.10875569, 0.90314464, 0.1996534 , 0.00575243, 0.19888495, 0.80968552, 0.66819322]) Concatenate the Array A = np . random . rand( 3 , 4 ) B = np . random . rand( 3 , 4 ) Verticle addition C = np . concatenate((A,B),axis = 0 ) Horizontal Addition D = np . concatenate((A,B),axis = 1 ) Array Disply plt . figure(figsize = [ 15 , 10 ]) plt . subplot( 2 , 2 , 1 ) sns . heatmap(A, annot = True ) plt . subplot( 2 , 2 , 2 ) sns . heatmap(B, annot = True ) plt . subplot( 2 , 2 , 3 ) sns . heatmap(C, annot = True ) plt . subplot( 2 , 2 , 4 ) sns . heatmap(D, annot = True ) plt . show() Row sum and column sum X = np . random . rand( 8 , 6 ) sum of all elements X . sum() 24.566949569026413 Row sum np . sum(X,axis = 1 ) array([4.20288936, 2.3996539 , 2.46388761, 4.30291271, 2.60671914, 2.23461285, 2.68832592, 3.6679481 ]) Column sum np . sum(X,axis = 0 ) array([3.69453055, 3.84555591, 5.87500707, 2.90605569, 4.41441644, 3.83138392])","title":"Array"},{"location":"Numpy/array/array/#numpy-tutorial-array","text":"numpy.array , numpy.array.shape , numpy.reshape , numpy.concatenate","title":"Numpy tutorial : Array"},{"location":"Numpy/array/array/#how-to-import-numpy-library-in-python","text":"import numpy as np","title":"How to import Numpy library in python"},{"location":"Numpy/array/array/#array-creating-array-from-list","text":"np . array([ 1 , 2 , 3 , 4 ]) array([1, 2, 3, 4]) np . zeros([ 3 , 4 ]) array([[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]]) np . ones([ 3 , 3 ]) array([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]])","title":"array : creating array from list"},{"location":"Numpy/array/array/#creating-random-matrix-of-size-1010","text":"X = np . random . rand( 10 , 10 ) X array([[0.9311515 , 0.67884177, 0.76163691, 0.28748043, 0.06952019, 0.57756085, 0.94357278, 0.66863999, 0.24938489, 0.99347099], [0.76764289, 0.18683651, 0.78127182, 0.8623092 , 0.94597186, 0.9789824 , 0.85675548, 0.86551995, 0.99206322, 0.15268812], [0.981708 , 0.47869981, 0.77828502, 0.76757002, 0.33535944, 0.53592128, 0.40202929, 0.52178243, 0.15259798, 0.94683227], [0.65286918, 0.27848771, 0.32560513, 0.45056069, 0.18100504, 0.84261218, 0.422149 , 0.21301687, 0.57554343, 0.07971878], [0.57691729, 0.16052797, 0.02820894, 0.66050066, 0.56261128, 0.32582041, 0.87131161, 0.36877167, 0.21222514, 0.47938826], [0.32899391, 0.31987521, 0.89478609, 0.90458122, 0.00820214, 0.96769399, 0.56621307, 0.70947586, 0.64261577, 0.61137201], [0.79006349, 0.89937467, 0.46190865, 0.90491822, 0.66487736, 0.24496253, 0.16261092, 0.31506249, 0.0064727 , 0.42401798], [0.99436863, 0.6323061 , 0.4935466 , 0.912547 , 0.38500243, 0.4960181 , 0.71025453, 0.79576306, 0.03396949, 0.71088413], [0.26799206, 0.41849134, 0.72926964, 0.57710126, 0.36234733, 0.85170595, 0.91298546, 0.8891687 , 0.4984441 , 0.23131008], [0.86864829, 0.0859265 , 0.44055358, 0.92151801, 0.65870714, 0.0021256 , 0.49167628, 0.47508668, 0.48659742, 0.92269232]])","title":"Creating random matrix of size 10,10"},{"location":"Numpy/array/array/#visualization-of-array-heat-map","text":"import matplotlib.pyplot as plt import seaborn as sns sns . set() plt . figure(figsize = [ 15 , 10 ]) sns . heatmap(X, annot = True ) &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f3dcc89b9b0&gt;","title":"Visualization of array : Heat map"},{"location":"Numpy/array/array/#find-subarray-splitting-to-4-subarrays","text":"plt . figure(figsize = [ 15 , 10 ]) plt . subplot( 2 , 2 , 1 ) sns . heatmap(X[ 0 : 5 , 0 : 5 ], annot = True ) plt . subplot( 2 , 2 , 2 ) sns . heatmap(X[ 5 : 10 , 0 : 5 ], annot = True ) plt . subplot( 2 , 2 , 3 ) sns . heatmap(X[ 0 : 5 , 5 : 10 ], annot = True ) plt . subplot( 2 , 2 , 4 ) sns . heatmap(X[ 5 : 10 , 5 : 10 ], annot = True ) plt . show()","title":"Find subarray : splitting to 4 subarrays"},{"location":"Numpy/array/array/#change-shape-of-array-from-10-by-10-to-20-by-5","text":"X . shape = ( 20 , 5 ) X array([[0.3188929 , 0.6959524 , 0.73093736, 0.43628291, 0.49014931], [0.67804229, 0.26945929, 0.6787637 , 0.49544915, 0.28129553], [0.75672154, 0.09647321, 0.46344621, 0.57988862, 0.0843775 ], [0.21978525, 0.99238676, 0.80045437, 0.83230711, 0.1300036 ], [0.18533263, 0.85867317, 0.4496527 , 0.72355778, 0.97670987], [0.03893615, 0.41116845, 0.61798222, 0.67051191, 0.53543659], [0.95259274, 0.14566815, 0.74642352, 0.92240214, 0.02270644], [0.82583077, 0.22513541, 0.68126091, 0.80503395, 0.20994175], [0.5572094 , 0.61159805, 0.23046078, 0.9423228 , 0.34073382], [0.55237701, 0.56379448, 0.99317082, 0.27824713, 0.75344767], [0.90546342, 0.3534449 , 0.83919252, 0.03603223, 0.56586636], [0.54411187, 0.10881733, 0.29566636, 0.49793917, 0.66915791], [0.23681423, 0.23852226, 0.84623021, 0.2428092 , 0.04609895], [0.42833986, 0.05091374, 0.69665706, 0.29589828, 0.52183274], [0.03402203, 0.23676124, 0.87017633, 0.3862823 , 0.48184487], [0.2613791 , 0.52247716, 0.98673513, 0.35632249, 0.62492285], [0.3047382 , 0.6577787 , 0.10553745, 0.37968368, 0.45684338], [0.86127512, 0.72617042, 0.79811008, 0.71744026, 0.01710278], [0.97486086, 0.3659719 , 0.68581511, 0.85199672, 0.15147913], [0.38602888, 0.60889842, 0.43274503, 0.49652464, 0.55742177]]) plt . figure(figsize = [ 15 , 10 ]) sns . heatmap(X, annot = True ) &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f3dccc2e4a8&gt;","title":"Change shape of array:  from [10 by 10] to [20 by 5]"},{"location":"Numpy/array/array/#reshaping-the-array","text":"X = np . arange( 35 ) X array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]) XX = np . reshape(X, ( 7 , 5 )) plt . figure(figsize = [ 8 , 6 ]) sns . heatmap(XX, annot = True ) &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f3dcca794e0&gt;","title":"Reshaping the Array"},{"location":"Numpy/array/array/#flatten-the-array","text":"X = np . random . rand( 4 , 5 ) X . shape (4, 5) X array([[0.46425663, 0.26273357, 0.76715497, 0.02803551, 0.06077554], [0.45085724, 0.20021709, 0.83866571, 0.93010498, 0.82287586], [0.64015274, 0.40214994, 0.46140888, 0.10875569, 0.90314464], [0.1996534 , 0.00575243, 0.19888495, 0.80968552, 0.66819322]]) Y = X . flatten() Y . shape (20,) Y array([0.46425663, 0.26273357, 0.76715497, 0.02803551, 0.06077554, 0.45085724, 0.20021709, 0.83866571, 0.93010498, 0.82287586, 0.64015274, 0.40214994, 0.46140888, 0.10875569, 0.90314464, 0.1996534 , 0.00575243, 0.19888495, 0.80968552, 0.66819322])","title":"Flatten the Array"},{"location":"Numpy/array/array/#concatenate-the-array","text":"A = np . random . rand( 3 , 4 ) B = np . random . rand( 3 , 4 )","title":"Concatenate the Array"},{"location":"Numpy/array/array/#verticle-addition","text":"C = np . concatenate((A,B),axis = 0 )","title":"Verticle addition"},{"location":"Numpy/array/array/#horizontal-addition","text":"D = np . concatenate((A,B),axis = 1 )","title":"Horizontal Addition"},{"location":"Numpy/array/array/#array-disply","text":"plt . figure(figsize = [ 15 , 10 ]) plt . subplot( 2 , 2 , 1 ) sns . heatmap(A, annot = True ) plt . subplot( 2 , 2 , 2 ) sns . heatmap(B, annot = True ) plt . subplot( 2 , 2 , 3 ) sns . heatmap(C, annot = True ) plt . subplot( 2 , 2 , 4 ) sns . heatmap(D, annot = True ) plt . show()","title":"Array Disply"},{"location":"Numpy/array/array/#row-sum-and-column-sum","text":"X = np . random . rand( 8 , 6 )","title":"Row sum and column sum"},{"location":"Numpy/array/array/#sum-of-all-elements","text":"X . sum() 24.566949569026413","title":"sum of all elements"},{"location":"Numpy/array/array/#row-sum","text":"np . sum(X,axis = 1 ) array([4.20288936, 2.3996539 , 2.46388761, 4.30291271, 2.60671914, 2.23461285, 2.68832592, 3.6679481 ])","title":"Row sum"},{"location":"Numpy/array/array/#column-sum","text":"np . sum(X,axis = 0 ) array([3.69453055, 3.84555591, 5.87500707, 2.90605569, 4.41441644, 3.83138392])","title":"Column sum"},{"location":"Numpy/grid/meshgrid/","text":"Numpy tutorial : arange , meshgrid How to import Numpy library in python import numpy as np 1. arange : How to generate integers from n1 to n2 X = np . arange( 10 ) X array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) X * X array([ 0, 1, 4, 9, 16, 25, 36, 49, 64, 81]) np . arange( 3 , 12 ) array([ 3, 4, 5, 6, 7, 8, 9, 10, 11]) np . arange( 1 , 10 , 2 ) array([1, 3, 5, 7, 9]) np . arange( 10 , 11 , 0.01 ) array([10. , 10.01, 10.02, 10.03, 10.04, 10.05, 10.06, 10.07, 10.08, 10.09, 10.1 , 10.11, 10.12, 10.13, 10.14, 10.15, 10.16, 10.17, 10.18, 10.19, 10.2 , 10.21, 10.22, 10.23, 10.24, 10.25, 10.26, 10.27, 10.28, 10.29, 10.3 , 10.31, 10.32, 10.33, 10.34, 10.35, 10.36, 10.37, 10.38, 10.39, 10.4 , 10.41, 10.42, 10.43, 10.44, 10.45, 10.46, 10.47, 10.48, 10.49, 10.5 , 10.51, 10.52, 10.53, 10.54, 10.55, 10.56, 10.57, 10.58, 10.59, 10.6 , 10.61, 10.62, 10.63, 10.64, 10.65, 10.66, 10.67, 10.68, 10.69, 10.7 , 10.71, 10.72, 10.73, 10.74, 10.75, 10.76, 10.77, 10.78, 10.79, 10.8 , 10.81, 10.82, 10.83, 10.84, 10.85, 10.86, 10.87, 10.88, 10.89, 10.9 , 10.91, 10.92, 10.93, 10.94, 10.95, 10.96, 10.97, 10.98, 10.99]) 1.1 Application import matplotlib.pyplot as plt import seaborn as sns % matplotlib inline sns . set() x = np . arange( 0 , 1 , 0.01 ) plt . figure(figsize = [ 10 , 10 ]) plt . plot(x,x ** 2 ,\\ x,x ** 3 ,\\ x,x ** 4 ,\\ x,x ** 5 ,\\ x,x ** 6 ,\\ x,x ** 7 ,\\ x,x ** 8 ,\\ x,x ** 9 ,\\ x,x ** 10 ) plt . grid( True ) plt . xlabel( 'x' ,fontsize = 20 ) plt . ylabel( 'x^n' ,fontsize = 20 ) plt . show() Creating Numpy array data_array = np . array([x,x ** 2 ,x ** 3 ,x ** 4 ,x ** 5 ,x ** 6 ,x ** 7 ,x ** 8 ,x ** 9 ,x ** 10 ]) data_array[ 0 : 5 , 0 : 5 ] array([[0.000e+00, 1.000e-02, 2.000e-02, 3.000e-02, 4.000e-02], [0.000e+00, 1.000e-04, 4.000e-04, 9.000e-04, 1.600e-03], [0.000e+00, 1.000e-06, 8.000e-06, 2.700e-05, 6.400e-05], [0.000e+00, 1.000e-08, 1.600e-07, 8.100e-07, 2.560e-06], [0.000e+00, 1.000e-10, 3.200e-09, 2.430e-08, 1.024e-07]]) data_array . shape (10, 100) 2. meshgrid : How to create a grid and it's application to ploting cost functions x = np . arange( - 6 , 6 , 0.05 ) y = np . arange( - 6 , 6 , 0.05 ) grid = np . meshgrid(x,y) 1. Example Cost function $\\large{Z = \\frac{sin(x^2+y^2).cos(x^2-y^2)}{x^2+y^2}}$ xx, yy = np . meshgrid(x, y, sparse = True ) # <--------------------------------------------- z = np . sin(xx ** 2 + yy ** 2 ) * np . cos(xx ** 2 - yy ** 2 ) / (xx ** 2 + yy ** 2 ) plt . figure(figsize = [ 15 , 10 ]) h = plt . contour(x,y,z) # Label contours plt . clabel(CS, inline = 1 , fontsize = 10 ) &lt;a list of 147 text.Text objects&gt; from mpl_toolkits.mplot3d import Axes3D import matplotlib.pyplot as plt from matplotlib import cm from matplotlib.ticker import LinearLocator, FormatStrFormatter import numpy as np fig = plt . figure(figsize = [ 15 , 10 ]) ax = fig . gca(projection = '3d' ) # Make data. x = np . arange( - 3 , 3 , 0.01 ) y = np . arange( - 3 , 3 , 0.01 ) xx, yy = np . meshgrid(x, y) # <------------------------------- z = np . sin(xx ** 2 + yy ** 2 ) * np . cos(xx ** 2 - yy ** 2 ) / (xx ** 2 + yy ** 2 ) # Plot the surface. surf = ax . plot_surface(xx, yy, z, cmap = cm . coolwarm,linewidth = 0 , antialiased = False ) # Customize the z axis. #ax.set_zlim(-1.01, 1.01) ax . zaxis . set_major_locator(LinearLocator( 10 )) ax . zaxis . set_major_formatter(FormatStrFormatter( ' %.02f ' )) # Add a color bar which maps values to colors. fig . colorbar(surf, shrink = 0.5 , aspect = 5 ) plt . show() Example 2: Simulated annealing import math as math $\\large{Z = 0.2 + x^2 + y^2 -0.1 \\cos(6 \\pi x) - 0.1\\cos(6 \\pi y)}$ # Design variables at mesh points x = np . arange( - 1.0 , 1.0 , 0.01 ) y = np . arange( - 1.0 , 1.0 , 0.01 ) xx, yy = np . meshgrid(x, y) #<----------------------------- z = 0.2 + xx ** 2 + yy ** 2 \\ - 0.5 * np . cos( 6.0 * math . pi * xx)\\ - 0.5 * np . cos( 6.0 * math . pi * yy) # Create a contour plot plt . figure(figsize = [ 15 , 10 ]) CS = plt . contour(xx, yy, z) # Label contours plt . clabel(CS, inline = 1 , fontsize = 10 ) plt . title( 'Non-Convex Function' ) plt . xlabel( 'x1' ) plt . ylabel( 'x2' ) Text(0,0.5,'x2') from mpl_toolkits.mplot3d import Axes3D import matplotlib.pyplot as plt from matplotlib import cm from matplotlib.ticker import LinearLocator, FormatStrFormatter import numpy as np fig = plt . figure(figsize = [ 15 , 10 ]) ax = fig . gca(projection = '3d' ) # Make data. x = np . arange( - 1 , 1 , 0.01 ) y = np . arange( - 1 , 1 , 0.01 ) xx, yy = np . meshgrid(x, y) # <------------------------------- z = 0.2 + xx ** 2 + yy ** 2 \\ - 0.5 * np . cos( 6.0 * math . pi * xx)\\ - 0.5 * np . cos( 6.0 * math . pi * yy) # Plot the surface. surf = ax . plot_surface(xx, yy, z, cmap = cm . coolwarm,linewidth = 0 , antialiased = False ) # Customize the z axis. #ax.set_zlim(-1.01, 1.01) ax . zaxis . set_major_locator(LinearLocator( 10 )) ax . zaxis . set_major_formatter(FormatStrFormatter( ' %.02f ' )) # Add a color bar which maps values to colors. fig . colorbar(surf, shrink = 0.5 , aspect = 5 ) plt . show()","title":"MeshGrid"},{"location":"Numpy/grid/meshgrid/#numpy-tutorial-arangemeshgrid","text":"","title":"Numpy tutorial : arange,meshgrid"},{"location":"Numpy/grid/meshgrid/#how-to-import-numpy-library-in-python","text":"import numpy as np","title":"How to import Numpy library in python"},{"location":"Numpy/grid/meshgrid/#1-arange-how-to-generate-integers-from-n1-to-n2","text":"X = np . arange( 10 ) X array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) X * X array([ 0, 1, 4, 9, 16, 25, 36, 49, 64, 81]) np . arange( 3 , 12 ) array([ 3, 4, 5, 6, 7, 8, 9, 10, 11]) np . arange( 1 , 10 , 2 ) array([1, 3, 5, 7, 9]) np . arange( 10 , 11 , 0.01 ) array([10. , 10.01, 10.02, 10.03, 10.04, 10.05, 10.06, 10.07, 10.08, 10.09, 10.1 , 10.11, 10.12, 10.13, 10.14, 10.15, 10.16, 10.17, 10.18, 10.19, 10.2 , 10.21, 10.22, 10.23, 10.24, 10.25, 10.26, 10.27, 10.28, 10.29, 10.3 , 10.31, 10.32, 10.33, 10.34, 10.35, 10.36, 10.37, 10.38, 10.39, 10.4 , 10.41, 10.42, 10.43, 10.44, 10.45, 10.46, 10.47, 10.48, 10.49, 10.5 , 10.51, 10.52, 10.53, 10.54, 10.55, 10.56, 10.57, 10.58, 10.59, 10.6 , 10.61, 10.62, 10.63, 10.64, 10.65, 10.66, 10.67, 10.68, 10.69, 10.7 , 10.71, 10.72, 10.73, 10.74, 10.75, 10.76, 10.77, 10.78, 10.79, 10.8 , 10.81, 10.82, 10.83, 10.84, 10.85, 10.86, 10.87, 10.88, 10.89, 10.9 , 10.91, 10.92, 10.93, 10.94, 10.95, 10.96, 10.97, 10.98, 10.99])","title":"1. arange : How to generate integers from n1 to n2"},{"location":"Numpy/grid/meshgrid/#11-application","text":"import matplotlib.pyplot as plt import seaborn as sns % matplotlib inline sns . set() x = np . arange( 0 , 1 , 0.01 ) plt . figure(figsize = [ 10 , 10 ]) plt . plot(x,x ** 2 ,\\ x,x ** 3 ,\\ x,x ** 4 ,\\ x,x ** 5 ,\\ x,x ** 6 ,\\ x,x ** 7 ,\\ x,x ** 8 ,\\ x,x ** 9 ,\\ x,x ** 10 ) plt . grid( True ) plt . xlabel( 'x' ,fontsize = 20 ) plt . ylabel( 'x^n' ,fontsize = 20 ) plt . show()","title":"1.1 Application"},{"location":"Numpy/grid/meshgrid/#creating-numpy-array","text":"data_array = np . array([x,x ** 2 ,x ** 3 ,x ** 4 ,x ** 5 ,x ** 6 ,x ** 7 ,x ** 8 ,x ** 9 ,x ** 10 ]) data_array[ 0 : 5 , 0 : 5 ] array([[0.000e+00, 1.000e-02, 2.000e-02, 3.000e-02, 4.000e-02], [0.000e+00, 1.000e-04, 4.000e-04, 9.000e-04, 1.600e-03], [0.000e+00, 1.000e-06, 8.000e-06, 2.700e-05, 6.400e-05], [0.000e+00, 1.000e-08, 1.600e-07, 8.100e-07, 2.560e-06], [0.000e+00, 1.000e-10, 3.200e-09, 2.430e-08, 1.024e-07]]) data_array . shape (10, 100)","title":"Creating Numpy array"},{"location":"Numpy/grid/meshgrid/#2-meshgrid-how-to-create-a-grid-and-its-application-to-ploting-cost-functions","text":"x = np . arange( - 6 , 6 , 0.05 ) y = np . arange( - 6 , 6 , 0.05 ) grid = np . meshgrid(x,y)","title":"2. meshgrid :  How to create a grid and it's application to ploting cost functions"},{"location":"Numpy/grid/meshgrid/#1-example-cost-function","text":"$\\large{Z = \\frac{sin(x^2+y^2).cos(x^2-y^2)}{x^2+y^2}}$ xx, yy = np . meshgrid(x, y, sparse = True ) # <--------------------------------------------- z = np . sin(xx ** 2 + yy ** 2 ) * np . cos(xx ** 2 - yy ** 2 ) / (xx ** 2 + yy ** 2 ) plt . figure(figsize = [ 15 , 10 ]) h = plt . contour(x,y,z) # Label contours plt . clabel(CS, inline = 1 , fontsize = 10 ) &lt;a list of 147 text.Text objects&gt; from mpl_toolkits.mplot3d import Axes3D import matplotlib.pyplot as plt from matplotlib import cm from matplotlib.ticker import LinearLocator, FormatStrFormatter import numpy as np fig = plt . figure(figsize = [ 15 , 10 ]) ax = fig . gca(projection = '3d' ) # Make data. x = np . arange( - 3 , 3 , 0.01 ) y = np . arange( - 3 , 3 , 0.01 ) xx, yy = np . meshgrid(x, y) # <------------------------------- z = np . sin(xx ** 2 + yy ** 2 ) * np . cos(xx ** 2 - yy ** 2 ) / (xx ** 2 + yy ** 2 ) # Plot the surface. surf = ax . plot_surface(xx, yy, z, cmap = cm . coolwarm,linewidth = 0 , antialiased = False ) # Customize the z axis. #ax.set_zlim(-1.01, 1.01) ax . zaxis . set_major_locator(LinearLocator( 10 )) ax . zaxis . set_major_formatter(FormatStrFormatter( ' %.02f ' )) # Add a color bar which maps values to colors. fig . colorbar(surf, shrink = 0.5 , aspect = 5 ) plt . show()","title":"1. Example Cost function"},{"location":"Numpy/grid/meshgrid/#example-2-simulated-annealing","text":"import math as math $\\large{Z = 0.2 + x^2 + y^2 -0.1 \\cos(6 \\pi x) - 0.1\\cos(6 \\pi y)}$ # Design variables at mesh points x = np . arange( - 1.0 , 1.0 , 0.01 ) y = np . arange( - 1.0 , 1.0 , 0.01 ) xx, yy = np . meshgrid(x, y) #<----------------------------- z = 0.2 + xx ** 2 + yy ** 2 \\ - 0.5 * np . cos( 6.0 * math . pi * xx)\\ - 0.5 * np . cos( 6.0 * math . pi * yy) # Create a contour plot plt . figure(figsize = [ 15 , 10 ]) CS = plt . contour(xx, yy, z) # Label contours plt . clabel(CS, inline = 1 , fontsize = 10 ) plt . title( 'Non-Convex Function' ) plt . xlabel( 'x1' ) plt . ylabel( 'x2' ) Text(0,0.5,'x2') from mpl_toolkits.mplot3d import Axes3D import matplotlib.pyplot as plt from matplotlib import cm from matplotlib.ticker import LinearLocator, FormatStrFormatter import numpy as np fig = plt . figure(figsize = [ 15 , 10 ]) ax = fig . gca(projection = '3d' ) # Make data. x = np . arange( - 1 , 1 , 0.01 ) y = np . arange( - 1 , 1 , 0.01 ) xx, yy = np . meshgrid(x, y) # <------------------------------- z = 0.2 + xx ** 2 + yy ** 2 \\ - 0.5 * np . cos( 6.0 * math . pi * xx)\\ - 0.5 * np . cos( 6.0 * math . pi * yy) # Plot the surface. surf = ax . plot_surface(xx, yy, z, cmap = cm . coolwarm,linewidth = 0 , antialiased = False ) # Customize the z axis. #ax.set_zlim(-1.01, 1.01) ax . zaxis . set_major_locator(LinearLocator( 10 )) ax . zaxis . set_major_formatter(FormatStrFormatter( ' %.02f ' )) # Add a color bar which maps values to colors. fig . colorbar(surf, shrink = 0.5 , aspect = 5 ) plt . show()","title":"Example 2: Simulated annealing"},{"location":"Numpy/stat/stat/","text":"Numpy Tutorial: Sattistics import numpy as np A = np . random . rand( 10 , 10 ) A array([[ 0.14686345, 0.4890597 , 0.95867873, 0.70684257, 0.4571942 , 0.88999371, 0.94088925, 0.39260288, 0.63593196, 0.91229658], [ 0.48868386, 0.12750921, 0.53490442, 0.85215575, 0.68973691, 0.86751533, 0.10078796, 0.570043 , 0.57702153, 0.82067194], [ 0.36724515, 0.44952882, 0.14455048, 0.92813935, 0.82722792, 0.15527858, 0.72646211, 0.78073294, 0.07039077, 0.02742469], [ 0.93215519, 0.78939341, 0.05433345, 0.63077664, 0.12257019, 0.38620259, 0.50937441, 0.5286639 , 0.13651584, 0.08476626], [ 0.02843688, 0.87159643, 0.58489852, 0.51735115, 0.5759214 , 0.74431949, 0.55435673, 0.28474752, 0.77936953, 0.39612921], [ 0.73733762, 0.90077575, 0.86999616, 0.69950308, 0.68703817, 0.5340441 , 0.42895176, 0.99739758, 0.05244868, 0.30736319], [ 0.43320514, 0.29546614, 0.70165549, 0.07203503, 0.58150789, 0.64377815, 0.4401386 , 0.14734496, 0.43707321, 0.7376671 ], [ 0.32679964, 0.39911744, 0.82063392, 0.42401808, 0.95892193, 0.72902086, 0.90609457, 0.66556375, 0.00359741, 0.51747374], [ 0.8956028 , 0.32121392, 0.77467549, 0.53642615, 0.85494789, 0.56736366, 0.54043267, 0.26757794, 0.68840931, 0.47665347], [ 0.13547503, 0.36567291, 0.28543209, 0.91452908, 0.57162599, 0.34107302, 0.14420041, 0.87902944, 0.68964655, 0.42688096]]) A . shape (10, 10) A . max() 0.9973975789240529 A . min() 0.0035974071135107533 A . mean() 0.53281080467131514 np . median(A) 0.53842941044090042 A . std() 0.30117722286159915 Mean and std of row np . mean(A,axis = 0 ) array([ 0.39053118, 0.38187714, 0.45877901, 0.61380263, 0.64927112, 0.46921661, 0.60222378, 0.50624244, 0.27490776, 0.54313828]) np . std(A,axis = 0 ) array([ 0.32438441, 0.29543643, 0.35045695, 0.27228791, 0.2178827 , 0.26696293, 0.32834441, 0.1804856 , 0.18123375, 0.31624931]) Mean and std of col np . mean(A,axis = 1 ) array([ 0.24116424, 0.48699354, 0.4219599 , 0.43972281, 0.47657735, 0.57537948, 0.55289209, 0.66313705, 0.46013377, 0.57202973]) np . std(A,axis = 1 ) array([ 0.19852809, 0.33144078, 0.28219183, 0.27736838, 0.22851456, 0.33649951, 0.31209182, 0.20212544, 0.27845001, 0.31922731]) Standard scalar : feature scaling A_ss = A - A . min() / A . max() - A . min() A_ss array([[ 4.10527053e-01, 7.54472423e-01, 7.94834247e-01, 7.30222536e-01, 2.05492062e-01, 7.26892063e-01, 7.31928243e-01, 5.75438370e-01, 5.93092716e-01, 3.64078100e-01], [ 6.91227074e-01, 3.26851066e-01, 6.93898234e-01, 5.00701344e-02, 7.09305332e-01, 8.38725771e-02, 7.42069173e-03, 7.35359172e-01, 2.95379821e-01, 6.85238478e-01], [ 7.79833365e-01, 7.41786430e-01, 3.19212366e-01, 6.81133292e-01, 8.84713319e-01, 6.79371426e-01, 5.60580679e-01, 1.38029784e-01, 8.13389121e-02, 8.39869709e-01], [ 4.95468619e-01, 8.24202013e-01, -5.68858232e-04, 4.31268827e-01, 3.38998416e-01, 9.81193130e-01, 4.35576371e-01, 6.44445825e-02, 4.62408333e-02, 6.89168348e-01], [ 4.83093638e-01, 4.35554299e-01, 8.99866132e-01, 9.27827109e-01, 2.75868637e-01, 2.11364337e-01, 5.98421053e-01, 4.63481490e-01, 5.65166049e-01, 4.00957652e-01], [ 6.48396649e-01, 8.91158478e-01, 2.62023883e-01, 4.97729268e-01, 4.86885273e-01, 1.39092208e-01, 3.41289269e-01, 3.48342574e-01, 4.56935240e-01, 4.67606744e-01], [ 2.80349932e-01, 6.49334474e-01, 6.69037668e-01, 5.85372776e-01, 7.04975765e-01, 8.00871254e-01, 5.35627470e-01, 9.70136155e-01, 1.16631608e-01, 2.79515900e-01], [ 9.08763495e-01, 3.00196542e-02, 5.99466774e-02, 6.81881968e-01, 7.96816096e-01, 2.63795132e-01, 9.82016613e-02, 8.91612556e-01, 4.33903568e-01, 1.27023490e-02], [ 2.62261736e-01, -4.47661111e-03, 4.01181988e-01, 6.78168605e-01, 9.14777758e-01, 7.66458761e-01, 1.03201876e-01, 9.73263369e-01, 5.80404084e-01, 1.51452341e-01], [ 5.09890394e-01, 6.55040004e-01, 1.14783059e-01, 1.22995637e-02, 2.48635616e-01, 8.57963655e-01, 9.79050239e-01, 7.73205705e-01, 8.14437099e-01, 2.42875670e-01]]) Normalization : sample Normalization B = np . random . rand( 5 , 5 ) Bn = B - B . mean() / B . std() Bn array([[-1.50414377, -1.87642236, -1.57282944, -1.72966009, -1.87154951], [-1.55166607, -1.52753634, -1.52791948, -1.46539342, -1.37820295], [-1.93716316, -1.53215212, -1.40664146, -1.72826488, -1.89673994], [-1.45251097, -1.46672865, -1.58718485, -1.3702699 , -1.17691599], [-1.98244464, -1.73232164, -1.75732137, -1.74607803, -1.64767919]]) Normal Distribution x = np . arange( - 20 , 20 , 0.01 ) import matplotlib.pyplot as plt % matplotlib inline import seaborn as sns import math as math sns . set() $f(x,\\mu,\\sigma^{2}) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} e ^{\\frac{-(x-\\mu)^{2}}{2\\sigma^{2}}}$ def normal (x,m,s): f = ( 1 / np . sqrt( 2 * np . pi * s)) * np . exp( - (x - m) ** 2 / ( 2 * s ** 2 )) return f plt . figure(figsize = [ 15 , 10 ]) plt . plot(x,normal(x, 1.0 , 1.0 ),\\ x,normal(x, 2.0 , 2.0 ),\\ x,normal(x, 3.0 , 3.0 ),\\ x,normal(x, 4.0 , 4.0 ),\\ x,normal(x, 5.0 , 5.0 ),\\ x,normal(x, 6.0 , 6.0 )) plt . show()","title":"Statistics"},{"location":"Numpy/stat/stat/#numpy-tutorial-sattistics","text":"import numpy as np A = np . random . rand( 10 , 10 ) A array([[ 0.14686345, 0.4890597 , 0.95867873, 0.70684257, 0.4571942 , 0.88999371, 0.94088925, 0.39260288, 0.63593196, 0.91229658], [ 0.48868386, 0.12750921, 0.53490442, 0.85215575, 0.68973691, 0.86751533, 0.10078796, 0.570043 , 0.57702153, 0.82067194], [ 0.36724515, 0.44952882, 0.14455048, 0.92813935, 0.82722792, 0.15527858, 0.72646211, 0.78073294, 0.07039077, 0.02742469], [ 0.93215519, 0.78939341, 0.05433345, 0.63077664, 0.12257019, 0.38620259, 0.50937441, 0.5286639 , 0.13651584, 0.08476626], [ 0.02843688, 0.87159643, 0.58489852, 0.51735115, 0.5759214 , 0.74431949, 0.55435673, 0.28474752, 0.77936953, 0.39612921], [ 0.73733762, 0.90077575, 0.86999616, 0.69950308, 0.68703817, 0.5340441 , 0.42895176, 0.99739758, 0.05244868, 0.30736319], [ 0.43320514, 0.29546614, 0.70165549, 0.07203503, 0.58150789, 0.64377815, 0.4401386 , 0.14734496, 0.43707321, 0.7376671 ], [ 0.32679964, 0.39911744, 0.82063392, 0.42401808, 0.95892193, 0.72902086, 0.90609457, 0.66556375, 0.00359741, 0.51747374], [ 0.8956028 , 0.32121392, 0.77467549, 0.53642615, 0.85494789, 0.56736366, 0.54043267, 0.26757794, 0.68840931, 0.47665347], [ 0.13547503, 0.36567291, 0.28543209, 0.91452908, 0.57162599, 0.34107302, 0.14420041, 0.87902944, 0.68964655, 0.42688096]]) A . shape (10, 10) A . max() 0.9973975789240529 A . min() 0.0035974071135107533 A . mean() 0.53281080467131514 np . median(A) 0.53842941044090042 A . std() 0.30117722286159915","title":"Numpy Tutorial: Sattistics"},{"location":"Numpy/stat/stat/#mean-and-std-of-row","text":"np . mean(A,axis = 0 ) array([ 0.39053118, 0.38187714, 0.45877901, 0.61380263, 0.64927112, 0.46921661, 0.60222378, 0.50624244, 0.27490776, 0.54313828]) np . std(A,axis = 0 ) array([ 0.32438441, 0.29543643, 0.35045695, 0.27228791, 0.2178827 , 0.26696293, 0.32834441, 0.1804856 , 0.18123375, 0.31624931])","title":"Mean and std of row"},{"location":"Numpy/stat/stat/#mean-and-std-of-col","text":"np . mean(A,axis = 1 ) array([ 0.24116424, 0.48699354, 0.4219599 , 0.43972281, 0.47657735, 0.57537948, 0.55289209, 0.66313705, 0.46013377, 0.57202973]) np . std(A,axis = 1 ) array([ 0.19852809, 0.33144078, 0.28219183, 0.27736838, 0.22851456, 0.33649951, 0.31209182, 0.20212544, 0.27845001, 0.31922731])","title":"Mean and std of col"},{"location":"Numpy/stat/stat/#standard-scalar-feature-scaling","text":"A_ss = A - A . min() / A . max() - A . min() A_ss array([[ 4.10527053e-01, 7.54472423e-01, 7.94834247e-01, 7.30222536e-01, 2.05492062e-01, 7.26892063e-01, 7.31928243e-01, 5.75438370e-01, 5.93092716e-01, 3.64078100e-01], [ 6.91227074e-01, 3.26851066e-01, 6.93898234e-01, 5.00701344e-02, 7.09305332e-01, 8.38725771e-02, 7.42069173e-03, 7.35359172e-01, 2.95379821e-01, 6.85238478e-01], [ 7.79833365e-01, 7.41786430e-01, 3.19212366e-01, 6.81133292e-01, 8.84713319e-01, 6.79371426e-01, 5.60580679e-01, 1.38029784e-01, 8.13389121e-02, 8.39869709e-01], [ 4.95468619e-01, 8.24202013e-01, -5.68858232e-04, 4.31268827e-01, 3.38998416e-01, 9.81193130e-01, 4.35576371e-01, 6.44445825e-02, 4.62408333e-02, 6.89168348e-01], [ 4.83093638e-01, 4.35554299e-01, 8.99866132e-01, 9.27827109e-01, 2.75868637e-01, 2.11364337e-01, 5.98421053e-01, 4.63481490e-01, 5.65166049e-01, 4.00957652e-01], [ 6.48396649e-01, 8.91158478e-01, 2.62023883e-01, 4.97729268e-01, 4.86885273e-01, 1.39092208e-01, 3.41289269e-01, 3.48342574e-01, 4.56935240e-01, 4.67606744e-01], [ 2.80349932e-01, 6.49334474e-01, 6.69037668e-01, 5.85372776e-01, 7.04975765e-01, 8.00871254e-01, 5.35627470e-01, 9.70136155e-01, 1.16631608e-01, 2.79515900e-01], [ 9.08763495e-01, 3.00196542e-02, 5.99466774e-02, 6.81881968e-01, 7.96816096e-01, 2.63795132e-01, 9.82016613e-02, 8.91612556e-01, 4.33903568e-01, 1.27023490e-02], [ 2.62261736e-01, -4.47661111e-03, 4.01181988e-01, 6.78168605e-01, 9.14777758e-01, 7.66458761e-01, 1.03201876e-01, 9.73263369e-01, 5.80404084e-01, 1.51452341e-01], [ 5.09890394e-01, 6.55040004e-01, 1.14783059e-01, 1.22995637e-02, 2.48635616e-01, 8.57963655e-01, 9.79050239e-01, 7.73205705e-01, 8.14437099e-01, 2.42875670e-01]])","title":"Standard scalar :  feature scaling"},{"location":"Numpy/stat/stat/#normalization-sample-normalization","text":"B = np . random . rand( 5 , 5 ) Bn = B - B . mean() / B . std() Bn array([[-1.50414377, -1.87642236, -1.57282944, -1.72966009, -1.87154951], [-1.55166607, -1.52753634, -1.52791948, -1.46539342, -1.37820295], [-1.93716316, -1.53215212, -1.40664146, -1.72826488, -1.89673994], [-1.45251097, -1.46672865, -1.58718485, -1.3702699 , -1.17691599], [-1.98244464, -1.73232164, -1.75732137, -1.74607803, -1.64767919]])","title":"Normalization :  sample Normalization"},{"location":"Numpy/stat/stat/#normal-distribution","text":"x = np . arange( - 20 , 20 , 0.01 ) import matplotlib.pyplot as plt % matplotlib inline import seaborn as sns import math as math sns . set() $f(x,\\mu,\\sigma^{2}) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} e ^{\\frac{-(x-\\mu)^{2}}{2\\sigma^{2}}}$ def normal (x,m,s): f = ( 1 / np . sqrt( 2 * np . pi * s)) * np . exp( - (x - m) ** 2 / ( 2 * s ** 2 )) return f plt . figure(figsize = [ 15 , 10 ]) plt . plot(x,normal(x, 1.0 , 1.0 ),\\ x,normal(x, 2.0 , 2.0 ),\\ x,normal(x, 3.0 , 3.0 ),\\ x,normal(x, 4.0 , 4.0 ),\\ x,normal(x, 5.0 , 5.0 ),\\ x,normal(x, 6.0 , 6.0 )) plt . show()","title":"Normal Distribution"},{"location":"Pandas/DataFrame/dataframe/","text":"Data View with Titanic dataset import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt % matplotlib inline sns . set() Load data titanic = pd . read_csv( 'data/titanic.csv' ) titanic . head( 10 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 Braund, Mr. Owen Harris male 22.0 1 0 A/5 21171 7.2500 NaN S 1 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 1 3 Heikkinen, Miss. Laina female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S 3 4 1 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0 1 0 113803 53.1000 C123 S 4 5 0 3 Allen, Mr. William Henry male 35.0 0 0 373450 8.0500 NaN S 5 6 0 3 Moran, Mr. James male NaN 0 0 330877 8.4583 NaN Q 6 7 0 1 McCarthy, Mr. Timothy J male 54.0 0 0 17463 51.8625 E46 S 7 8 0 3 Palsson, Master. Gosta Leonard male 2.0 3 1 349909 21.0750 NaN S 8 9 1 3 Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg) female 27.0 0 2 347742 11.1333 NaN S 9 10 1 2 Nasser, Mrs. Nicholas (Adele Achem) female 14.0 1 0 237736 30.0708 NaN C titanic . shape (891, 12) titanic . columns Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'], dtype='object') titanic . index RangeIndex(start=0, stop=891, step=1) Preliminary Satatistics titanic . describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Age SibSp Parch Fare count 891.000000 891.000000 891.000000 714.000000 891.000000 891.000000 891.000000 mean 446.000000 0.383838 2.308642 29.699118 0.523008 0.381594 32.204208 std 257.353842 0.486592 0.836071 14.526497 1.102743 0.806057 49.693429 min 1.000000 0.000000 1.000000 0.420000 0.000000 0.000000 0.000000 25% 223.500000 0.000000 2.000000 20.125000 0.000000 0.000000 7.910400 50% 446.000000 0.000000 3.000000 28.000000 0.000000 0.000000 14.454200 75% 668.500000 1.000000 3.000000 38.000000 1.000000 0.000000 31.000000 max 891.000000 1.000000 3.000000 80.000000 8.000000 6.000000 512.329200 Setting Name column as index titanic_df1 = titanic . copy(deep = True ) titanic_df1 = titanic . set_index( 'Name' ) titanic_df1 . head( 2 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Sex Age SibSp Parch Ticket Fare Cabin Embarked Name Braund, Mr. Owen Harris 1 0 3 male 22.0 1 0 A/5 21171 7.2500 NaN S Cumings, Mrs. John Bradley (Florence Briggs Thayer) 2 1 1 female 38.0 1 0 PC 17599 71.2833 C85 C DataFrame Creation (Numpy Array) From Numpy array => Dataframe import random as random A = np . random . rand( 100 , 20 ) A . shape (100, 20) letter = [ 'A' , 'B' , 'C' , 'D' , 'E' , 'F' , 'G' , 'H' , 'X' ] col_names = [ random . choice(letter)\\ + random . choice(letter)\\ + random . choice(letter)\\ + random . choice(letter) for i in range (A . shape[ 1 ])] col_names ['BHBE', 'FCEX', 'HCDC', 'BABH', 'DHCC', 'BEAH', 'CGCA', 'CBFF', 'GDBX', 'GDBD', 'GCXG', 'EBAF', 'FHFC', 'ADXF', 'XGDB', 'FDCB', 'BGGD', 'CXXC', 'GBDE', 'HGXX'] df = pd . DataFrame(A, columns = col_names ) df . to_csv( 'data/test.csv' ) df . head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } BHBE FCEX HCDC BABH DHCC BEAH CGCA CBFF GDBX GDBD GCXG EBAF FHFC ADXF XGDB FDCB BGGD CXXC GBDE HGXX 0 0.505613 0.863802 0.064671 0.044665 0.661631 0.010504 0.441470 0.749321 0.389375 0.468762 0.061095 0.779629 0.262696 0.834874 0.762955 0.464253 0.762709 0.425008 0.131542 0.791414 1 0.722504 0.465511 0.555621 0.843348 0.987537 0.955876 0.788946 0.461034 0.278317 0.269186 0.616559 0.095630 0.552730 0.531865 0.056233 0.796237 0.705609 0.683914 0.168146 0.312616 2 0.577487 0.355478 0.939323 0.547315 0.158492 0.226048 0.941994 0.025595 0.291006 0.549547 0.157811 0.358243 0.297590 0.767994 0.804289 0.349676 0.786392 0.806113 0.386147 0.766741 3 0.612300 0.610556 0.141520 0.657244 0.694400 0.555290 0.912868 0.350494 0.203160 0.703884 0.873016 0.420604 0.361509 0.380023 0.819483 0.988239 0.455447 0.732307 0.063254 0.123586 4 0.476368 0.890201 0.923328 0.931688 0.481882 0.411059 0.540152 0.831890 0.737365 0.681351 0.620813 0.018067 0.794526 0.491711 0.116032 0.096085 0.086113 0.813632 0.828594 0.063989 Data Frame => Numpy Array Data Frame (List of Dictionary) LD = [] for i in range ( 100 ): LD . append({ 'Player' : random . choice(letter) + random . choice(letter) + random . choice(letter) + random . choice(letter),\\ 'game1' : random . uniform( 0 , 1 ),\\ 'game2' : random . uniform( 0 , 1 ),\\ 'game3' : random . uniform( 0 , 1 ), 'game4' : random . uniform( 0 , 1 ), 'game5' : random . uniform( 0 , 1 )}) DF = pd . DataFrame(LD) DF . head( 10 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Player game1 game2 game3 game4 game5 0 CGHX 0.222859 0.370064 0.966385 0.350200 0.294583 1 FFHX 0.390963 0.339934 0.614410 0.251014 0.132956 2 BACC 0.143930 0.217735 0.117256 0.999636 0.566992 3 BCGH 0.499326 0.749363 0.457431 0.087111 0.385008 4 HBCH 0.944682 0.199605 0.372076 0.745106 0.278212 5 DAEE 0.491847 0.137834 0.517073 0.175743 0.289975 6 DCBA 0.747629 0.920831 0.151625 0.168380 0.153710 7 FGBB 0.964832 0.963819 0.465629 0.928988 0.448380 8 EHBF 0.727150 0.743628 0.510928 0.363017 0.856924 9 CXDB 0.448294 0.261936 0.147476 0.539172 0.736563 DF = DF . set_index( 'Player' ) DF . head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } game1 game2 game3 game4 game5 Player CGBG 0.030258 0.018657 0.932341 0.586397 0.089513 DXHD 0.759187 0.309100 0.862211 0.094455 0.169772 FDHF 0.058685 0.568902 0.405327 0.592841 0.244399 CGCE 0.130951 0.806490 0.185252 0.341298 0.262757 XHAC 0.433210 0.658711 0.680462 0.682604 0.179386 Data View DF . plot(figsize = [ 18 , 10 ]) &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a19fd8860&gt; DF[ 0 : 50 ] . plot . bar(stacked = True ,figsize = ( 20 , 15 ),fontsize = 10 ) &lt;matplotlib.axes._subplots.AxesSubplot at 0x107cd7198&gt; DF[ 0 : 10 ] . plot . bar(stacked = False ,figsize = ( 20 , 15 ),fontsize = 10 ) &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a1a0b9cf8&gt; from pandas.plotting import scatter_matrix scatter_matrix(DF, alpha = 0.2 , figsize = ( 18 , 15 ), diagonal = 'kde' ) plt . show() DF . plot . hexbin(x = 'game1' , y = 'game2' ,figsize = ( 12 , 10 ), gridsize = 25 ) &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a242a7550&gt; A = np . array([ 1 , 2 , 3 , 4 , 5 ]) type (A) numpy.ndarray A * 2 array([ 2, 4, 6, 8, 10])","title":"Dataframe"},{"location":"Pandas/DataFrame/dataframe/#data-view-with-titanic-dataset","text":"import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt % matplotlib inline sns . set()","title":"Data View with Titanic dataset"},{"location":"Pandas/DataFrame/dataframe/#load-data","text":"titanic = pd . read_csv( 'data/titanic.csv' ) titanic . head( 10 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 Braund, Mr. Owen Harris male 22.0 1 0 A/5 21171 7.2500 NaN S 1 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 1 3 Heikkinen, Miss. Laina female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S 3 4 1 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0 1 0 113803 53.1000 C123 S 4 5 0 3 Allen, Mr. William Henry male 35.0 0 0 373450 8.0500 NaN S 5 6 0 3 Moran, Mr. James male NaN 0 0 330877 8.4583 NaN Q 6 7 0 1 McCarthy, Mr. Timothy J male 54.0 0 0 17463 51.8625 E46 S 7 8 0 3 Palsson, Master. Gosta Leonard male 2.0 3 1 349909 21.0750 NaN S 8 9 1 3 Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg) female 27.0 0 2 347742 11.1333 NaN S 9 10 1 2 Nasser, Mrs. Nicholas (Adele Achem) female 14.0 1 0 237736 30.0708 NaN C titanic . shape (891, 12) titanic . columns Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'], dtype='object') titanic . index RangeIndex(start=0, stop=891, step=1)","title":"Load data"},{"location":"Pandas/DataFrame/dataframe/#preliminary-satatistics","text":"titanic . describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Age SibSp Parch Fare count 891.000000 891.000000 891.000000 714.000000 891.000000 891.000000 891.000000 mean 446.000000 0.383838 2.308642 29.699118 0.523008 0.381594 32.204208 std 257.353842 0.486592 0.836071 14.526497 1.102743 0.806057 49.693429 min 1.000000 0.000000 1.000000 0.420000 0.000000 0.000000 0.000000 25% 223.500000 0.000000 2.000000 20.125000 0.000000 0.000000 7.910400 50% 446.000000 0.000000 3.000000 28.000000 0.000000 0.000000 14.454200 75% 668.500000 1.000000 3.000000 38.000000 1.000000 0.000000 31.000000 max 891.000000 1.000000 3.000000 80.000000 8.000000 6.000000 512.329200","title":"Preliminary Satatistics"},{"location":"Pandas/DataFrame/dataframe/#setting-name-column-as-index","text":"titanic_df1 = titanic . copy(deep = True ) titanic_df1 = titanic . set_index( 'Name' ) titanic_df1 . head( 2 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Sex Age SibSp Parch Ticket Fare Cabin Embarked Name Braund, Mr. Owen Harris 1 0 3 male 22.0 1 0 A/5 21171 7.2500 NaN S Cumings, Mrs. John Bradley (Florence Briggs Thayer) 2 1 1 female 38.0 1 0 PC 17599 71.2833 C85 C","title":"Setting Name column as index"},{"location":"Pandas/DataFrame/dataframe/#dataframe-creation-numpy-array","text":"","title":"DataFrame Creation (Numpy Array)"},{"location":"Pandas/DataFrame/dataframe/#from-numpy-array-dataframe","text":"import random as random A = np . random . rand( 100 , 20 ) A . shape (100, 20) letter = [ 'A' , 'B' , 'C' , 'D' , 'E' , 'F' , 'G' , 'H' , 'X' ] col_names = [ random . choice(letter)\\ + random . choice(letter)\\ + random . choice(letter)\\ + random . choice(letter) for i in range (A . shape[ 1 ])] col_names ['BHBE', 'FCEX', 'HCDC', 'BABH', 'DHCC', 'BEAH', 'CGCA', 'CBFF', 'GDBX', 'GDBD', 'GCXG', 'EBAF', 'FHFC', 'ADXF', 'XGDB', 'FDCB', 'BGGD', 'CXXC', 'GBDE', 'HGXX'] df = pd . DataFrame(A, columns = col_names ) df . to_csv( 'data/test.csv' ) df . head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } BHBE FCEX HCDC BABH DHCC BEAH CGCA CBFF GDBX GDBD GCXG EBAF FHFC ADXF XGDB FDCB BGGD CXXC GBDE HGXX 0 0.505613 0.863802 0.064671 0.044665 0.661631 0.010504 0.441470 0.749321 0.389375 0.468762 0.061095 0.779629 0.262696 0.834874 0.762955 0.464253 0.762709 0.425008 0.131542 0.791414 1 0.722504 0.465511 0.555621 0.843348 0.987537 0.955876 0.788946 0.461034 0.278317 0.269186 0.616559 0.095630 0.552730 0.531865 0.056233 0.796237 0.705609 0.683914 0.168146 0.312616 2 0.577487 0.355478 0.939323 0.547315 0.158492 0.226048 0.941994 0.025595 0.291006 0.549547 0.157811 0.358243 0.297590 0.767994 0.804289 0.349676 0.786392 0.806113 0.386147 0.766741 3 0.612300 0.610556 0.141520 0.657244 0.694400 0.555290 0.912868 0.350494 0.203160 0.703884 0.873016 0.420604 0.361509 0.380023 0.819483 0.988239 0.455447 0.732307 0.063254 0.123586 4 0.476368 0.890201 0.923328 0.931688 0.481882 0.411059 0.540152 0.831890 0.737365 0.681351 0.620813 0.018067 0.794526 0.491711 0.116032 0.096085 0.086113 0.813632 0.828594 0.063989","title":"From Numpy array =&gt; Dataframe"},{"location":"Pandas/DataFrame/dataframe/#data-frame-numpy-array","text":"","title":"Data Frame =&gt; Numpy Array"},{"location":"Pandas/DataFrame/dataframe/#data-frame-list-of-dictionary","text":"LD = [] for i in range ( 100 ): LD . append({ 'Player' : random . choice(letter) + random . choice(letter) + random . choice(letter) + random . choice(letter),\\ 'game1' : random . uniform( 0 , 1 ),\\ 'game2' : random . uniform( 0 , 1 ),\\ 'game3' : random . uniform( 0 , 1 ), 'game4' : random . uniform( 0 , 1 ), 'game5' : random . uniform( 0 , 1 )}) DF = pd . DataFrame(LD) DF . head( 10 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Player game1 game2 game3 game4 game5 0 CGHX 0.222859 0.370064 0.966385 0.350200 0.294583 1 FFHX 0.390963 0.339934 0.614410 0.251014 0.132956 2 BACC 0.143930 0.217735 0.117256 0.999636 0.566992 3 BCGH 0.499326 0.749363 0.457431 0.087111 0.385008 4 HBCH 0.944682 0.199605 0.372076 0.745106 0.278212 5 DAEE 0.491847 0.137834 0.517073 0.175743 0.289975 6 DCBA 0.747629 0.920831 0.151625 0.168380 0.153710 7 FGBB 0.964832 0.963819 0.465629 0.928988 0.448380 8 EHBF 0.727150 0.743628 0.510928 0.363017 0.856924 9 CXDB 0.448294 0.261936 0.147476 0.539172 0.736563 DF = DF . set_index( 'Player' ) DF . head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } game1 game2 game3 game4 game5 Player CGBG 0.030258 0.018657 0.932341 0.586397 0.089513 DXHD 0.759187 0.309100 0.862211 0.094455 0.169772 FDHF 0.058685 0.568902 0.405327 0.592841 0.244399 CGCE 0.130951 0.806490 0.185252 0.341298 0.262757 XHAC 0.433210 0.658711 0.680462 0.682604 0.179386","title":"Data Frame (List of Dictionary)"},{"location":"Pandas/DataFrame/dataframe/#data-view","text":"DF . plot(figsize = [ 18 , 10 ]) &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a19fd8860&gt; DF[ 0 : 50 ] . plot . bar(stacked = True ,figsize = ( 20 , 15 ),fontsize = 10 ) &lt;matplotlib.axes._subplots.AxesSubplot at 0x107cd7198&gt; DF[ 0 : 10 ] . plot . bar(stacked = False ,figsize = ( 20 , 15 ),fontsize = 10 ) &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a1a0b9cf8&gt; from pandas.plotting import scatter_matrix scatter_matrix(DF, alpha = 0.2 , figsize = ( 18 , 15 ), diagonal = 'kde' ) plt . show() DF . plot . hexbin(x = 'game1' , y = 'game2' ,figsize = ( 12 , 10 ), gridsize = 25 ) &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a242a7550&gt; A = np . array([ 1 , 2 , 3 , 4 , 5 ]) type (A) numpy.ndarray A * 2 array([ 2, 4, 6, 8, 10])","title":"Data View"},{"location":"Pandas/Explor/explor/","text":"Data Exploration With Pandas import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt % matplotlib inline sns . set() Load data titanic = pd . read_csv( 'data/titanic.csv' ) Setting Name column as index titanic_df1 = titanic . copy(deep = True ) titanic_df1 = titanic . set_index( 'Name' ) titanic_df1 . head( 2 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Sex Age SibSp Parch Ticket Fare Cabin Embarked Name Braund, Mr. Owen Harris 1 0 3 male 22.0 1 0 A/5 21171 7.2500 NaN S Cumings, Mrs. John Bradley (Florence Briggs Thayer) 2 1 1 female 38.0 1 0 PC 17599 71.2833 C85 C Data Exploration: Titanic Dataset titanic_df1 . describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Age SibSp Parch Fare count 891.000000 891.000000 891.000000 714.000000 891.000000 891.000000 891.000000 mean 446.000000 0.383838 2.308642 29.699118 0.523008 0.381594 32.204208 std 257.353842 0.486592 0.836071 14.526497 1.102743 0.806057 49.693429 min 1.000000 0.000000 1.000000 0.420000 0.000000 0.000000 0.000000 25% 223.500000 0.000000 2.000000 20.125000 0.000000 0.000000 7.910400 50% 446.000000 0.000000 3.000000 28.000000 0.000000 0.000000 14.454200 75% 668.500000 1.000000 3.000000 38.000000 1.000000 0.000000 31.000000 max 891.000000 1.000000 3.000000 80.000000 8.000000 6.000000 512.329200 plt . figure(figsize = [ 15 , 15 ]) plt . subplot( 2 , 2 , 1 ) plt . xlabel( 'Sex' ) titanic_df1[ 'Sex' ] . hist() plt . subplot( 2 , 2 , 2 ) plt . xlabel( 'Age' ) titanic_df1[ 'Age' ] . hist(bins = 50 ) plt . subplot( 2 , 2 , 3 ) plt . xlabel( 'Fare' ) titanic_df1[ 'Fare' ] . hist(bins = 50 ) plt . subplot( 2 , 2 , 4 ) plt . xlabel( 'Age' ) plt . ylabel( 'Fare' ) plt . scatter(titanic_df1[ 'Age' ],titanic_df1[ 'Fare' ]) plt . show() Data Exploration with Charity Dataset charitydf = pd . read_csv( 'data/charity.csv' ) charitydf . head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age workclass education_level education-num marital-status occupation relationship race sex capital-gain capital-loss hours-per-week native-country income 0 39 State-gov Bachelors 13.0 Never-married Adm-clerical Not-in-family White Male 2174.0 0.0 40.0 United-States <=50K 1 50 Self-emp-not-inc Bachelors 13.0 Married-civ-spouse Exec-managerial Husband White Male 0.0 0.0 13.0 United-States <=50K 2 38 Private HS-grad 9.0 Divorced Handlers-cleaners Not-in-family White Male 0.0 0.0 40.0 United-States <=50K 3 53 Private 11th 7.0 Married-civ-spouse Handlers-cleaners Husband Black Male 0.0 0.0 40.0 United-States <=50K 4 28 Private Bachelors 13.0 Married-civ-spouse Prof-specialty Wife Black Female 0.0 0.0 40.0 Cuba <=50K charitydf . describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age education-num capital-gain capital-loss hours-per-week count 45222.000000 45222.000000 45222.000000 45222.000000 45222.000000 mean 38.547941 10.118460 1101.430344 88.595418 40.938017 std 13.217870 2.552881 7506.430084 404.956092 12.007508 min 17.000000 1.000000 0.000000 0.000000 1.000000 25% 28.000000 9.000000 0.000000 0.000000 40.000000 50% 37.000000 10.000000 0.000000 0.000000 40.000000 75% 47.000000 13.000000 0.000000 0.000000 45.000000 max 90.000000 16.000000 99999.000000 4356.000000 99.000000 charitydf . info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 45222 entries, 0 to 45221 Data columns (total 14 columns): age 45222 non-null int64 workclass 45222 non-null object education_level 45222 non-null object education-num 45222 non-null float64 marital-status 45222 non-null object occupation 45222 non-null object relationship 45222 non-null object race 45222 non-null object sex 45222 non-null object capital-gain 45222 non-null float64 capital-loss 45222 non-null float64 hours-per-week 45222 non-null float64 native-country 45222 non-null object income 45222 non-null object dtypes: float64(4), int64(1), object(9) memory usage: 4.8+ MB import matplotlib.pyplot as plt plt . figure(figsize = ( 14 , 10 )) plt . subplot( 2 , 3 , 1 ) plt . title( \"distribution of age\" ) charitydf[ 'age' ] . hist(bins = 100 ) plt . subplot( 2 , 3 , 2 ) plt . title( \"distribution of education-num\" ) charitydf[ 'education-num' ] . hist(bins = 40 ) plt . subplot( 2 , 3 , 3 ) plt . title( \"distribution of capital-gain\" ) charitydf[ 'capital-gain' ] . hist(bins = 100 ) plt . subplot( 2 , 3 , 4 ) plt . title( \"distribution of hours-per-week\" ) charitydf[ 'hours-per-week' ] . hist(bins = 50 ) plt . subplot( 2 , 3 , 5 ) plt . title( \"distribution of capital-loss\" ) charitydf[ 'capital-loss' ] . hist(bins = 50 ) plt . show() &lt;matplotlib.axes._subplots.AxesSubplot at 0x7efd84af9358&gt; plt . figure(figsize = ( 16 , 21 )) sns . set() sns . pairplot(charitydf, hue = \"income\" ) &lt;seaborn.axisgrid.PairGrid at 0x7efd84b28ba8&gt; &lt;matplotlib.figure.Figure at 0x7efd84b197b8&gt; sns . countplot(y = \"marital-status\" , hue = \"income\" , data = charitydf, palette = \"Greens_d\" ); plt . figure(figsize = ( 16 , 21 )) sns . set(style = \"whitegrid\" , color_codes = True ) sns . factorplot( \"sex\" , col = \"marital-status\" , data = charitydf, hue = 'income' , kind = \"count\" , col_wrap = 2 ); &lt;matplotlib.figure.Figure at 0x7efd860fd9b0&gt; plt . figure(figsize = ( 15 , 21 )) plt . subplot( 1 , 2 , 1 ) sns . countplot(y = \"age\" , hue = \"income\" , data = charitydf, palette = \"Greens_d\" ); plt . subplot( 1 , 2 , 2 ) sns . countplot(y = \"hours-per-week\" , hue = \"income\" , data = charitydf, palette = \"Greens_d\" ); plt . figure(figsize = ( 15 , 21 )) sns . jointplot(x = \"age\" , y = \"hours-per-week\" , data = charitydf,size = 15 ,kind = 'reg' ); &lt;matplotlib.figure.Figure at 0x7efd7ee362b0&gt; plt . figure(figsize = ( 10 , 10 )) sns . barplot(x = \"education-num\" , y = \"education_level\" , data = charitydf); plt . figure(figsize = ( 16 , 16 )) plt . subplot( 1 , 2 , 1 ) sns . countplot(y = \"education-num\" , hue = \"income\" , data = charitydf, palette = \"Greens_d\" ); plt . subplot( 1 , 2 , 2 ) sns . countplot(y = \"education_level\" , hue = \"income\" , data = charitydf, palette = \"Greens_d\" );","title":"Data Exploration"},{"location":"Pandas/Explor/explor/#data-exploration-with-pandas","text":"import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt % matplotlib inline sns . set()","title":"Data Exploration With Pandas"},{"location":"Pandas/Explor/explor/#load-data","text":"titanic = pd . read_csv( 'data/titanic.csv' )","title":"Load data"},{"location":"Pandas/Explor/explor/#setting-name-column-as-index","text":"titanic_df1 = titanic . copy(deep = True ) titanic_df1 = titanic . set_index( 'Name' ) titanic_df1 . head( 2 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Sex Age SibSp Parch Ticket Fare Cabin Embarked Name Braund, Mr. Owen Harris 1 0 3 male 22.0 1 0 A/5 21171 7.2500 NaN S Cumings, Mrs. John Bradley (Florence Briggs Thayer) 2 1 1 female 38.0 1 0 PC 17599 71.2833 C85 C","title":"Setting Name column as index"},{"location":"Pandas/Explor/explor/#data-exploration-titanic-dataset","text":"titanic_df1 . describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Age SibSp Parch Fare count 891.000000 891.000000 891.000000 714.000000 891.000000 891.000000 891.000000 mean 446.000000 0.383838 2.308642 29.699118 0.523008 0.381594 32.204208 std 257.353842 0.486592 0.836071 14.526497 1.102743 0.806057 49.693429 min 1.000000 0.000000 1.000000 0.420000 0.000000 0.000000 0.000000 25% 223.500000 0.000000 2.000000 20.125000 0.000000 0.000000 7.910400 50% 446.000000 0.000000 3.000000 28.000000 0.000000 0.000000 14.454200 75% 668.500000 1.000000 3.000000 38.000000 1.000000 0.000000 31.000000 max 891.000000 1.000000 3.000000 80.000000 8.000000 6.000000 512.329200 plt . figure(figsize = [ 15 , 15 ]) plt . subplot( 2 , 2 , 1 ) plt . xlabel( 'Sex' ) titanic_df1[ 'Sex' ] . hist() plt . subplot( 2 , 2 , 2 ) plt . xlabel( 'Age' ) titanic_df1[ 'Age' ] . hist(bins = 50 ) plt . subplot( 2 , 2 , 3 ) plt . xlabel( 'Fare' ) titanic_df1[ 'Fare' ] . hist(bins = 50 ) plt . subplot( 2 , 2 , 4 ) plt . xlabel( 'Age' ) plt . ylabel( 'Fare' ) plt . scatter(titanic_df1[ 'Age' ],titanic_df1[ 'Fare' ]) plt . show()","title":"Data Exploration: Titanic Dataset"},{"location":"Pandas/Explor/explor/#data-exploration-with-charity-dataset","text":"charitydf = pd . read_csv( 'data/charity.csv' ) charitydf . head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age workclass education_level education-num marital-status occupation relationship race sex capital-gain capital-loss hours-per-week native-country income 0 39 State-gov Bachelors 13.0 Never-married Adm-clerical Not-in-family White Male 2174.0 0.0 40.0 United-States <=50K 1 50 Self-emp-not-inc Bachelors 13.0 Married-civ-spouse Exec-managerial Husband White Male 0.0 0.0 13.0 United-States <=50K 2 38 Private HS-grad 9.0 Divorced Handlers-cleaners Not-in-family White Male 0.0 0.0 40.0 United-States <=50K 3 53 Private 11th 7.0 Married-civ-spouse Handlers-cleaners Husband Black Male 0.0 0.0 40.0 United-States <=50K 4 28 Private Bachelors 13.0 Married-civ-spouse Prof-specialty Wife Black Female 0.0 0.0 40.0 Cuba <=50K charitydf . describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age education-num capital-gain capital-loss hours-per-week count 45222.000000 45222.000000 45222.000000 45222.000000 45222.000000 mean 38.547941 10.118460 1101.430344 88.595418 40.938017 std 13.217870 2.552881 7506.430084 404.956092 12.007508 min 17.000000 1.000000 0.000000 0.000000 1.000000 25% 28.000000 9.000000 0.000000 0.000000 40.000000 50% 37.000000 10.000000 0.000000 0.000000 40.000000 75% 47.000000 13.000000 0.000000 0.000000 45.000000 max 90.000000 16.000000 99999.000000 4356.000000 99.000000 charitydf . info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 45222 entries, 0 to 45221 Data columns (total 14 columns): age 45222 non-null int64 workclass 45222 non-null object education_level 45222 non-null object education-num 45222 non-null float64 marital-status 45222 non-null object occupation 45222 non-null object relationship 45222 non-null object race 45222 non-null object sex 45222 non-null object capital-gain 45222 non-null float64 capital-loss 45222 non-null float64 hours-per-week 45222 non-null float64 native-country 45222 non-null object income 45222 non-null object dtypes: float64(4), int64(1), object(9) memory usage: 4.8+ MB import matplotlib.pyplot as plt plt . figure(figsize = ( 14 , 10 )) plt . subplot( 2 , 3 , 1 ) plt . title( \"distribution of age\" ) charitydf[ 'age' ] . hist(bins = 100 ) plt . subplot( 2 , 3 , 2 ) plt . title( \"distribution of education-num\" ) charitydf[ 'education-num' ] . hist(bins = 40 ) plt . subplot( 2 , 3 , 3 ) plt . title( \"distribution of capital-gain\" ) charitydf[ 'capital-gain' ] . hist(bins = 100 ) plt . subplot( 2 , 3 , 4 ) plt . title( \"distribution of hours-per-week\" ) charitydf[ 'hours-per-week' ] . hist(bins = 50 ) plt . subplot( 2 , 3 , 5 ) plt . title( \"distribution of capital-loss\" ) charitydf[ 'capital-loss' ] . hist(bins = 50 ) plt . show() &lt;matplotlib.axes._subplots.AxesSubplot at 0x7efd84af9358&gt; plt . figure(figsize = ( 16 , 21 )) sns . set() sns . pairplot(charitydf, hue = \"income\" ) &lt;seaborn.axisgrid.PairGrid at 0x7efd84b28ba8&gt; &lt;matplotlib.figure.Figure at 0x7efd84b197b8&gt; sns . countplot(y = \"marital-status\" , hue = \"income\" , data = charitydf, palette = \"Greens_d\" ); plt . figure(figsize = ( 16 , 21 )) sns . set(style = \"whitegrid\" , color_codes = True ) sns . factorplot( \"sex\" , col = \"marital-status\" , data = charitydf, hue = 'income' , kind = \"count\" , col_wrap = 2 ); &lt;matplotlib.figure.Figure at 0x7efd860fd9b0&gt; plt . figure(figsize = ( 15 , 21 )) plt . subplot( 1 , 2 , 1 ) sns . countplot(y = \"age\" , hue = \"income\" , data = charitydf, palette = \"Greens_d\" ); plt . subplot( 1 , 2 , 2 ) sns . countplot(y = \"hours-per-week\" , hue = \"income\" , data = charitydf, palette = \"Greens_d\" ); plt . figure(figsize = ( 15 , 21 )) sns . jointplot(x = \"age\" , y = \"hours-per-week\" , data = charitydf,size = 15 ,kind = 'reg' ); &lt;matplotlib.figure.Figure at 0x7efd7ee362b0&gt; plt . figure(figsize = ( 10 , 10 )) sns . barplot(x = \"education-num\" , y = \"education_level\" , data = charitydf); plt . figure(figsize = ( 16 , 16 )) plt . subplot( 1 , 2 , 1 ) sns . countplot(y = \"education-num\" , hue = \"income\" , data = charitydf, palette = \"Greens_d\" ); plt . subplot( 1 , 2 , 2 ) sns . countplot(y = \"education_level\" , hue = \"income\" , data = charitydf, palette = \"Greens_d\" );","title":"Data Exploration with Charity Dataset"},{"location":"Pandas/GroupBy/groupby/","text":"Data Handeling: Groupby, Merge, Split import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt % matplotlib inline sns . set() Load data titanic = pd . read_csv( 'data/titanic.csv' ) Setting Name column as index titanic_df1 = titanic . copy(deep = True ) titanic_df1 = titanic . set_index( 'Name' ) titanic_df1 . head( 2 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Sex Age SibSp Parch Ticket Fare Cabin Embarked Name Braund, Mr. Owen Harris 1 0 3 male 22.0 1 0 A/5 21171 7.2500 NaN S Cumings, Mrs. John Bradley (Florence Briggs Thayer) 2 1 1 female 38.0 1 0 PC 17599 71.2833 C85 C Group By : groupby.aggregate() , groupby.size() , groupby.mean() , The groupby operation (split-apply-combine) The \"group by\" concept: we want to apply the same function on subsets of your dataframe, based on some key to split the dataframe in subsets This operation is also referred to as the \"split-apply-combine\" operation, involving the following steps: Splitting the data into groups based on some criteria Applying a function to each group independently Combining the results into a data structure df = pd . DataFrame({ 'key' :[ 'A' , 'B' , 'C' , 'A' , 'B' , 'C' , 'A' , 'B' , 'C' ], 'data' : [ 0 , 5 , 10 , 5 , 10 , 15 , 10 , 15 , 20 ]}) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key data 0 A 0 1 B 5 2 C 10 3 A 5 4 B 10 5 C 15 6 A 10 7 B 15 8 C 20 df . groupby( 'key' ) . aggregate(np . sum) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data key A 15 B 30 C 45 Exercise with Titanic Dataset titanic_df1 . groupby( 'Sex' ) . size() Sex female 314 male 577 dtype: int64 EXERCISE : Using groupby(), calculate the average age for each sex. titanic_df1 . groupby( 'Sex' )[ 'Age' ] . mean() Sex female 27.915709 male 30.726645 Name: Age, dtype: float64 EXERCISE : Calculate the average survival ratio for all passengers. titanic_df1[ 'Survived' ] . sum() / len (titanic_df1[ 'Survived' ]) 0.3838383838383838 EXERCISE : Calculate this survival ratio for all passengers younger that 25 (remember: filtering/boolean indexing). df25 = titanic_df1[titanic_df1[ 'Age' ] <= 25 ] df25[ 'Survived' ] . sum() / len (df25[ 'Survived' ]) 0.4119601328903654 EXERCISE : Is there a difference in this survival ratio between the sexes? (tip: write the above calculation of the survival ratio as a function) def survival_ratio (survived): return survived . sum() / len (survived) titanic_df1 . groupby( 'Sex' )[ 'Survived' ] . aggregate(survival_ratio) Sex female 0.742038 male 0.188908 Name: Survived, dtype: float64 EXERCISE : Make a bar plot of the survival ratio for the different classes ('Pclass' column). titanic_df1 . groupby( 'Pclass' )[ 'Survived' ] . aggregate(survival_ratio) . plot(kind = 'bar' ) &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb321d163c8&gt; Advanced Groupby Operations EXERCISE : Find data for age distribution. type ( 10 // 2 ) int df = titanic_df1 . copy(deep = True ) df . groupby(df . Age // 10 * 10 ) . size() . plot(kind = 'bar' ,figsize = [ 10 , 10 ]) &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb31d63f2b0&gt; EXERCISE : Find data for male age distribution. Male = df[df[ 'Sex' ] == 'male' ] Male . groupby(Male . Age // 10 * 10 ) . size() . plot(kind = 'bar' ,figsize = [ 10 , 10 ]) &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb31d524dd8&gt; EXERCISE : List data with Fare size greater then 50. Fare50 = df[df . Fare > 50 ] Fare50 . groupby([ 'Sex' ]) . size() Sex female 87 male 73 dtype: int64 Fare50 . groupby([ 'Age' , 'Sex' , 'Survived' ]) . size() Age Sex Survived 0.92 male 1 1 2.00 female 0 1 4.00 male 1 1 11.00 male 1 1 14.00 female 1 1 15.00 female 1 1 16.00 female 1 2 17.00 female 1 2 male 1 1 18.00 female 1 3 male 0 2 19.00 female 1 1 male 0 2 21.00 female 1 2 male 0 3 22.00 female 1 3 male 0 1 23.00 female 1 2 male 1 1 24.00 female 1 5 male 0 3 25.00 female 0 1 male 1 2 26.00 female 1 1 male 1 1 27.00 male 0 1 1 2 28.00 male 0 2 29.00 female 1 1 male 0 1 .. 45.00 female 1 1 male 0 1 46.00 male 0 2 47.00 female 1 1 male 0 1 48.00 female 1 1 male 1 2 49.00 female 1 1 male 0 1 1 2 50.00 female 1 1 male 0 2 1 1 51.00 female 1 1 male 0 1 52.00 female 1 2 male 0 1 53.00 female 1 1 54.00 female 1 2 male 0 2 56.00 female 1 1 58.00 female 1 2 male 0 1 60.00 female 1 1 male 1 1 62.00 female 1 1 63.00 female 1 1 64.00 male 0 1 65.00 male 0 1 70.00 male 0 1 Length: 87, dtype: int64 Group by followed by Transformation: groupby.transform() df = pd . DataFrame({ 'key' :[ 'A' , 'B' , 'C' , 'A' , 'B' , 'C' , 'A' , 'B' , 'C' ], 'data' : [ 0 , 5 , 10 , 5 , 10 , 15 , 10 , 15 , 20 ]}) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data key 0 0 A 1 5 B 2 10 C 3 5 A 4 10 B 5 15 C 6 10 A 7 15 B 8 20 C df . groupby( 'key' ) . transform( 'mean' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data 0 5 1 10 2 15 3 5 4 10 5 15 6 5 7 10 8 15 def normalize (group): return (group - group . mean()) / group . std() df . groupby( 'key' ) . transform(normalize) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data 0 -1.0 1 -1.0 2 -1.0 3 0.0 4 0.0 5 0.0 6 1.0 7 1.0 8 1.0 df . groupby( 'key' ) . transform( 'sum' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data 0 15 1 30 2 45 3 15 4 30 5 45 6 15 7 30 8 45","title":"GroupBy"},{"location":"Pandas/GroupBy/groupby/#data-handeling-groupby-merge-split","text":"import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt % matplotlib inline sns . set()","title":"Data Handeling: Groupby, Merge, Split"},{"location":"Pandas/GroupBy/groupby/#load-data","text":"titanic = pd . read_csv( 'data/titanic.csv' )","title":"Load data"},{"location":"Pandas/GroupBy/groupby/#setting-name-column-as-index","text":"titanic_df1 = titanic . copy(deep = True ) titanic_df1 = titanic . set_index( 'Name' ) titanic_df1 . head( 2 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Sex Age SibSp Parch Ticket Fare Cabin Embarked Name Braund, Mr. Owen Harris 1 0 3 male 22.0 1 0 A/5 21171 7.2500 NaN S Cumings, Mrs. John Bradley (Florence Briggs Thayer) 2 1 1 female 38.0 1 0 PC 17599 71.2833 C85 C","title":"Setting Name column as index"},{"location":"Pandas/GroupBy/groupby/#group-by-groupbyaggregate-groupbysizegroupbymean","text":"","title":"Group By : groupby.aggregate(), groupby.size(),groupby.mean(),"},{"location":"Pandas/GroupBy/groupby/#the-groupby-operation-split-apply-combine","text":"The \"group by\" concept: we want to apply the same function on subsets of your dataframe, based on some key to split the dataframe in subsets This operation is also referred to as the \"split-apply-combine\" operation, involving the following steps: Splitting the data into groups based on some criteria Applying a function to each group independently Combining the results into a data structure df = pd . DataFrame({ 'key' :[ 'A' , 'B' , 'C' , 'A' , 'B' , 'C' , 'A' , 'B' , 'C' ], 'data' : [ 0 , 5 , 10 , 5 , 10 , 15 , 10 , 15 , 20 ]}) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key data 0 A 0 1 B 5 2 C 10 3 A 5 4 B 10 5 C 15 6 A 10 7 B 15 8 C 20 df . groupby( 'key' ) . aggregate(np . sum) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data key A 15 B 30 C 45","title":"The groupby operation (split-apply-combine)"},{"location":"Pandas/GroupBy/groupby/#exercise-with-titanic-dataset","text":"titanic_df1 . groupby( 'Sex' ) . size() Sex female 314 male 577 dtype: int64 EXERCISE : Using groupby(), calculate the average age for each sex. titanic_df1 . groupby( 'Sex' )[ 'Age' ] . mean() Sex female 27.915709 male 30.726645 Name: Age, dtype: float64 EXERCISE : Calculate the average survival ratio for all passengers. titanic_df1[ 'Survived' ] . sum() / len (titanic_df1[ 'Survived' ]) 0.3838383838383838 EXERCISE : Calculate this survival ratio for all passengers younger that 25 (remember: filtering/boolean indexing). df25 = titanic_df1[titanic_df1[ 'Age' ] <= 25 ] df25[ 'Survived' ] . sum() / len (df25[ 'Survived' ]) 0.4119601328903654 EXERCISE : Is there a difference in this survival ratio between the sexes? (tip: write the above calculation of the survival ratio as a function) def survival_ratio (survived): return survived . sum() / len (survived) titanic_df1 . groupby( 'Sex' )[ 'Survived' ] . aggregate(survival_ratio) Sex female 0.742038 male 0.188908 Name: Survived, dtype: float64 EXERCISE : Make a bar plot of the survival ratio for the different classes ('Pclass' column). titanic_df1 . groupby( 'Pclass' )[ 'Survived' ] . aggregate(survival_ratio) . plot(kind = 'bar' ) &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb321d163c8&gt;","title":"Exercise with Titanic Dataset"},{"location":"Pandas/GroupBy/groupby/#advanced-groupby-operations","text":"EXERCISE : Find data for age distribution. type ( 10 // 2 ) int df = titanic_df1 . copy(deep = True ) df . groupby(df . Age // 10 * 10 ) . size() . plot(kind = 'bar' ,figsize = [ 10 , 10 ]) &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb31d63f2b0&gt; EXERCISE : Find data for male age distribution. Male = df[df[ 'Sex' ] == 'male' ] Male . groupby(Male . Age // 10 * 10 ) . size() . plot(kind = 'bar' ,figsize = [ 10 , 10 ]) &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb31d524dd8&gt; EXERCISE : List data with Fare size greater then 50. Fare50 = df[df . Fare > 50 ] Fare50 . groupby([ 'Sex' ]) . size() Sex female 87 male 73 dtype: int64 Fare50 . groupby([ 'Age' , 'Sex' , 'Survived' ]) . size() Age Sex Survived 0.92 male 1 1 2.00 female 0 1 4.00 male 1 1 11.00 male 1 1 14.00 female 1 1 15.00 female 1 1 16.00 female 1 2 17.00 female 1 2 male 1 1 18.00 female 1 3 male 0 2 19.00 female 1 1 male 0 2 21.00 female 1 2 male 0 3 22.00 female 1 3 male 0 1 23.00 female 1 2 male 1 1 24.00 female 1 5 male 0 3 25.00 female 0 1 male 1 2 26.00 female 1 1 male 1 1 27.00 male 0 1 1 2 28.00 male 0 2 29.00 female 1 1 male 0 1 .. 45.00 female 1 1 male 0 1 46.00 male 0 2 47.00 female 1 1 male 0 1 48.00 female 1 1 male 1 2 49.00 female 1 1 male 0 1 1 2 50.00 female 1 1 male 0 2 1 1 51.00 female 1 1 male 0 1 52.00 female 1 2 male 0 1 53.00 female 1 1 54.00 female 1 2 male 0 2 56.00 female 1 1 58.00 female 1 2 male 0 1 60.00 female 1 1 male 1 1 62.00 female 1 1 63.00 female 1 1 64.00 male 0 1 65.00 male 0 1 70.00 male 0 1 Length: 87, dtype: int64","title":"Advanced Groupby Operations"},{"location":"Pandas/GroupBy/groupby/#group-by-followed-by-transformation-groupbytransform","text":"df = pd . DataFrame({ 'key' :[ 'A' , 'B' , 'C' , 'A' , 'B' , 'C' , 'A' , 'B' , 'C' ], 'data' : [ 0 , 5 , 10 , 5 , 10 , 15 , 10 , 15 , 20 ]}) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data key 0 0 A 1 5 B 2 10 C 3 5 A 4 10 B 5 15 C 6 10 A 7 15 B 8 20 C df . groupby( 'key' ) . transform( 'mean' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data 0 5 1 10 2 15 3 5 4 10 5 15 6 5 7 10 8 15 def normalize (group): return (group - group . mean()) / group . std() df . groupby( 'key' ) . transform(normalize) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data 0 -1.0 1 -1.0 2 -1.0 3 0.0 4 0.0 5 0.0 6 1.0 7 1.0 8 1.0 df . groupby( 'key' ) . transform( 'sum' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data 0 15 1 30 2 45 3 15 4 30 5 45 6 15 7 30 8 45","title":"Group by followed by Transformation: groupby.transform()"},{"location":"Pandas/Indexing/Indexing/","text":"Data Iteration and Indexing import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt % matplotlib inline titanic = pd . read_csv( 'data/titanic.csv' ) titanic . head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 Braund, Mr. Owen Harris male 22.0 1 0 A/5 21171 7.2500 NaN S 1 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 1 3 Heikkinen, Miss. Laina female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S 3 4 1 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0 1 0 113803 53.1000 C123 S 4 5 0 3 Allen, Mr. William Henry male 35.0 0 0 373450 8.0500 NaN S Setting Name column as index titanic_df1 = titanic . copy(deep = True ) titanic_df1 = titanic . set_index( 'Name' ) titanic_df1 . head( 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Sex Age SibSp Parch Ticket Fare Cabin Embarked Name Braund, Mr. Owen Harris 1 0 3 male 22.0 1 0 A/5 21171 7.2500 NaN S Cumings, Mrs. John Bradley (Florence Briggs Thayer) 2 1 1 female 38.0 1 0 PC 17599 71.2833 C85 C Heikkinen, Miss. Laina 3 1 3 female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S Futrelle, Mrs. Jacques Heath (Lily May Peel) 4 1 1 female 35.0 1 0 113803 53.1000 C123 S Allen, Mr. William Henry 5 0 3 male 35.0 0 0 373450 8.0500 NaN S Data Frame item iteration sample = titanic_df1[ 0 : 5 ] iterrows for index,row in sample . iterrows(): print (index, list (row)) Braund, Mr. Owen Harris [1, 0, 3, 'male', 22.0, 1, 0, 'A/5 21171', 7.25, nan, 'S'] Cumings, Mrs. John Bradley (Florence Briggs Thayer) [2, 1, 1, 'female', 38.0, 1, 0, 'PC 17599', 71.2833, 'C85', 'C'] Heikkinen, Miss. Laina [3, 1, 3, 'female', 26.0, 0, 0, 'STON/O2. 3101282', 7.925, nan, 'S'] Futrelle, Mrs. Jacques Heath (Lily May Peel) [4, 1, 1, 'female', 35.0, 1, 0, '113803', 53.1, 'C123', 'S'] Allen, Mr. William Henry [5, 0, 3, 'male', 35.0, 0, 0, '373450', 8.05, nan, 'S'] for index,row in sample . iterrows(): print (index,row[ 'Sex' ],row[ 'Age' ]) Braund, Mr. Owen Harris male 22.0 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female 38.0 Heikkinen, Miss. Laina female 26.0 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0 Allen, Mr. William Henry male 35.0 iteritems for index,row in sample . T . iteritems(): print (index, list (row)) Braund, Mr. Owen Harris [1, 0, 3, 'male', 22.0, 1, 0, 'A/5 21171', 7.25, nan, 'S'] Cumings, Mrs. John Bradley (Florence Briggs Thayer) [2, 1, 1, 'female', 38.0, 1, 0, 'PC 17599', 71.2833, 'C85', 'C'] Heikkinen, Miss. Laina [3, 1, 3, 'female', 26.0, 0, 0, 'STON/O2. 3101282', 7.925, nan, 'S'] Futrelle, Mrs. Jacques Heath (Lily May Peel) [4, 1, 1, 'female', 35.0, 1, 0, '113803', 53.1, 'C123', 'S'] Allen, Mr. William Henry [5, 0, 3, 'male', 35.0, 0, 0, '373450', 8.05, nan, 'S'] for index,row in sample . iteritems(): print (index,row[ 0 ],row[ 1 ],row[ 2 ]) PassengerId 1 2 3 Survived 0 1 1 Pclass 3 1 3 Sex male female female Age 22.0 38.0 26.0 SibSp 1 1 0 Parch 0 0 0 Ticket A/5 21171 PC 17599 STON/O2. 3101282 Fare 7.25 71.2833 7.925 Cabin nan C85 nan Embarked S C S Indexing Data Source: Using iloc, loc, & ix to select rows and columns in Pandas DataFrames loc and iloc : The iloc indexer for Pandas Dataframe is used for integer-location based indexing / selection by position. The Pandas loc indexer can be used with DataFrames for two different use cases: a.) Selecting rows by label/index b.) Selecting rows with a boolean / conditional lookup sample . iloc[ 0 : 2 ,:] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Sex Age SibSp Parch Ticket Fare Cabin Embarked Name Braund, Mr. Owen Harris 1 0 3 male 22.0 1 0 A/5 21171 7.2500 NaN S Cumings, Mrs. John Bradley (Florence Briggs Thayer) 2 1 1 female 38.0 1 0 PC 17599 71.2833 C85 C sample . iloc[ 1 , 0 : 3 ] PassengerId 2 Survived 1 Pclass 1 Name: Cumings, Mrs. John Bradley (Florence Briggs Thayer), dtype: object sample . loc[:, 'Survived' : 'Ticket' ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Survived Pclass Sex Age SibSp Parch Ticket Name Braund, Mr. Owen Harris 0 3 male 22.0 1 0 A/5 21171 Cumings, Mrs. John Bradley (Florence Briggs Thayer) 1 1 female 38.0 1 0 PC 17599 Heikkinen, Miss. Laina 1 3 female 26.0 0 0 STON/O2. 3101282 Futrelle, Mrs. Jacques Heath (Lily May Peel) 1 1 female 35.0 1 0 113803 Allen, Mr. William Henry 0 3 male 35.0 0 0 373450 sample . loc[ 'Braund, Mr. Owen Harris' ,:] PassengerId 1 Survived 0 Pclass 3 Sex male Age 22 SibSp 1 Parch 0 Ticket A/5 21171 Fare 7.25 Cabin NaN Embarked S Name: Braund, Mr. Owen Harris, dtype: object Data Filters Dictionary to DataFrame data = { 'country' : [ 'Belgium' , 'France' , 'Germany' , 'Netherlands' , 'United Kingdom' ], 'population' : [ 11.3 , 64.3 , 81.3 , 16.9 , 64.9 ], 'area' : [ 30510 , 671308 , 357050 , 41526 , 244820 ], 'capital' : [ 'Brussels' , 'Paris' , 'Berlin' , 'Amsterdam' , 'London' ]} countries = pd . DataFrame(data) countries .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } country population area capital 0 Belgium 11.3 30510 Brussels 1 France 64.3 671308 Paris 2 Germany 81.3 357050 Berlin 3 Netherlands 16.9 41526 Amsterdam 4 United Kingdom 64.9 244820 London countries = countries . set_index( 'country' ) countries .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } population area capital country Belgium 11.3 30510 Brussels France 64.3 671308 Paris Germany 81.3 357050 Berlin Netherlands 16.9 41526 Amsterdam United Kingdom 64.9 244820 London countries[ 'area' ] country Belgium 30510 France 671308 Germany 357050 Netherlands 41526 United Kingdom 244820 Name: area, dtype: int64 countries[[ 'area' , 'population' ]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } area population country Belgium 30510 11.3 France 671308 64.3 Germany 357050 81.3 Netherlands 41526 16.9 United Kingdom 244820 64.9 countries[ 'France' : 'Netherlands' ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } population area capital country France 64.3 671308 Paris Germany 81.3 357050 Berlin Netherlands 16.9 41526 Amsterdam countries . loc[ 'Germany' , 'area' ] 357050 countries . loc[ 'France' : 'Germany' , [ 'area' , 'population' ]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } area population country France 671308 64.3 Germany 357050 81.3 countries . iloc[ 0 : 2 , 1 : 3 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } area capital country Belgium 30510 Brussels France 671308 Paris countries2 = countries . copy() countries2 . loc[ 'Belgium' : 'Germany' , 'population' ] = 10 countries2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } population area capital country Belgium 10.0 30510 Brussels France 10.0 671308 Paris Germany 10.0 357050 Berlin Netherlands 16.9 41526 Amsterdam United Kingdom 64.9 244820 London countries[ 'area' ] > 100000 country Belgium False France True Germany True Netherlands False United Kingdom True Name: area, dtype: bool countries[countries[ 'area' ] > 100000 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } population area capital country France 64.3 671308 Paris Germany 81.3 357050 Berlin United Kingdom 64.9 244820 London EXERCISE : Add a column `density` with the population density (note: population column is expressed in millions) countries[ 'density' ] = countries[ 'population' ] * 1000000 / countries[ 'area' ] countries .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } population area capital density country Belgium 11.3 30510 Brussels 370.370370 France 64.3 671308 Paris 95.783158 Germany 81.3 357050 Berlin 227.699202 Netherlands 16.9 41526 Amsterdam 406.973944 United Kingdom 64.9 244820 London 265.092721 EXERCISE : Select the capital and the population column of those countries where the density is larger than 300 countries . loc[countries[ 'density' ] > 300 , [ 'capital' , 'population' ]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } capital population country Belgium Brussels 11.3 Netherlands Amsterdam 16.9 EXERCISE : Add a column 'density_ratio' with the ratio of the density to the mean density countries[ 'density_ratio' ] = countries[ 'density' ] / countries[ 'density' ] . mean() countries .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } population area capital density density_ratio country Belgium 11.3 30510 Brussels 370.370370 1.355755 France 64.3 671308 Paris 95.783158 0.350618 Germany 81.3 357050 Berlin 227.699202 0.833502 Netherlands 16.9 41526 Amsterdam 406.973944 1.489744 United Kingdom 64.9 244820 London 265.092721 0.970382 EXERCISE : Change the capital of the UK to Cambridge countries . loc[ 'United Kingdom' , 'capital' ] = 'Cambridge' countries .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } population area capital density density_ratio country Belgium 11.3 30510 Brussels 370.370370 1.355755 France 64.3 671308 Paris 95.783158 0.350618 Germany 81.3 357050 Berlin 227.699202 0.833502 Netherlands 16.9 41526 Amsterdam 406.973944 1.489744 United Kingdom 64.9 244820 Cambridge 265.092721 0.970382 EXERCISE : Select all countries whose population density is between 100 and 300 people/km\u00b2 countries[(countries[ 'density' ] > 100 ) & (countries[ 'density' ] < 300 )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } population area capital density density_ratio country Germany 81.3 357050 Berlin 227.699202 0.833502 United Kingdom 64.9 244820 Cambridge 265.092721 0.970382","title":"Indexing"},{"location":"Pandas/Indexing/Indexing/#data-iteration-and-indexing","text":"import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt % matplotlib inline titanic = pd . read_csv( 'data/titanic.csv' ) titanic . head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 Braund, Mr. Owen Harris male 22.0 1 0 A/5 21171 7.2500 NaN S 1 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 1 3 Heikkinen, Miss. Laina female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S 3 4 1 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0 1 0 113803 53.1000 C123 S 4 5 0 3 Allen, Mr. William Henry male 35.0 0 0 373450 8.0500 NaN S","title":"Data Iteration and Indexing"},{"location":"Pandas/Indexing/Indexing/#setting-name-column-as-index","text":"titanic_df1 = titanic . copy(deep = True ) titanic_df1 = titanic . set_index( 'Name' ) titanic_df1 . head( 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Sex Age SibSp Parch Ticket Fare Cabin Embarked Name Braund, Mr. Owen Harris 1 0 3 male 22.0 1 0 A/5 21171 7.2500 NaN S Cumings, Mrs. John Bradley (Florence Briggs Thayer) 2 1 1 female 38.0 1 0 PC 17599 71.2833 C85 C Heikkinen, Miss. Laina 3 1 3 female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S Futrelle, Mrs. Jacques Heath (Lily May Peel) 4 1 1 female 35.0 1 0 113803 53.1000 C123 S Allen, Mr. William Henry 5 0 3 male 35.0 0 0 373450 8.0500 NaN S","title":"Setting Name column as index"},{"location":"Pandas/Indexing/Indexing/#data-frame-item-iteration","text":"sample = titanic_df1[ 0 : 5 ]","title":"Data Frame item iteration"},{"location":"Pandas/Indexing/Indexing/#iterrows","text":"for index,row in sample . iterrows(): print (index, list (row)) Braund, Mr. Owen Harris [1, 0, 3, 'male', 22.0, 1, 0, 'A/5 21171', 7.25, nan, 'S'] Cumings, Mrs. John Bradley (Florence Briggs Thayer) [2, 1, 1, 'female', 38.0, 1, 0, 'PC 17599', 71.2833, 'C85', 'C'] Heikkinen, Miss. Laina [3, 1, 3, 'female', 26.0, 0, 0, 'STON/O2. 3101282', 7.925, nan, 'S'] Futrelle, Mrs. Jacques Heath (Lily May Peel) [4, 1, 1, 'female', 35.0, 1, 0, '113803', 53.1, 'C123', 'S'] Allen, Mr. William Henry [5, 0, 3, 'male', 35.0, 0, 0, '373450', 8.05, nan, 'S'] for index,row in sample . iterrows(): print (index,row[ 'Sex' ],row[ 'Age' ]) Braund, Mr. Owen Harris male 22.0 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female 38.0 Heikkinen, Miss. Laina female 26.0 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0 Allen, Mr. William Henry male 35.0","title":"iterrows"},{"location":"Pandas/Indexing/Indexing/#iteritems","text":"for index,row in sample . T . iteritems(): print (index, list (row)) Braund, Mr. Owen Harris [1, 0, 3, 'male', 22.0, 1, 0, 'A/5 21171', 7.25, nan, 'S'] Cumings, Mrs. John Bradley (Florence Briggs Thayer) [2, 1, 1, 'female', 38.0, 1, 0, 'PC 17599', 71.2833, 'C85', 'C'] Heikkinen, Miss. Laina [3, 1, 3, 'female', 26.0, 0, 0, 'STON/O2. 3101282', 7.925, nan, 'S'] Futrelle, Mrs. Jacques Heath (Lily May Peel) [4, 1, 1, 'female', 35.0, 1, 0, '113803', 53.1, 'C123', 'S'] Allen, Mr. William Henry [5, 0, 3, 'male', 35.0, 0, 0, '373450', 8.05, nan, 'S'] for index,row in sample . iteritems(): print (index,row[ 0 ],row[ 1 ],row[ 2 ]) PassengerId 1 2 3 Survived 0 1 1 Pclass 3 1 3 Sex male female female Age 22.0 38.0 26.0 SibSp 1 1 0 Parch 0 0 0 Ticket A/5 21171 PC 17599 STON/O2. 3101282 Fare 7.25 71.2833 7.925 Cabin nan C85 nan Embarked S C S","title":"iteritems"},{"location":"Pandas/Indexing/Indexing/#indexing-data","text":"Source: Using iloc, loc, & ix to select rows and columns in Pandas DataFrames","title":"Indexing Data"},{"location":"Pandas/Indexing/Indexing/#loc-and-iloc","text":"The iloc indexer for Pandas Dataframe is used for integer-location based indexing / selection by position. The Pandas loc indexer can be used with DataFrames for two different use cases: a.) Selecting rows by label/index b.) Selecting rows with a boolean / conditional lookup sample . iloc[ 0 : 2 ,:] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Sex Age SibSp Parch Ticket Fare Cabin Embarked Name Braund, Mr. Owen Harris 1 0 3 male 22.0 1 0 A/5 21171 7.2500 NaN S Cumings, Mrs. John Bradley (Florence Briggs Thayer) 2 1 1 female 38.0 1 0 PC 17599 71.2833 C85 C sample . iloc[ 1 , 0 : 3 ] PassengerId 2 Survived 1 Pclass 1 Name: Cumings, Mrs. John Bradley (Florence Briggs Thayer), dtype: object sample . loc[:, 'Survived' : 'Ticket' ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Survived Pclass Sex Age SibSp Parch Ticket Name Braund, Mr. Owen Harris 0 3 male 22.0 1 0 A/5 21171 Cumings, Mrs. John Bradley (Florence Briggs Thayer) 1 1 female 38.0 1 0 PC 17599 Heikkinen, Miss. Laina 1 3 female 26.0 0 0 STON/O2. 3101282 Futrelle, Mrs. Jacques Heath (Lily May Peel) 1 1 female 35.0 1 0 113803 Allen, Mr. William Henry 0 3 male 35.0 0 0 373450 sample . loc[ 'Braund, Mr. Owen Harris' ,:] PassengerId 1 Survived 0 Pclass 3 Sex male Age 22 SibSp 1 Parch 0 Ticket A/5 21171 Fare 7.25 Cabin NaN Embarked S Name: Braund, Mr. Owen Harris, dtype: object","title":"loc and iloc :"},{"location":"Pandas/Indexing/Indexing/#data-filters","text":"","title":"Data Filters"},{"location":"Pandas/Indexing/Indexing/#dictionary-to-dataframe","text":"data = { 'country' : [ 'Belgium' , 'France' , 'Germany' , 'Netherlands' , 'United Kingdom' ], 'population' : [ 11.3 , 64.3 , 81.3 , 16.9 , 64.9 ], 'area' : [ 30510 , 671308 , 357050 , 41526 , 244820 ], 'capital' : [ 'Brussels' , 'Paris' , 'Berlin' , 'Amsterdam' , 'London' ]} countries = pd . DataFrame(data) countries .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } country population area capital 0 Belgium 11.3 30510 Brussels 1 France 64.3 671308 Paris 2 Germany 81.3 357050 Berlin 3 Netherlands 16.9 41526 Amsterdam 4 United Kingdom 64.9 244820 London countries = countries . set_index( 'country' ) countries .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } population area capital country Belgium 11.3 30510 Brussels France 64.3 671308 Paris Germany 81.3 357050 Berlin Netherlands 16.9 41526 Amsterdam United Kingdom 64.9 244820 London countries[ 'area' ] country Belgium 30510 France 671308 Germany 357050 Netherlands 41526 United Kingdom 244820 Name: area, dtype: int64 countries[[ 'area' , 'population' ]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } area population country Belgium 30510 11.3 France 671308 64.3 Germany 357050 81.3 Netherlands 41526 16.9 United Kingdom 244820 64.9 countries[ 'France' : 'Netherlands' ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } population area capital country France 64.3 671308 Paris Germany 81.3 357050 Berlin Netherlands 16.9 41526 Amsterdam countries . loc[ 'Germany' , 'area' ] 357050 countries . loc[ 'France' : 'Germany' , [ 'area' , 'population' ]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } area population country France 671308 64.3 Germany 357050 81.3 countries . iloc[ 0 : 2 , 1 : 3 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } area capital country Belgium 30510 Brussels France 671308 Paris countries2 = countries . copy() countries2 . loc[ 'Belgium' : 'Germany' , 'population' ] = 10 countries2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } population area capital country Belgium 10.0 30510 Brussels France 10.0 671308 Paris Germany 10.0 357050 Berlin Netherlands 16.9 41526 Amsterdam United Kingdom 64.9 244820 London countries[ 'area' ] > 100000 country Belgium False France True Germany True Netherlands False United Kingdom True Name: area, dtype: bool countries[countries[ 'area' ] > 100000 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } population area capital country France 64.3 671308 Paris Germany 81.3 357050 Berlin United Kingdom 64.9 244820 London EXERCISE : Add a column `density` with the population density (note: population column is expressed in millions) countries[ 'density' ] = countries[ 'population' ] * 1000000 / countries[ 'area' ] countries .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } population area capital density country Belgium 11.3 30510 Brussels 370.370370 France 64.3 671308 Paris 95.783158 Germany 81.3 357050 Berlin 227.699202 Netherlands 16.9 41526 Amsterdam 406.973944 United Kingdom 64.9 244820 London 265.092721 EXERCISE : Select the capital and the population column of those countries where the density is larger than 300 countries . loc[countries[ 'density' ] > 300 , [ 'capital' , 'population' ]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } capital population country Belgium Brussels 11.3 Netherlands Amsterdam 16.9 EXERCISE : Add a column 'density_ratio' with the ratio of the density to the mean density countries[ 'density_ratio' ] = countries[ 'density' ] / countries[ 'density' ] . mean() countries .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } population area capital density density_ratio country Belgium 11.3 30510 Brussels 370.370370 1.355755 France 64.3 671308 Paris 95.783158 0.350618 Germany 81.3 357050 Berlin 227.699202 0.833502 Netherlands 16.9 41526 Amsterdam 406.973944 1.489744 United Kingdom 64.9 244820 London 265.092721 0.970382 EXERCISE : Change the capital of the UK to Cambridge countries . loc[ 'United Kingdom' , 'capital' ] = 'Cambridge' countries .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } population area capital density density_ratio country Belgium 11.3 30510 Brussels 370.370370 1.355755 France 64.3 671308 Paris 95.783158 0.350618 Germany 81.3 357050 Berlin 227.699202 0.833502 Netherlands 16.9 41526 Amsterdam 406.973944 1.489744 United Kingdom 64.9 244820 Cambridge 265.092721 0.970382 EXERCISE : Select all countries whose population density is between 100 and 300 people/km\u00b2 countries[(countries[ 'density' ] > 100 ) & (countries[ 'density' ] < 300 )] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } population area capital density density_ratio country Germany 81.3 357050 Berlin 227.699202 0.833502 United Kingdom 64.9 244820 Cambridge 265.092721 0.970382","title":"Dictionary to DataFrame"},{"location":"Pandas/Lambda/lambda/","text":"Lambda Transformation import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt % matplotlib inline Load data titanic = pd . read_csv( 'data/titanic.csv' ) Setting Name column as index df1 = titanic . copy(deep = True ) df1 = titanic . set_index( 'Name' ) df1 . head( 2 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Sex Age SibSp Parch Ticket Fare Cabin Embarked Name Braund, Mr. Owen Harris 1 0 3 male 22.0 1 0 A/5 21171 7.2500 NaN S Cumings, Mrs. John Bradley (Florence Briggs Thayer) 2 1 1 female 38.0 1 0 PC 17599 71.2833 C85 C Lambda Transformation df1[ 'Fare' ] . apply( lambda x: ( 10 * x ** 2 + 2 * x + 4 ) / 10 ) Name Braund, Mr. Owen Harris 0.72500 Cumings, Mrs. John Bradley (Florence Briggs Thayer) 7.12833 Heikkinen, Miss. Laina 0.79250 Futrelle, Mrs. Jacques Heath (Lily May Peel) 5.31000 Allen, Mr. William Henry 0.80500 Moran, Mr. James 0.84583 McCarthy, Mr. Timothy J 5.18625 Palsson, Master. Gosta Leonard 2.10750 Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg) 1.11333 Nasser, Mrs. Nicholas (Adele Achem) 3.00708 Sandstrom, Miss. Marguerite Rut 1.67000 Bonnell, Miss. Elizabeth 2.65500 Saundercock, Mr. William Henry 0.80500 Andersson, Mr. Anders Johan 3.12750 Vestrom, Miss. Hulda Amanda Adolfina 0.78542 Hewlett, Mrs. (Mary D Kingcome) 1.60000 Rice, Master. Eugene 2.91250 Williams, Mr. Charles Eugene 1.30000 Vander Planke, Mrs. Julius (Emelia Maria Vandemoortele) 1.80000 Masselmani, Mrs. Fatima 0.72250 Fynney, Mr. Joseph J 2.60000 Beesley, Mr. Lawrence 1.30000 McGowan, Miss. Anna \"Annie\" 0.80292 Sloper, Mr. William Thompson 3.55000 Palsson, Miss. Torborg Danira 2.10750 Asplund, Mrs. Carl Oscar (Selma Augusta Emilia Johansson) 3.13875 Emir, Mr. Farred Chehab 0.72250 Fortune, Mr. Charles Alexander 26.30000 O'Dwyer, Miss. Ellen \"Nellie\" 0.78792 Todoroff, Mr. Lalio 0.78958 ... Giles, Mr. Frederick Edward 1.15000 Swift, Mrs. Frederick Joel (Margaret Welles Barron) 2.59292 Sage, Miss. Dorothy Edith \"Dolly\" 6.95500 Gill, Mr. John William 1.30000 Bystrom, Mrs. (Karolina) 1.30000 Duran y More, Miss. Asuncion 1.38583 Roebling, Mr. Washington Augustus II 5.04958 van Melkebeke, Mr. Philemon 0.95000 Johnson, Master. Harold Theodor 1.11333 Balkic, Mr. Cerin 0.78958 Beckwith, Mrs. Richard Leonard (Sallie Monypeny) 5.25542 Carlsson, Mr. Frans Olof 0.50000 Vander Cruyssen, Mr. Victor 0.90000 Abelson, Mrs. Samuel (Hannah Wizosky) 2.40000 Najib, Miss. Adele Kiamie \"Jane\" 0.72250 Gustafsson, Mr. Alfred Ossian 0.98458 Petroff, Mr. Nedelio 0.78958 Laleff, Mr. Kristo 0.78958 Potter, Mrs. Thomas Jr (Lily Alexenia Wilson) 8.31583 Shelley, Mrs. William (Imanita Parrish Hall) 2.60000 Markun, Mr. Johann 0.78958 Dahlberg, Miss. Gerda Ulrika 1.05167 Banfield, Mr. Frederick James 1.05000 Sutehall, Mr. Henry Jr 0.70500 Rice, Mrs. William (Margaret Norton) 2.91250 Montvila, Rev. Juozas 1.30000 Graham, Miss. Margaret Edith 3.00000 Johnston, Miss. Catherine Helen \"Carrie\" 2.34500 Behr, Mr. Karl Howell 3.00000 Dooley, Mr. Patrick 0.77500 Name: Fare, Length: 891, dtype: float64 def newfeature (x): return 10 + x / 3 + x ** 2 df1[ 'Fare' ] . apply(newfeature) Name Braund, Mr. Owen Harris 64.979167 Cumings, Mrs. John Bradley (Florence Briggs Thayer) 5115.069959 Heikkinen, Miss. Laina 75.447292 Futrelle, Mrs. Jacques Heath (Lily May Peel) 2847.310000 Allen, Mr. William Henry 77.485833 Moran, Mr. James 84.362272 McCarthy, Mr. Timothy J 2717.006406 Palsson, Master. Gosta Leonard 461.180625 Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg) 137.661469 Nasser, Mrs. Nicholas (Adele Achem) 924.276613 Sandstrom, Miss. Marguerite Rut 294.456667 Bonnell, Miss. Elizabeth 723.752500 Saundercock, Mr. William Henry 77.485833 Andersson, Mr. Anders Johan 998.550625 Vestrom, Miss. Hulda Amanda Adolfina 74.306524 Hewlett, Mrs. (Mary D Kingcome) 271.333333 Rice, Master. Eugene 867.973958 Williams, Mr. Charles Eugene 183.333333 Vander Planke, Mrs. Julius (Emelia Maria Vandemoortele) 340.000000 Masselmani, Mrs. Fatima 64.608958 Fynney, Mr. Joseph J 694.666667 Beesley, Mr. Lawrence 183.333333 McGowan, Miss. Anna \"Annie\" 77.144453 Sloper, Mr. William Thompson 1282.083333 Palsson, Miss. Torborg Danira 461.180625 Asplund, Mrs. Carl Oscar (Selma Augusta Emilia Johansson) 1005.637656 Emir, Mr. Farred Chehab 64.608958 Fortune, Mr. Charles Alexander 69266.666667 O'Dwyer, Miss. Ellen \"Nellie\" 74.708193 Todoroff, Mr. Lalio 74.975591 ... Giles, Mr. Frederick Edward 146.083333 Swift, Mrs. Frederick Joel (Margaret Welles Barron) 690.966479 Sage, Miss. Dorothy Edith \"Dolly\" 4870.385833 Gill, Mr. John William 183.333333 Bystrom, Mrs. (Karolina) 183.333333 Duran y More, Miss. Asuncion 206.671912 Roebling, Mr. Washington Augustus II 2576.657751 van Melkebeke, Mr. Philemon 103.416667 Johnson, Master. Harold Theodor 137.661469 Balkic, Mr. Cerin 74.975591 Beckwith, Mrs. Richard Leonard (Sallie Monypeny) 2789.462004 Carlsson, Mr. Frans Olof 36.666667 Vander Cruyssen, Mr. Victor 94.000000 Abelson, Mrs. Samuel (Hannah Wizosky) 594.000000 Najib, Miss. Adele Kiamie \"Jane\" 64.608958 Gustafsson, Mr. Alfred Ossian 110.221711 Petroff, Mr. Nedelio 74.975591 Laleff, Mr. Kristo 74.975591 Potter, Mrs. Thomas Jr (Lily Alexenia Wilson) 6953.022292 Shelley, Mrs. William (Imanita Parrish Hall) 694.666667 Markun, Mr. Johann 74.975591 Dahlberg, Miss. Gerda Ulrika 124.106546 Banfield, Mr. Frederick James 123.750000 Sutehall, Mr. Henry Jr 62.052500 Rice, Mrs. William (Margaret Norton) 867.973958 Montvila, Rev. Juozas 183.333333 Graham, Miss. Margaret Edith 920.000000 Johnston, Miss. Catherine Helen \"Carrie\" 567.719167 Behr, Mr. Karl Howell 920.000000 Dooley, Mr. Patrick 72.645833 Name: Fare, Length: 891, dtype: float64","title":"Lambda Transform"},{"location":"Pandas/Lambda/lambda/#lambda-transformation","text":"import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt % matplotlib inline","title":"Lambda Transformation"},{"location":"Pandas/Lambda/lambda/#load-data","text":"titanic = pd . read_csv( 'data/titanic.csv' )","title":"Load data"},{"location":"Pandas/Lambda/lambda/#setting-name-column-as-index","text":"df1 = titanic . copy(deep = True ) df1 = titanic . set_index( 'Name' ) df1 . head( 2 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Sex Age SibSp Parch Ticket Fare Cabin Embarked Name Braund, Mr. Owen Harris 1 0 3 male 22.0 1 0 A/5 21171 7.2500 NaN S Cumings, Mrs. John Bradley (Florence Briggs Thayer) 2 1 1 female 38.0 1 0 PC 17599 71.2833 C85 C","title":"Setting Name column as index"},{"location":"Pandas/Lambda/lambda/#lambda-transformation_1","text":"df1[ 'Fare' ] . apply( lambda x: ( 10 * x ** 2 + 2 * x + 4 ) / 10 ) Name Braund, Mr. Owen Harris 0.72500 Cumings, Mrs. John Bradley (Florence Briggs Thayer) 7.12833 Heikkinen, Miss. Laina 0.79250 Futrelle, Mrs. Jacques Heath (Lily May Peel) 5.31000 Allen, Mr. William Henry 0.80500 Moran, Mr. James 0.84583 McCarthy, Mr. Timothy J 5.18625 Palsson, Master. Gosta Leonard 2.10750 Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg) 1.11333 Nasser, Mrs. Nicholas (Adele Achem) 3.00708 Sandstrom, Miss. Marguerite Rut 1.67000 Bonnell, Miss. Elizabeth 2.65500 Saundercock, Mr. William Henry 0.80500 Andersson, Mr. Anders Johan 3.12750 Vestrom, Miss. Hulda Amanda Adolfina 0.78542 Hewlett, Mrs. (Mary D Kingcome) 1.60000 Rice, Master. Eugene 2.91250 Williams, Mr. Charles Eugene 1.30000 Vander Planke, Mrs. Julius (Emelia Maria Vandemoortele) 1.80000 Masselmani, Mrs. Fatima 0.72250 Fynney, Mr. Joseph J 2.60000 Beesley, Mr. Lawrence 1.30000 McGowan, Miss. Anna \"Annie\" 0.80292 Sloper, Mr. William Thompson 3.55000 Palsson, Miss. Torborg Danira 2.10750 Asplund, Mrs. Carl Oscar (Selma Augusta Emilia Johansson) 3.13875 Emir, Mr. Farred Chehab 0.72250 Fortune, Mr. Charles Alexander 26.30000 O'Dwyer, Miss. Ellen \"Nellie\" 0.78792 Todoroff, Mr. Lalio 0.78958 ... Giles, Mr. Frederick Edward 1.15000 Swift, Mrs. Frederick Joel (Margaret Welles Barron) 2.59292 Sage, Miss. Dorothy Edith \"Dolly\" 6.95500 Gill, Mr. John William 1.30000 Bystrom, Mrs. (Karolina) 1.30000 Duran y More, Miss. Asuncion 1.38583 Roebling, Mr. Washington Augustus II 5.04958 van Melkebeke, Mr. Philemon 0.95000 Johnson, Master. Harold Theodor 1.11333 Balkic, Mr. Cerin 0.78958 Beckwith, Mrs. Richard Leonard (Sallie Monypeny) 5.25542 Carlsson, Mr. Frans Olof 0.50000 Vander Cruyssen, Mr. Victor 0.90000 Abelson, Mrs. Samuel (Hannah Wizosky) 2.40000 Najib, Miss. Adele Kiamie \"Jane\" 0.72250 Gustafsson, Mr. Alfred Ossian 0.98458 Petroff, Mr. Nedelio 0.78958 Laleff, Mr. Kristo 0.78958 Potter, Mrs. Thomas Jr (Lily Alexenia Wilson) 8.31583 Shelley, Mrs. William (Imanita Parrish Hall) 2.60000 Markun, Mr. Johann 0.78958 Dahlberg, Miss. Gerda Ulrika 1.05167 Banfield, Mr. Frederick James 1.05000 Sutehall, Mr. Henry Jr 0.70500 Rice, Mrs. William (Margaret Norton) 2.91250 Montvila, Rev. Juozas 1.30000 Graham, Miss. Margaret Edith 3.00000 Johnston, Miss. Catherine Helen \"Carrie\" 2.34500 Behr, Mr. Karl Howell 3.00000 Dooley, Mr. Patrick 0.77500 Name: Fare, Length: 891, dtype: float64 def newfeature (x): return 10 + x / 3 + x ** 2 df1[ 'Fare' ] . apply(newfeature) Name Braund, Mr. Owen Harris 64.979167 Cumings, Mrs. John Bradley (Florence Briggs Thayer) 5115.069959 Heikkinen, Miss. Laina 75.447292 Futrelle, Mrs. Jacques Heath (Lily May Peel) 2847.310000 Allen, Mr. William Henry 77.485833 Moran, Mr. James 84.362272 McCarthy, Mr. Timothy J 2717.006406 Palsson, Master. Gosta Leonard 461.180625 Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg) 137.661469 Nasser, Mrs. Nicholas (Adele Achem) 924.276613 Sandstrom, Miss. Marguerite Rut 294.456667 Bonnell, Miss. Elizabeth 723.752500 Saundercock, Mr. William Henry 77.485833 Andersson, Mr. Anders Johan 998.550625 Vestrom, Miss. Hulda Amanda Adolfina 74.306524 Hewlett, Mrs. (Mary D Kingcome) 271.333333 Rice, Master. Eugene 867.973958 Williams, Mr. Charles Eugene 183.333333 Vander Planke, Mrs. Julius (Emelia Maria Vandemoortele) 340.000000 Masselmani, Mrs. Fatima 64.608958 Fynney, Mr. Joseph J 694.666667 Beesley, Mr. Lawrence 183.333333 McGowan, Miss. Anna \"Annie\" 77.144453 Sloper, Mr. William Thompson 1282.083333 Palsson, Miss. Torborg Danira 461.180625 Asplund, Mrs. Carl Oscar (Selma Augusta Emilia Johansson) 1005.637656 Emir, Mr. Farred Chehab 64.608958 Fortune, Mr. Charles Alexander 69266.666667 O'Dwyer, Miss. Ellen \"Nellie\" 74.708193 Todoroff, Mr. Lalio 74.975591 ... Giles, Mr. Frederick Edward 146.083333 Swift, Mrs. Frederick Joel (Margaret Welles Barron) 690.966479 Sage, Miss. Dorothy Edith \"Dolly\" 4870.385833 Gill, Mr. John William 183.333333 Bystrom, Mrs. (Karolina) 183.333333 Duran y More, Miss. Asuncion 206.671912 Roebling, Mr. Washington Augustus II 2576.657751 van Melkebeke, Mr. Philemon 103.416667 Johnson, Master. Harold Theodor 137.661469 Balkic, Mr. Cerin 74.975591 Beckwith, Mrs. Richard Leonard (Sallie Monypeny) 2789.462004 Carlsson, Mr. Frans Olof 36.666667 Vander Cruyssen, Mr. Victor 94.000000 Abelson, Mrs. Samuel (Hannah Wizosky) 594.000000 Najib, Miss. Adele Kiamie \"Jane\" 64.608958 Gustafsson, Mr. Alfred Ossian 110.221711 Petroff, Mr. Nedelio 74.975591 Laleff, Mr. Kristo 74.975591 Potter, Mrs. Thomas Jr (Lily Alexenia Wilson) 6953.022292 Shelley, Mrs. William (Imanita Parrish Hall) 694.666667 Markun, Mr. Johann 74.975591 Dahlberg, Miss. Gerda Ulrika 124.106546 Banfield, Mr. Frederick James 123.750000 Sutehall, Mr. Henry Jr 62.052500 Rice, Mrs. William (Margaret Norton) 867.973958 Montvila, Rev. Juozas 183.333333 Graham, Miss. Margaret Edith 920.000000 Johnston, Miss. Catherine Helen \"Carrie\" 567.719167 Behr, Mr. Karl Howell 920.000000 Dooley, Mr. Patrick 72.645833 Name: Fare, Length: 891, dtype: float64","title":"Lambda Transformation"},{"location":"Projects/Diffusion/Diffusion/","text":"Diffusion Patterns https://scipython.com/book/chapter-7-matplotlib/examples/the-two-dimensional-diffusion-equation/ import numpy as np import json import random as random import math as math import matplotlib.pyplot as plt import seaborn as sns sns . set() Consider a 2D lattice of length L L = 10 Create initial configuration: We can use a vacant list to create initial configuration where initially particle is at middle of the lattice. def start (L): '''create a vacant list of list ''' P = [[ 0 for i in range (L)] for j in range (L)] '''put particle at center''' P[ int (L / 2 )][ int (L / 2 )] = 1 return P P = start(L) P [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]] Make a plot of the lattice. plt . figure(figsize = [ 8 , 6 ]) sns . heatmap(P,annot = True ,cmap = 'YlGn' ) &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a1dafde48&gt; Create a function to diffuse a particle: $ P[i,j] = P[i+1,j] + P[i-1,j] + P[i,j+1] + P[i,j-1]$ def diffuse_primitive (P,L): '''create vacant list of list''' PP = [[ 0 for i in range (L)] for j in range (L)] for i in range (L): for j in range (L): '''diffuse one step''' PP[i][j] = P[i + 1 ][j] + P[i - 1 ][j] + P[i][j + 1 ] + P[i][j - 1 ] '''normalize''' PP = PP / np . sum(PP) return PP L = 10 P = start(L) #P = diffuse_primitive(P,L) Set boundary conditons Lower limit P[0-1,j] = P[L,j] P[I,0-1] = P[i,L] Upper Limit P[L+1,j] = P[o,j] P[i,L+1] = P[i,0] def diffuse (P,L): '''create vacant list of list''' PP = [[ 0 for i in range (L)] for j in range (L)] '''diffuse 1-step over supplied configuration''' for i in range (L): for j in range (L): '''set boundary condition at bottom and left''' ni = 0 ; nj = 0 if i == 0 :ni = L if j == 0 :nj = L '''add modulo to control boundary at top and right''' PP[i][j] = P[(i + 1 ) % L][j] + P[(i - 1 ) + ni][j]\\ + P[i][(j + 1 ) % L] + P[i][(j - 1 ) + nj] '''normalize''' PP = PP / np . sum(PP) return PP L = 10 P = start(L) plt . figure(figsize = [ 20 , 15 ]) plt . subplot( 3 , 3 , 1 ) P = diffuse(P,L) sns . heatmap(P,annot = True ,cmap = 'YlGn' ) plt . subplot( 3 , 3 , 2 ) P = diffuse(P,L) sns . heatmap(P,annot = True ,cmap = 'YlGn' ) plt . subplot( 3 , 3 , 3 ) P = diffuse(P,L) sns . heatmap(P,annot = True ,cmap = 'YlGn' ) plt . subplot( 3 , 3 , 4 ) P = diffuse(P,L) sns . heatmap(P,annot = False ,cmap = 'YlGn' ) plt . subplot( 3 , 3 , 5 ) P = diffuse(P,L) sns . heatmap(P,annot = False ,cmap = 'YlGn' ) plt . subplot( 3 , 3 , 6 ) P = diffuse(P,L) sns . heatmap(P,annot = False ,cmap = 'YlGn' ) plt . subplot( 3 , 3 , 7 ) P = diffuse(P,L) sns . heatmap(P,annot = False ,cmap = 'YlGn' ) plt . subplot( 3 , 3 , 8 ) P = diffuse(P,L) sns . heatmap(P,annot = False ,cmap = 'YlGn' ) plt . subplot( 3 , 3 , 9 ) P = diffuse(P,L) sns . heatmap(P,annot = False ,cmap = 'YlGn' ) plt . savefig( 'plot/diffuse-step.pdf' ) plt . show() Run the diffusion step with desire no of running steps def run_diffuse (P,nrun,L): run = 0 '''diffuse N times''' while run < nrun: P = diffuse(P,L) run = run + 1 return P We can make a plot of arbitraty diffusion step by selecting \"irun\" in function runner. '''set parameters''' L = 100 ; nrun = 1000 ; P = start(L) '''run diffusion''' P = run_diffuse(P,nrun,L) plt . figure(figsize = [ 12 , 10 ]) sns . heatmap(P,annot = False ,cmap = 'YlGn' ) plt . savefig( 'plot/diffuse.pdf' ) plt . show() Much Finner L = 200 nrun = 1000 P = start(L) '''run diffusion''' P = run_diffuse(P,nrun,L) plt . figure(figsize = [ 12 , 10 ]) sns . heatmap(P,annot = False ,cmap = 'YlGn' ) plt . savefig( 'plot/diffuse-finner.pdf' ) plt . show()","title":"Project Diffusion"},{"location":"Projects/Diffusion/Diffusion/#diffusion-patterns","text":"https://scipython.com/book/chapter-7-matplotlib/examples/the-two-dimensional-diffusion-equation/ import numpy as np import json import random as random import math as math import matplotlib.pyplot as plt import seaborn as sns sns . set()","title":"Diffusion Patterns"},{"location":"Projects/Diffusion/Diffusion/#consider-a-2d-lattice-of-length-l","text":"L = 10","title":"Consider a 2D lattice of length L"},{"location":"Projects/Diffusion/Diffusion/#create-initial-configuration","text":"We can use a vacant list to create initial configuration where initially particle is at middle of the lattice. def start (L): '''create a vacant list of list ''' P = [[ 0 for i in range (L)] for j in range (L)] '''put particle at center''' P[ int (L / 2 )][ int (L / 2 )] = 1 return P P = start(L) P [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]","title":"Create initial configuration:"},{"location":"Projects/Diffusion/Diffusion/#make-a-plot-of-the-lattice","text":"plt . figure(figsize = [ 8 , 6 ]) sns . heatmap(P,annot = True ,cmap = 'YlGn' ) &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a1dafde48&gt;","title":"Make a plot of the lattice."},{"location":"Projects/Diffusion/Diffusion/#create-a-function-to-diffuse-a-particle","text":"$ P[i,j] = P[i+1,j] + P[i-1,j] + P[i,j+1] + P[i,j-1]$ def diffuse_primitive (P,L): '''create vacant list of list''' PP = [[ 0 for i in range (L)] for j in range (L)] for i in range (L): for j in range (L): '''diffuse one step''' PP[i][j] = P[i + 1 ][j] + P[i - 1 ][j] + P[i][j + 1 ] + P[i][j - 1 ] '''normalize''' PP = PP / np . sum(PP) return PP L = 10 P = start(L) #P = diffuse_primitive(P,L)","title":"Create a function to diffuse a particle:"},{"location":"Projects/Diffusion/Diffusion/#set-boundary-conditons","text":"Lower limit P[0-1,j] = P[L,j] P[I,0-1] = P[i,L] Upper Limit P[L+1,j] = P[o,j] P[i,L+1] = P[i,0] def diffuse (P,L): '''create vacant list of list''' PP = [[ 0 for i in range (L)] for j in range (L)] '''diffuse 1-step over supplied configuration''' for i in range (L): for j in range (L): '''set boundary condition at bottom and left''' ni = 0 ; nj = 0 if i == 0 :ni = L if j == 0 :nj = L '''add modulo to control boundary at top and right''' PP[i][j] = P[(i + 1 ) % L][j] + P[(i - 1 ) + ni][j]\\ + P[i][(j + 1 ) % L] + P[i][(j - 1 ) + nj] '''normalize''' PP = PP / np . sum(PP) return PP L = 10 P = start(L) plt . figure(figsize = [ 20 , 15 ]) plt . subplot( 3 , 3 , 1 ) P = diffuse(P,L) sns . heatmap(P,annot = True ,cmap = 'YlGn' ) plt . subplot( 3 , 3 , 2 ) P = diffuse(P,L) sns . heatmap(P,annot = True ,cmap = 'YlGn' ) plt . subplot( 3 , 3 , 3 ) P = diffuse(P,L) sns . heatmap(P,annot = True ,cmap = 'YlGn' ) plt . subplot( 3 , 3 , 4 ) P = diffuse(P,L) sns . heatmap(P,annot = False ,cmap = 'YlGn' ) plt . subplot( 3 , 3 , 5 ) P = diffuse(P,L) sns . heatmap(P,annot = False ,cmap = 'YlGn' ) plt . subplot( 3 , 3 , 6 ) P = diffuse(P,L) sns . heatmap(P,annot = False ,cmap = 'YlGn' ) plt . subplot( 3 , 3 , 7 ) P = diffuse(P,L) sns . heatmap(P,annot = False ,cmap = 'YlGn' ) plt . subplot( 3 , 3 , 8 ) P = diffuse(P,L) sns . heatmap(P,annot = False ,cmap = 'YlGn' ) plt . subplot( 3 , 3 , 9 ) P = diffuse(P,L) sns . heatmap(P,annot = False ,cmap = 'YlGn' ) plt . savefig( 'plot/diffuse-step.pdf' ) plt . show()","title":"Set boundary conditons"},{"location":"Projects/Diffusion/Diffusion/#run-the-diffusion-step-with-desire-no-of-running-steps","text":"def run_diffuse (P,nrun,L): run = 0 '''diffuse N times''' while run < nrun: P = diffuse(P,L) run = run + 1 return P We can make a plot of arbitraty diffusion step by selecting \"irun\" in function runner. '''set parameters''' L = 100 ; nrun = 1000 ; P = start(L) '''run diffusion''' P = run_diffuse(P,nrun,L) plt . figure(figsize = [ 12 , 10 ]) sns . heatmap(P,annot = False ,cmap = 'YlGn' ) plt . savefig( 'plot/diffuse.pdf' ) plt . show()","title":"Run the diffusion step with desire no of running steps"},{"location":"Projects/Diffusion/Diffusion/#much-finner","text":"L = 200 nrun = 1000 P = start(L) '''run diffusion''' P = run_diffuse(P,nrun,L) plt . figure(figsize = [ 12 , 10 ]) sns . heatmap(P,annot = False ,cmap = 'YlGn' ) plt . savefig( 'plot/diffuse-finner.pdf' ) plt . show()","title":"Much Finner"},{"location":"Projects/Fern/fern/","text":"Project: Fern import numpy as np import json import matplotlib.pyplot as plt import seaborn as sns import random as random % matplotlib inline sns . set() https://en.wikipedia.org/wiki/Barnsley_fern Method - 1: Blind implementation import random as random x = 0 y = 0 X = [x] Y = [y] n = 1 isprint = False while n < 1000000 : r = random . uniform( 0 , 100 ) if r < 1.0 : x = 0 y = 0.16 * Y[n - 1 ] X . append(x) ; Y . append(y) elif r > 1.0 and r < 86.0 : x = 0.85 * X[n - 1 ] + 0.04 * Y[n - 1 ] y = - 0.04 * X[n - 1 ] + 0.85 * Y[n - 1 ] + 1.6 X . append(x);Y . append(y) elif r > 86.0 and r < 93.0 : x = 0.2 * X[n - 1 ] - 0.26 * Y[n - 1 ] y = 0.23 * X[n - 1 ] + 0.22 * Y[n - 1 ] + 1.6 X . append(x);Y . append(y) elif r > 93.0 and r < 100.0 : x = - 0.15 * X[n - 1 ] + 0.28 * Y[n - 1 ] y = 0.26 * X[n - 1 ] + 0.24 * Y[n - 1 ] + 0.44 X . append(x);Y . append(y) if isprint: print ( \"step: \" ,n, \"random number is: \" , r, \"coordinate is : \" , x,y) n = n + 1 #for i in range(len(X)): # print(X[i],Y[i]) with open ( 'data/fern.json' , 'w' ) as f1: json . dump([X,Y],f1) plt . figure(figsize = [ 10 , 12 ]) plt . scatter(X,Y,color = 'g' ,marker = '.' ) plt . savefig( 'plot/charge-lattice.png' ) plt . savefig( 'plot/charge-lattice.pdf' ) plt . show() Method-2 : Manual Matrix Multiplication These correspond to the following transformations: $ {\\displaystyle f_{1}(x,y)={\\begin{bmatrix}\\ 0.00&\\ 0.00\\ \\0.00&\\ 0.16\\end{bmatrix}}{\\begin{bmatrix}\\ x\\y\\end{bmatrix}}}$ ${\\displaystyle f_{2}(x,y)={\\begin{bmatrix}\\ 0.85&\\ 0.04\\ \\-0.04&\\ 0.85\\end{bmatrix}}{\\begin{bmatrix}\\ x\\y\\end{bmatrix}}+{\\begin{bmatrix}\\ 0.00\\1.60\\end{bmatrix}}}$ $ {\\displaystyle f_{3}(x,y)={\\begin{bmatrix}\\ 0.20&\\ -0.26\\ \\0.23&\\ 0.22\\end{bmatrix}}{\\begin{bmatrix}\\ x\\y\\end{bmatrix}}+{\\begin{bmatrix}\\ 0.00\\1.60\\end{bmatrix}}}$ ${\\displaystyle f_{4}(x,y)={\\begin{bmatrix}\\ -0.15&\\ 0.28\\ \\0.26&\\ 0.24\\end{bmatrix}}{\\begin{bmatrix}\\ x\\y\\end{bmatrix}}+{\\begin{bmatrix}\\ 0.00\\0.44\\end{bmatrix}}}$ ITR = 100000 x = np . array([[ 0.0 , 0.0 ] for k in range (ITR)]) A = np . array([[ 0.0 , 0.0 ],[ 0.0 , 0.16 ]]) B = np . array([[ 0.85 , 0.04 ],[ - 0.04 , 0.85 ]]) C = np . array([[ 0.20 , - 0.26 ],[ 0.23 , 0.22 ]]) D = np . array([[ - 0.15 , 0.28 ],[ 0.26 , 0.24 ]]) AD = np . array([[ 0.0 , 0.0 ], [ 0.0 , 1.6 ], [ 0.0 , 1.6 ], [ 0.0 , 0.44 ]]) X = [] Y = [] x[ 0 , 0 ] = 0.0 x[ 0 , 1 ] = 0.0 t = 0 while t < ITR: ct = random . uniform( 0 , 100 ) '''First condition''' if ct < 1.0 : for p in range ( 2 ): x[t,p] = 0.0 for q in range ( 2 ): x[t,p] = x[t,p] + A[p,q] * x[t - 1 ,q] '''second condition''' elif ct > 1.0 and ct < 86.0 : for p in range ( 2 ): x[t,p] = 0.0 for q in range ( 2 ): x[t,p] = x[t,p] + B[p,q] * x[t - 1 ,q] for p in range ( 2 ): x[t,p] = x[t,p] + AD[ 1 ,p] '''third condition''' elif ct > 86.0 and ct < 93.0 : for p in range ( 2 ): x[t,p] = 0.0 for q in range ( 2 ): x[t,p] = x[t,p] + C[p,q] * x[t - 1 ,q] for p in range ( 2 ): x[t,p] = x[t,p] + AD[ 2 ,p] '''fourth condition ''' elif ct > 93.0 and ct < 100.0 : for p in range ( 2 ): x[ 1 ,p] = 0.0 for q in range ( 2 ): x[t,p] = x[t,p] + D[p,q] * x[t - 1 ,q] for p in range ( 2 ): x[t,p] = x[t,p] + AD[ 3 ,p] X . append(x[t, 0 ]) Y . append(x[t, 1 ]) t = t + 1 plt . figure(figsize = [ 10 , 12 ]) plt . scatter(X,Y,color = 'g' ,marker = '.' ) plt . savefig( 'plot/fern.pdf' ) plt . show() Method 3-Numpy '''Matrices''' A = np . array([[ 0.0 , 0.0 ],[ 0.0 , 0.16 ]]) B = np . array([[ 0.85 , 0.04 ],[ - 0.04 , 0.85 ]]) C = np . array([[ 0.20 , - 0.26 ],[ 0.23 , 0.22 ]]) D = np . array([[ - 0.15 , 0.28 ],[ 0.26 , 0.24 ]]) AD = np . array([[ 0.0 , 0.0 ], [ 0.0 , 1.6 ], [ 0.0 , 1.6 ], [ 0.0 , 0.44 ]]) u = np . array([ 0 , 0 ]) U = [u] n = 1 while n < 10000 : '''generate a random number''' r = random . uniform( 0 , 100 ) '''1rst condition''' if r < 1.0 : u = np . dot(A,u) U . append(u) '''second condition''' elif r > 1.0 and r < 86.0 : u = np . dot(B,u) + AD[ 1 ] U . append(u) '''third condition''' elif r > 86.0 and r < 93.0 : u = np . dot(C,u) + AD[ 2 ] U . append(u) '''fourth condition''' elif r > 93.0 and r < 100.0 : u = np . dot(D,u) + AD[ 3 ] U . append(u) '''update n''' n = n + 1 plt . figure(figsize = [ 10 , 12 ]) for item in U: plt . scatter(item[ 0 ],item[ 1 ],color = 'g' ,marker = '.' ) plt . show()","title":"Project Fern"},{"location":"Projects/Fern/fern/#project-fern","text":"import numpy as np import json import matplotlib.pyplot as plt import seaborn as sns import random as random % matplotlib inline sns . set() https://en.wikipedia.org/wiki/Barnsley_fern","title":"Project: Fern"},{"location":"Projects/Fern/fern/#method-1-blind-implementation","text":"import random as random x = 0 y = 0 X = [x] Y = [y] n = 1 isprint = False while n < 1000000 : r = random . uniform( 0 , 100 ) if r < 1.0 : x = 0 y = 0.16 * Y[n - 1 ] X . append(x) ; Y . append(y) elif r > 1.0 and r < 86.0 : x = 0.85 * X[n - 1 ] + 0.04 * Y[n - 1 ] y = - 0.04 * X[n - 1 ] + 0.85 * Y[n - 1 ] + 1.6 X . append(x);Y . append(y) elif r > 86.0 and r < 93.0 : x = 0.2 * X[n - 1 ] - 0.26 * Y[n - 1 ] y = 0.23 * X[n - 1 ] + 0.22 * Y[n - 1 ] + 1.6 X . append(x);Y . append(y) elif r > 93.0 and r < 100.0 : x = - 0.15 * X[n - 1 ] + 0.28 * Y[n - 1 ] y = 0.26 * X[n - 1 ] + 0.24 * Y[n - 1 ] + 0.44 X . append(x);Y . append(y) if isprint: print ( \"step: \" ,n, \"random number is: \" , r, \"coordinate is : \" , x,y) n = n + 1 #for i in range(len(X)): # print(X[i],Y[i]) with open ( 'data/fern.json' , 'w' ) as f1: json . dump([X,Y],f1) plt . figure(figsize = [ 10 , 12 ]) plt . scatter(X,Y,color = 'g' ,marker = '.' ) plt . savefig( 'plot/charge-lattice.png' ) plt . savefig( 'plot/charge-lattice.pdf' ) plt . show()","title":"Method - 1: Blind implementation"},{"location":"Projects/Fern/fern/#method-2-manual-matrix-multiplication","text":"These correspond to the following transformations: $ {\\displaystyle f_{1}(x,y)={\\begin{bmatrix}\\ 0.00&\\ 0.00\\ \\0.00&\\ 0.16\\end{bmatrix}}{\\begin{bmatrix}\\ x\\y\\end{bmatrix}}}$ ${\\displaystyle f_{2}(x,y)={\\begin{bmatrix}\\ 0.85&\\ 0.04\\ \\-0.04&\\ 0.85\\end{bmatrix}}{\\begin{bmatrix}\\ x\\y\\end{bmatrix}}+{\\begin{bmatrix}\\ 0.00\\1.60\\end{bmatrix}}}$ $ {\\displaystyle f_{3}(x,y)={\\begin{bmatrix}\\ 0.20&\\ -0.26\\ \\0.23&\\ 0.22\\end{bmatrix}}{\\begin{bmatrix}\\ x\\y\\end{bmatrix}}+{\\begin{bmatrix}\\ 0.00\\1.60\\end{bmatrix}}}$ ${\\displaystyle f_{4}(x,y)={\\begin{bmatrix}\\ -0.15&\\ 0.28\\ \\0.26&\\ 0.24\\end{bmatrix}}{\\begin{bmatrix}\\ x\\y\\end{bmatrix}}+{\\begin{bmatrix}\\ 0.00\\0.44\\end{bmatrix}}}$ ITR = 100000 x = np . array([[ 0.0 , 0.0 ] for k in range (ITR)]) A = np . array([[ 0.0 , 0.0 ],[ 0.0 , 0.16 ]]) B = np . array([[ 0.85 , 0.04 ],[ - 0.04 , 0.85 ]]) C = np . array([[ 0.20 , - 0.26 ],[ 0.23 , 0.22 ]]) D = np . array([[ - 0.15 , 0.28 ],[ 0.26 , 0.24 ]]) AD = np . array([[ 0.0 , 0.0 ], [ 0.0 , 1.6 ], [ 0.0 , 1.6 ], [ 0.0 , 0.44 ]]) X = [] Y = [] x[ 0 , 0 ] = 0.0 x[ 0 , 1 ] = 0.0 t = 0 while t < ITR: ct = random . uniform( 0 , 100 ) '''First condition''' if ct < 1.0 : for p in range ( 2 ): x[t,p] = 0.0 for q in range ( 2 ): x[t,p] = x[t,p] + A[p,q] * x[t - 1 ,q] '''second condition''' elif ct > 1.0 and ct < 86.0 : for p in range ( 2 ): x[t,p] = 0.0 for q in range ( 2 ): x[t,p] = x[t,p] + B[p,q] * x[t - 1 ,q] for p in range ( 2 ): x[t,p] = x[t,p] + AD[ 1 ,p] '''third condition''' elif ct > 86.0 and ct < 93.0 : for p in range ( 2 ): x[t,p] = 0.0 for q in range ( 2 ): x[t,p] = x[t,p] + C[p,q] * x[t - 1 ,q] for p in range ( 2 ): x[t,p] = x[t,p] + AD[ 2 ,p] '''fourth condition ''' elif ct > 93.0 and ct < 100.0 : for p in range ( 2 ): x[ 1 ,p] = 0.0 for q in range ( 2 ): x[t,p] = x[t,p] + D[p,q] * x[t - 1 ,q] for p in range ( 2 ): x[t,p] = x[t,p] + AD[ 3 ,p] X . append(x[t, 0 ]) Y . append(x[t, 1 ]) t = t + 1 plt . figure(figsize = [ 10 , 12 ]) plt . scatter(X,Y,color = 'g' ,marker = '.' ) plt . savefig( 'plot/fern.pdf' ) plt . show()","title":"Method-2 : Manual Matrix Multiplication"},{"location":"Projects/Fern/fern/#method-3-numpy","text":"'''Matrices''' A = np . array([[ 0.0 , 0.0 ],[ 0.0 , 0.16 ]]) B = np . array([[ 0.85 , 0.04 ],[ - 0.04 , 0.85 ]]) C = np . array([[ 0.20 , - 0.26 ],[ 0.23 , 0.22 ]]) D = np . array([[ - 0.15 , 0.28 ],[ 0.26 , 0.24 ]]) AD = np . array([[ 0.0 , 0.0 ], [ 0.0 , 1.6 ], [ 0.0 , 1.6 ], [ 0.0 , 0.44 ]]) u = np . array([ 0 , 0 ]) U = [u] n = 1 while n < 10000 : '''generate a random number''' r = random . uniform( 0 , 100 ) '''1rst condition''' if r < 1.0 : u = np . dot(A,u) U . append(u) '''second condition''' elif r > 1.0 and r < 86.0 : u = np . dot(B,u) + AD[ 1 ] U . append(u) '''third condition''' elif r > 86.0 and r < 93.0 : u = np . dot(C,u) + AD[ 2 ] U . append(u) '''fourth condition''' elif r > 93.0 and r < 100.0 : u = np . dot(D,u) + AD[ 3 ] U . append(u) '''update n''' n = n + 1 plt . figure(figsize = [ 10 , 12 ]) for item in U: plt . scatter(item[ 0 ],item[ 1 ],color = 'g' ,marker = '.' ) plt . show()","title":"Method 3-Numpy"},{"location":"Projects/Ncharges/Ncharges/","text":"Project: N-charge system $ E = q * \\frac{\\hat{r}}{r}$ $ V = q * \\frac{1}{r}$ $ V = q * \\frac{1}{\\sqrt{((x-x^{'})^{2} + (y - y^{'})^{2})}} $ import matplotlib.pyplot as plt from matplotlib import cm % matplotlib inline import seaborn as sns sns . set() Class Charge class Charge : '''Data incapsulation''' def __init__ ( self , q, pos): self . q = q self . pos = pos def line ( self , x,y): '''create a vector from charge to observation point''' self . vector = [x - self . pos[ 0 ],y - self . pos[ 1 ]] '''norm of the vector''' self . norm = np . sqrt(( self . vector[ 0 ]) ** 2 + ( self . vector[ 1 ]) ** 2 ) def V_point_charge ( self , x, y): '''recall length''' self . line(x,y) '''Make sure to exclude source itself''' if self . norm > 0 : self . V = self . q / self . norm '''if length is zero, set V equal to 0''' else : self . V = 0 return self . V Example : Lets use charge q = 100 at posiotion x =5 and y =5 to find electric potential at different points in 2D C = Charge( 100 , [ 5 , 5 ]) for x in range ( 10 ): for y in range ( 10 ): print (x,y, \"|\" , C . V_point_charge(x, y)) 0 0 | 5.652334189442215 0 1 | 5.892556509887896 0 2 | 6.142951168339512 0 3 | 6.401843996644799 0 4 | 6.666666666666667 0 5 | 6.933752452815364 0 6 | 7.198157507486946 0 7 | 7.453559924999299 0 8 | 7.6923076923076925 0 9 | 7.905694150420948 1 0 | 5.872202195147034 1 1 | 6.142951168339512 1 2 | 6.42824346533225 1 3 | 6.726727939963125 1 4 | 7.035975447302919 1 5 | 7.352146220938077 1 6 | 7.669649888473704 1 7 | 7.9808688446762215 1 8 | 8.27605888602368 1 9 | 8.54357657716761 2 0 | 6.097107608496923 2 1 | 6.401843996644799 2 2 | 6.726727939963125 2 3 | 7.071067811865475 2 4 | 7.432941462471663 2 5 | 7.808688094430304 2 6 | 8.192319205190405 2 7 | 8.574929257125442 2 8 | 8.94427190999916 2 9 | 9.284766908852594 3 0 | 6.324555320336759 3 1 | 6.666666666666667 3 2 | 7.035975447302919 3 3 | 7.432941462471663 3 4 | 7.856742013183862 3 5 | 8.304547985373997 3 6 | 8.770580193070293 3 7 | 9.245003270420485 3 8 | 9.712858623572641 3 9 | 10.15346165133619 4 0 | 6.551217820804184 4 1 | 6.933752452815364 4 2 | 7.352146220938077 4 3 | 7.808688094430304 4 4 | 8.304547985373997 4 5 | 8.838834764831843 4 6 | 9.407208683835972 4 7 | 10.0 4 8 | 10.599978800063601 4 9 | 11.180339887498947 5 0 | 6.772854614785964 5 1 | 7.198157507486946 5 2 | 7.669649888473704 5 3 | 8.192319205190405 5 4 | 8.770580193070293 5 5 | 9.407208683835972 5 6 | 10.101525445522107 5 7 | 10.846522890932809 5 8 | 11.624763874381928 5 9 | 12.403473458920846 6 0 | 6.984302957695782 6 1 | 7.453559924999299 6 2 | 7.9808688446762215 6 3 | 8.574929257125442 6 4 | 9.245003270420485 6 5 | 10.0 6 6 | 10.846522890932809 6 7 | 11.785113019775793 6 8 | 12.803687993289598 6 9 | 13.867504905630728 7 0 | 7.179581586177381 7 1 | 7.6923076923076925 7 2 | 8.27605888602368 7 3 | 8.94427190999916 7 4 | 9.712858623572641 7 5 | 10.599978800063601 7 6 | 11.624763874381928 7 7 | 12.803687993289598 7 8 | 14.14213562373095 7 9 | 15.617376188860607 8 0 | 7.352146220938077 8 1 | 7.905694150420948 8 2 | 8.54357657716761 8 3 | 9.284766908852594 8 4 | 10.15346165133619 8 5 | 11.180339887498947 8 6 | 12.403473458920846 8 7 | 13.867504905630728 8 8 | 15.617376188860607 8 9 | 17.677669529663685 9 0 | 7.495316889958614 9 1 | 8.084520834544433 9 2 | 8.770580193070293 9 3 | 9.578262852211514 9 4 | 10.540925533894598 9 5 | 11.704114719613058 9 6 | 13.130643285972255 9 7 | 14.907119849998598 9 8 | 17.149858514250884 9 9 | 20.0 Total Electric potential def V_total (x, y, charges): V = 0 for C in charges: Vp = C . V_point_charge(x, y) V = V + Vp return V Example: Lets use collection of charges to find a electric potential at point x = 4, y =4 sample_charges = [Charge(q = 20 , pos = [ 23 , 34 ]), Charge(q = 25 , pos = [ 13 , 48 ]), Charge(q = 40 , pos = [ 3 , 14 ]), Charge(q = 80 , pos = [ 88 , 60 ])] V_total(x = 4 , y = 4 , charges = sample_charges) 5.892446541150622 Implementation - 1 : Lattice of charges '''first charge to be at x=1,y=1''' q = 100 '''Dictionary to collect charges, x and y xoordinates''' Qd = [] '''List to collect Charge objects''' charges = [] '''use for loops to construct collection of charges objects''' for i in range ( 5 ): for j in range ( 5 ): '''Collecting charges and their coordinates''' Qd . append({ \"q\" : q, \"x\" : i * 20 , \"y\" :j * 20 }) '''charge objects are being collected''' charges . append(Charge(q , [ 20 * i, 20 * j])) '''change the sign of charge alternatly''' q = - q import json with open ( 'data/charg-lattice.json' , 'w' ) as f1: json . dump(Qd,f1) Plot of lattice of charges '''Plot the lattice of charges''' plt . figure(figsize = [ 10 , 8 ]) for item in Qd: '''Sctaeer as red dot if charge is positive''' if item[ 'q' ] > 0 : plt . scatter(item[ 'x' ], item[ 'y' ], c = 'r' ) '''Scatter as blue dot if charge is negative''' else : plt . scatter(item[ 'x' ], item[ 'y' ], c = 'b' ) plt . savefig( 'plot/charge-lattice.pdf' ) plt . show() Find Electric Potential '''Create X and Y coordinate''' X = np . arange( - 10 , 110 , 1 ) Y = np . arange( - 10 , 110 , 1 ) '''Initiate vacant V-list of list''' V = [[ 0.0 for i in range ( len (X))] for j in range ( len (Y))] '''Calculate Electric potential at each x,y coordinate''' for i,x in enumerate (X): for j,y in enumerate (Y): v = V_total(x, y, charges) V[i][j] = v VV = np . array(V) Save electric potential data import json with open ( 'data/potential-lattice.json' , 'w' ) as f2: json . dump(V,f2) Plot Electric potential plt . figure(figsize = [ 18 , 14 ]) sns . heatmap(VV,annot = False ,cmap = 'YlGnBu' ) plt . savefig( 'plot/potential-lattice.png' ) plt . show() Implementation - 2 : Random Charges import random as random '''first charge to be at x=1,y=1''' q = 20 '''Dictionary to collect charges, x and y xoordinates''' Qd = [] '''List to collect Charge objects''' charges = [] '''use for loops to construct collection of charges objects''' for i in range ( 10 ): for j in range ( 10 ): rx = random . randint( 1 , 100 ) ry = random . randint( 1 , 100 ) '''collecting charges and their coordinates''' Qd . append({ \"q\" : q, \"x\" : rx, \"y\" :ry}) '''charge objects are being collected''' charges . append(Charge(q , [rx,ry])) '''change the sign of charge alternatly''' q = - q Save charge configuration import json with open ( 'data/charg-random.json' , 'w' ) as f3: json . dump(Qd,f3) Plot charge system '''Plot the lattice of charges''' plt . figure(figsize = [ 10 , 8 ]) for item in Qd: '''Sctaeer as red dot if charge is positive''' if item[ 'q' ] > 0 : plt . scatter(item[ 'x' ], item[ 'y' ], c = 'r' ) '''Scatter as blue dot if charge is negative''' else : plt . scatter(item[ 'x' ], item[ 'y' ], c = 'b' ) plt . savefig( 'plot/charge-random.pdf' ) plt . show() Calculate electric potential '''Create X and Y coordinate''' X = np . arange( - 10 , 110 , 1 ) Y = np . arange( - 10 , 110 , 1 ) '''Initiate vacant V-list of list''' V = [[ 0.0 for i in range ( len (X))] for j in range ( len (Y))] '''Calculate Electric potential at each x,y coordinate''' for i,x in enumerate (X): for j,y in enumerate (Y): v = V_total(x, y, charges) V[i][j] = v Save electric potential dta import json with open ( 'data/potential-random.json' , 'w' ) as f4: json . dump(V,f4) Plot potential V = np . array(V) plt . figure(figsize = [ 18 , 14 ]) sns . heatmap(V,annot = False ,cmap = 'YlGnBu' ) plt . savefig( 'plot/potential-random.pdf' ) plt . show() Much finner '''Create X and Y coordinate''' X = np . arange( - 10 , 110 , 0.5 ) Y = np . arange( - 10 , 110 , 0.5 ) '''Initiate vacant V-list of list''' V = [[ 0.0 for i in range ( len (X))] for j in range ( len (Y))] '''Calculate Electric potential at each x,y coordinate''' for i,x in enumerate (X): for j,y in enumerate (Y): v = V_total(x, y, charges) V[i][j] = v V = np . array(V) plt . figure(figsize = [ 18 , 14 ]) sns . heatmap(V,annot = False ,cmap = 'YlGnBu' ) plt . savefig( 'plot/potential-random.pdf' ) plt . show()","title":"Project N-charges"},{"location":"Projects/Ncharges/Ncharges/#project-n-charge-system","text":"$ E = q * \\frac{\\hat{r}}{r}$ $ V = q * \\frac{1}{r}$ $ V = q * \\frac{1}{\\sqrt{((x-x^{'})^{2} + (y - y^{'})^{2})}} $ import matplotlib.pyplot as plt from matplotlib import cm % matplotlib inline import seaborn as sns sns . set()","title":"Project: N-charge system"},{"location":"Projects/Ncharges/Ncharges/#class-charge","text":"class Charge : '''Data incapsulation''' def __init__ ( self , q, pos): self . q = q self . pos = pos def line ( self , x,y): '''create a vector from charge to observation point''' self . vector = [x - self . pos[ 0 ],y - self . pos[ 1 ]] '''norm of the vector''' self . norm = np . sqrt(( self . vector[ 0 ]) ** 2 + ( self . vector[ 1 ]) ** 2 ) def V_point_charge ( self , x, y): '''recall length''' self . line(x,y) '''Make sure to exclude source itself''' if self . norm > 0 : self . V = self . q / self . norm '''if length is zero, set V equal to 0''' else : self . V = 0 return self . V","title":"Class Charge"},{"location":"Projects/Ncharges/Ncharges/#example","text":"Lets use charge q = 100 at posiotion x =5 and y =5 to find electric potential at different points in 2D C = Charge( 100 , [ 5 , 5 ]) for x in range ( 10 ): for y in range ( 10 ): print (x,y, \"|\" , C . V_point_charge(x, y)) 0 0 | 5.652334189442215 0 1 | 5.892556509887896 0 2 | 6.142951168339512 0 3 | 6.401843996644799 0 4 | 6.666666666666667 0 5 | 6.933752452815364 0 6 | 7.198157507486946 0 7 | 7.453559924999299 0 8 | 7.6923076923076925 0 9 | 7.905694150420948 1 0 | 5.872202195147034 1 1 | 6.142951168339512 1 2 | 6.42824346533225 1 3 | 6.726727939963125 1 4 | 7.035975447302919 1 5 | 7.352146220938077 1 6 | 7.669649888473704 1 7 | 7.9808688446762215 1 8 | 8.27605888602368 1 9 | 8.54357657716761 2 0 | 6.097107608496923 2 1 | 6.401843996644799 2 2 | 6.726727939963125 2 3 | 7.071067811865475 2 4 | 7.432941462471663 2 5 | 7.808688094430304 2 6 | 8.192319205190405 2 7 | 8.574929257125442 2 8 | 8.94427190999916 2 9 | 9.284766908852594 3 0 | 6.324555320336759 3 1 | 6.666666666666667 3 2 | 7.035975447302919 3 3 | 7.432941462471663 3 4 | 7.856742013183862 3 5 | 8.304547985373997 3 6 | 8.770580193070293 3 7 | 9.245003270420485 3 8 | 9.712858623572641 3 9 | 10.15346165133619 4 0 | 6.551217820804184 4 1 | 6.933752452815364 4 2 | 7.352146220938077 4 3 | 7.808688094430304 4 4 | 8.304547985373997 4 5 | 8.838834764831843 4 6 | 9.407208683835972 4 7 | 10.0 4 8 | 10.599978800063601 4 9 | 11.180339887498947 5 0 | 6.772854614785964 5 1 | 7.198157507486946 5 2 | 7.669649888473704 5 3 | 8.192319205190405 5 4 | 8.770580193070293 5 5 | 9.407208683835972 5 6 | 10.101525445522107 5 7 | 10.846522890932809 5 8 | 11.624763874381928 5 9 | 12.403473458920846 6 0 | 6.984302957695782 6 1 | 7.453559924999299 6 2 | 7.9808688446762215 6 3 | 8.574929257125442 6 4 | 9.245003270420485 6 5 | 10.0 6 6 | 10.846522890932809 6 7 | 11.785113019775793 6 8 | 12.803687993289598 6 9 | 13.867504905630728 7 0 | 7.179581586177381 7 1 | 7.6923076923076925 7 2 | 8.27605888602368 7 3 | 8.94427190999916 7 4 | 9.712858623572641 7 5 | 10.599978800063601 7 6 | 11.624763874381928 7 7 | 12.803687993289598 7 8 | 14.14213562373095 7 9 | 15.617376188860607 8 0 | 7.352146220938077 8 1 | 7.905694150420948 8 2 | 8.54357657716761 8 3 | 9.284766908852594 8 4 | 10.15346165133619 8 5 | 11.180339887498947 8 6 | 12.403473458920846 8 7 | 13.867504905630728 8 8 | 15.617376188860607 8 9 | 17.677669529663685 9 0 | 7.495316889958614 9 1 | 8.084520834544433 9 2 | 8.770580193070293 9 3 | 9.578262852211514 9 4 | 10.540925533894598 9 5 | 11.704114719613058 9 6 | 13.130643285972255 9 7 | 14.907119849998598 9 8 | 17.149858514250884 9 9 | 20.0","title":"Example :"},{"location":"Projects/Ncharges/Ncharges/#total-electric-potential","text":"def V_total (x, y, charges): V = 0 for C in charges: Vp = C . V_point_charge(x, y) V = V + Vp return V","title":"Total Electric potential"},{"location":"Projects/Ncharges/Ncharges/#example_1","text":"Lets use collection of charges to find a electric potential at point x = 4, y =4 sample_charges = [Charge(q = 20 , pos = [ 23 , 34 ]), Charge(q = 25 , pos = [ 13 , 48 ]), Charge(q = 40 , pos = [ 3 , 14 ]), Charge(q = 80 , pos = [ 88 , 60 ])] V_total(x = 4 , y = 4 , charges = sample_charges) 5.892446541150622","title":"Example:"},{"location":"Projects/Ncharges/Ncharges/#implementation-1-lattice-of-charges","text":"'''first charge to be at x=1,y=1''' q = 100 '''Dictionary to collect charges, x and y xoordinates''' Qd = [] '''List to collect Charge objects''' charges = [] '''use for loops to construct collection of charges objects''' for i in range ( 5 ): for j in range ( 5 ): '''Collecting charges and their coordinates''' Qd . append({ \"q\" : q, \"x\" : i * 20 , \"y\" :j * 20 }) '''charge objects are being collected''' charges . append(Charge(q , [ 20 * i, 20 * j])) '''change the sign of charge alternatly''' q = - q import json with open ( 'data/charg-lattice.json' , 'w' ) as f1: json . dump(Qd,f1)","title":"Implementation - 1 : Lattice of charges"},{"location":"Projects/Ncharges/Ncharges/#plot-of-lattice-of-charges","text":"'''Plot the lattice of charges''' plt . figure(figsize = [ 10 , 8 ]) for item in Qd: '''Sctaeer as red dot if charge is positive''' if item[ 'q' ] > 0 : plt . scatter(item[ 'x' ], item[ 'y' ], c = 'r' ) '''Scatter as blue dot if charge is negative''' else : plt . scatter(item[ 'x' ], item[ 'y' ], c = 'b' ) plt . savefig( 'plot/charge-lattice.pdf' ) plt . show()","title":"Plot of lattice of charges"},{"location":"Projects/Ncharges/Ncharges/#find-electric-potential","text":"'''Create X and Y coordinate''' X = np . arange( - 10 , 110 , 1 ) Y = np . arange( - 10 , 110 , 1 ) '''Initiate vacant V-list of list''' V = [[ 0.0 for i in range ( len (X))] for j in range ( len (Y))] '''Calculate Electric potential at each x,y coordinate''' for i,x in enumerate (X): for j,y in enumerate (Y): v = V_total(x, y, charges) V[i][j] = v VV = np . array(V)","title":"Find Electric Potential"},{"location":"Projects/Ncharges/Ncharges/#save-electric-potential-data","text":"import json with open ( 'data/potential-lattice.json' , 'w' ) as f2: json . dump(V,f2)","title":"Save electric potential data"},{"location":"Projects/Ncharges/Ncharges/#plot-electric-potential","text":"plt . figure(figsize = [ 18 , 14 ]) sns . heatmap(VV,annot = False ,cmap = 'YlGnBu' ) plt . savefig( 'plot/potential-lattice.png' ) plt . show()","title":"Plot Electric potential"},{"location":"Projects/Ncharges/Ncharges/#implementation-2-random-charges","text":"import random as random '''first charge to be at x=1,y=1''' q = 20 '''Dictionary to collect charges, x and y xoordinates''' Qd = [] '''List to collect Charge objects''' charges = [] '''use for loops to construct collection of charges objects''' for i in range ( 10 ): for j in range ( 10 ): rx = random . randint( 1 , 100 ) ry = random . randint( 1 , 100 ) '''collecting charges and their coordinates''' Qd . append({ \"q\" : q, \"x\" : rx, \"y\" :ry}) '''charge objects are being collected''' charges . append(Charge(q , [rx,ry])) '''change the sign of charge alternatly''' q = - q","title":"Implementation - 2 : Random Charges"},{"location":"Projects/Ncharges/Ncharges/#save-charge-configuration","text":"import json with open ( 'data/charg-random.json' , 'w' ) as f3: json . dump(Qd,f3)","title":"Save charge configuration"},{"location":"Projects/Ncharges/Ncharges/#plot-charge-system","text":"'''Plot the lattice of charges''' plt . figure(figsize = [ 10 , 8 ]) for item in Qd: '''Sctaeer as red dot if charge is positive''' if item[ 'q' ] > 0 : plt . scatter(item[ 'x' ], item[ 'y' ], c = 'r' ) '''Scatter as blue dot if charge is negative''' else : plt . scatter(item[ 'x' ], item[ 'y' ], c = 'b' ) plt . savefig( 'plot/charge-random.pdf' ) plt . show()","title":"Plot charge system"},{"location":"Projects/Ncharges/Ncharges/#calculate-electric-potential","text":"'''Create X and Y coordinate''' X = np . arange( - 10 , 110 , 1 ) Y = np . arange( - 10 , 110 , 1 ) '''Initiate vacant V-list of list''' V = [[ 0.0 for i in range ( len (X))] for j in range ( len (Y))] '''Calculate Electric potential at each x,y coordinate''' for i,x in enumerate (X): for j,y in enumerate (Y): v = V_total(x, y, charges) V[i][j] = v","title":"Calculate electric potential"},{"location":"Projects/Ncharges/Ncharges/#save-electric-potential-dta","text":"import json with open ( 'data/potential-random.json' , 'w' ) as f4: json . dump(V,f4)","title":"Save electric potential dta"},{"location":"Projects/Ncharges/Ncharges/#plot-potential","text":"V = np . array(V) plt . figure(figsize = [ 18 , 14 ]) sns . heatmap(V,annot = False ,cmap = 'YlGnBu' ) plt . savefig( 'plot/potential-random.pdf' ) plt . show()","title":"Plot potential"},{"location":"Projects/Ncharges/Ncharges/#much-finner","text":"'''Create X and Y coordinate''' X = np . arange( - 10 , 110 , 0.5 ) Y = np . arange( - 10 , 110 , 0.5 ) '''Initiate vacant V-list of list''' V = [[ 0.0 for i in range ( len (X))] for j in range ( len (Y))] '''Calculate Electric potential at each x,y coordinate''' for i,x in enumerate (X): for j,y in enumerate (Y): v = V_total(x, y, charges) V[i][j] = v V = np . array(V) plt . figure(figsize = [ 18 , 14 ]) sns . heatmap(V,annot = False ,cmap = 'YlGnBu' ) plt . savefig( 'plot/potential-random.pdf' ) plt . show()","title":"Much finner"},{"location":"Projects/Rwalk/Rwalk/","text":"Random Walk import numpy as np import random as random import matplotlib.pyplot as plt import seaborn as sns sns . set() Random walk in one dimension Direct Implementation '''X stores 1-D coordinate''' X = [] '''T stores time coordinate''' T = [] '''starting point''' x = 0 '''length of each step''' d = 1 '''iteratefor N steps''' for t in range ( 100 ): '''Walk one step ahead''' x = x + d * random . choice([ - 1 , 1 ]) '''collect time''' T . append(t) '''collect position''' X . append(x) '''Make a plot''' plt . figure(figsize = [ 12 , 6 ]) plt . scatter(T,X,marker = '.' ) plt . plot(T,X) plt . xlabel( \"Time\" ) plt . ylabel( 'Displacement' ) plt . grid( True ) plt . show() Implement with function def walk1D (x,N,d): '''This function returns the space 'X' and time 'T' for random walk in 1D x: initial position N: total number of steps d: step length''' '''X stores 1-D coordinate''' X = [] '''T stores time coordinate''' T = [] '''iteratefor N steps''' for t in range (N): '''Walk one step ahead''' x = x + d * random . choice([ - 1 , 1 ]) '''collect time''' T . append(t) '''collect position''' X . append(x) return X,T '''Implement function to get data''' X,T = walk1D(x = 0 ,N = 100 ,d = 1 ) '''Make a plot''' plt . figure(figsize = [ 12 , 6 ]) plt . scatter(T,X,marker = '.' ) plt . plot(T,X) plt . xlabel( \"Time\" ) plt . ylabel( 'Displacement' ) plt . grid( True ) plt . show() Object Oriented Programming class Walker1D ( object ): ''' This is a class to create on dimentional walk: x0 : initial position d : step size N : number of steps in random walk ''' def __init__ ( self ,N,d,x0): self . N = N self . X = [] self . d = d self . x0 = x0 def walk1D ( self ): '''note initial position''' x = self . x0 k = 0 while k < self . N: '''Walk one step ahead''' x = x + self . d * random . choice([ - 1 , 1 ]) '''collect position''' self . X . append(x) k = k + 1 return self . X Let's generate 4 different random walks of step 1000 each X1 = Walker1D(N = 1000 ,d = 1 ,x0 = 0 ) . walk1D() X2 = Walker1D(N = 1000 ,d = 1 ,x0 = 0 ) . walk1D() X3 = Walker1D(N = 1000 ,d = 1 ,x0 = 0 ) . walk1D() X4 = Walker1D(N = 1000 ,d = 1 ,x0 = 0 ) . walk1D() T = [i for i in range ( len (X1))] import json with open ( 'data/rwalk4.json' , 'w' ) as f4: json . dump([X1,X2,X3,X4,T],f4) Let's visualize them plt . figure(figsize = [ 20 , 12 ]) plt . plot(T,X1) plt . plot(T,X2) plt . plot(T,X3) plt . plot(T,X4) plt . xlabel( \"Time\" ) plt . ylabel( 'Displacement' ) plt . grid( True ) plt . savefig( 'plot/rwalk4.pdf' ) plt . show() Random Walk in 2D Create a function to walk a single step def move2D (xi,yi): ''' This function choose a direction and walk one step in 2D ''' direction = random . choice([ 'x' , 'y' ]) if direction == 'x' : r = random . choice([ - 1 , 1 ]) if r == 1 :xf = xi + 1 else : xf = xi - 1 yf = yi if direction == 'y' : r = random . choice([ - 1 , 1 ]) if r == 1 :yf = yi + 1 else : yf = yi - 1 xf = xi return xf,yf Lets walk in 2D using above function def walk2D (N,pos): ''' This function walks N step in 2d implementing move2D ''' '''coordinate collectors''' X = [] ; Y = [] '''from where to start''' x0 = pos[ 0 ] ; y0 = pos[ 0 ] k = 0 while k < N: '''move a step''' x,y = move2D(x0,y0) '''collect X coordinate''' X . append(x) '''collect Y coordinate''' Y . append(y) '''set previous position for next step''' x0 = x y0 = y k = k + 1 return X,Y Lets create a 3 different Random walk in 2D N = 10000 U = walk2D(N,[ 0 , 0 ]) V = walk2D(N,[ 0 , 0 ]) W = walk2D(N,[ 0 , 0 ]) with open ( 'data/rwalk4.json' , 'w' ) as f: json . dump([U,V,W],f) plt . figure(figsize = [ 15 , 12 ]) plt . plot(U[ 0 ],U[ 1 ], \"-\" ) plt . plot(V[ 0 ],V[ 1 ], \"-\" ) plt . plot(W[ 0 ],W[ 1 ], \"-\" ) plt . axis([ - 100 , 100 , - 100 , 100 ]) plt . xlabel( \"x-axis\" ) plt . ylabel( \"y-axis\" ) plt . grid( True ) plt . title( \"Brownian motion in 2D\" ) plt . savefig( 'plot/rwalk2D.pdf' ) plt . show()","title":"Project Random Walk"},{"location":"Projects/Rwalk/Rwalk/#random-walk","text":"import numpy as np import random as random import matplotlib.pyplot as plt import seaborn as sns sns . set()","title":"Random Walk"},{"location":"Projects/Rwalk/Rwalk/#random-walk-in-one-dimension","text":"","title":"Random walk in one dimension"},{"location":"Projects/Rwalk/Rwalk/#direct-implementation","text":"'''X stores 1-D coordinate''' X = [] '''T stores time coordinate''' T = [] '''starting point''' x = 0 '''length of each step''' d = 1 '''iteratefor N steps''' for t in range ( 100 ): '''Walk one step ahead''' x = x + d * random . choice([ - 1 , 1 ]) '''collect time''' T . append(t) '''collect position''' X . append(x) '''Make a plot''' plt . figure(figsize = [ 12 , 6 ]) plt . scatter(T,X,marker = '.' ) plt . plot(T,X) plt . xlabel( \"Time\" ) plt . ylabel( 'Displacement' ) plt . grid( True ) plt . show()","title":"Direct Implementation"},{"location":"Projects/Rwalk/Rwalk/#implement-with-function","text":"def walk1D (x,N,d): '''This function returns the space 'X' and time 'T' for random walk in 1D x: initial position N: total number of steps d: step length''' '''X stores 1-D coordinate''' X = [] '''T stores time coordinate''' T = [] '''iteratefor N steps''' for t in range (N): '''Walk one step ahead''' x = x + d * random . choice([ - 1 , 1 ]) '''collect time''' T . append(t) '''collect position''' X . append(x) return X,T '''Implement function to get data''' X,T = walk1D(x = 0 ,N = 100 ,d = 1 ) '''Make a plot''' plt . figure(figsize = [ 12 , 6 ]) plt . scatter(T,X,marker = '.' ) plt . plot(T,X) plt . xlabel( \"Time\" ) plt . ylabel( 'Displacement' ) plt . grid( True ) plt . show()","title":"Implement with function"},{"location":"Projects/Rwalk/Rwalk/#object-oriented-programming","text":"class Walker1D ( object ): ''' This is a class to create on dimentional walk: x0 : initial position d : step size N : number of steps in random walk ''' def __init__ ( self ,N,d,x0): self . N = N self . X = [] self . d = d self . x0 = x0 def walk1D ( self ): '''note initial position''' x = self . x0 k = 0 while k < self . N: '''Walk one step ahead''' x = x + self . d * random . choice([ - 1 , 1 ]) '''collect position''' self . X . append(x) k = k + 1 return self . X","title":"Object Oriented Programming"},{"location":"Projects/Rwalk/Rwalk/#lets-generate-4-different-random-walks-of-step-1000-each","text":"X1 = Walker1D(N = 1000 ,d = 1 ,x0 = 0 ) . walk1D() X2 = Walker1D(N = 1000 ,d = 1 ,x0 = 0 ) . walk1D() X3 = Walker1D(N = 1000 ,d = 1 ,x0 = 0 ) . walk1D() X4 = Walker1D(N = 1000 ,d = 1 ,x0 = 0 ) . walk1D() T = [i for i in range ( len (X1))] import json with open ( 'data/rwalk4.json' , 'w' ) as f4: json . dump([X1,X2,X3,X4,T],f4)","title":"Let's generate 4 different random walks of step 1000 each"},{"location":"Projects/Rwalk/Rwalk/#lets-visualize-them","text":"plt . figure(figsize = [ 20 , 12 ]) plt . plot(T,X1) plt . plot(T,X2) plt . plot(T,X3) plt . plot(T,X4) plt . xlabel( \"Time\" ) plt . ylabel( 'Displacement' ) plt . grid( True ) plt . savefig( 'plot/rwalk4.pdf' ) plt . show()","title":"Let's visualize them"},{"location":"Projects/Rwalk/Rwalk/#random-walk-in-2d","text":"","title":"Random Walk in 2D"},{"location":"Projects/Rwalk/Rwalk/#create-a-function-to-walk-a-single-step","text":"def move2D (xi,yi): ''' This function choose a direction and walk one step in 2D ''' direction = random . choice([ 'x' , 'y' ]) if direction == 'x' : r = random . choice([ - 1 , 1 ]) if r == 1 :xf = xi + 1 else : xf = xi - 1 yf = yi if direction == 'y' : r = random . choice([ - 1 , 1 ]) if r == 1 :yf = yi + 1 else : yf = yi - 1 xf = xi return xf,yf","title":"Create a function to walk a single step"},{"location":"Projects/Rwalk/Rwalk/#lets-walk-in-2d-using-above-function","text":"def walk2D (N,pos): ''' This function walks N step in 2d implementing move2D ''' '''coordinate collectors''' X = [] ; Y = [] '''from where to start''' x0 = pos[ 0 ] ; y0 = pos[ 0 ] k = 0 while k < N: '''move a step''' x,y = move2D(x0,y0) '''collect X coordinate''' X . append(x) '''collect Y coordinate''' Y . append(y) '''set previous position for next step''' x0 = x y0 = y k = k + 1 return X,Y","title":"Lets walk in 2D using above function"},{"location":"Projects/Rwalk/Rwalk/#lets-create-a-3-different-random-walk-in-2d","text":"N = 10000 U = walk2D(N,[ 0 , 0 ]) V = walk2D(N,[ 0 , 0 ]) W = walk2D(N,[ 0 , 0 ]) with open ( 'data/rwalk4.json' , 'w' ) as f: json . dump([U,V,W],f) plt . figure(figsize = [ 15 , 12 ]) plt . plot(U[ 0 ],U[ 1 ], \"-\" ) plt . plot(V[ 0 ],V[ 1 ], \"-\" ) plt . plot(W[ 0 ],W[ 1 ], \"-\" ) plt . axis([ - 100 , 100 , - 100 , 100 ]) plt . xlabel( \"x-axis\" ) plt . ylabel( \"y-axis\" ) plt . grid( True ) plt . title( \"Brownian motion in 2D\" ) plt . savefig( 'plot/rwalk2D.pdf' ) plt . show()","title":"Lets create a 3 different Random walk in 2D"},{"location":"References/ref/","text":"References Mkdocs Deploy MkDocs MkDoc Black and Blue Codehilit Syntax highlight all Syntax highlight code","title":"Reference"},{"location":"References/ref/#references","text":"","title":"References"},{"location":"References/ref/#mkdocs","text":"Deploy MkDocs MkDoc Black and Blue Codehilit Syntax highlight all Syntax highlight code","title":"Mkdocs"}]}